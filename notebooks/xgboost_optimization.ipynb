{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lukas/Projects/dmc2019'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# ggf muss Pfad angepasst werden\n",
    "os.chdir(\"{}/..\".format(os.getcwd()))\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection, linear_model, metrics\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split, cross_validate\n",
    "#from scripts.utils import own_scorer, calc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate multiple scores\n",
    "def calc_scores(y_test, y_pred):\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "    f2_score = (0 if all(y_pred == 0) else metrics.fbeta_score(y_test, y_pred, beta=2))\n",
    "    dmc_score = np.sum(confusion_matrix * np.array([[0, -25], [-5, 5]]))\n",
    "\n",
    "    return accuracy, f2_score, dmc_score/len(y_test), confusion_matrix\n",
    "\n",
    "# Normalized DMC Score for usage as scorer\n",
    "def own_scorer_normalized(estimator, X_val, ground_truth):\n",
    "    prediction = estimator.predict(X_val)\n",
    "    confusion_matrix = metrics.confusion_matrix(ground_truth, prediction)\n",
    "    dmc_score = np.sum(confusion_matrix * np.array([[0, -25], [-5, 5]]))\n",
    "    return dmc_score/len(ground_truth)\n",
    "\n",
    "# DMC Score for usage as scorer\n",
    "def own_scorer(estimator, X_val, ground_truth):\n",
    "    prediction = estimator.predict(X_val)\n",
    "    confusion_matrix = metrics.confusion_matrix(ground_truth, prediction)\n",
    "    dmc_score = np.sum(confusion_matrix * np.array([[0, -25], [-5, 5]]))\n",
    "    return dmc_score\n",
    "# F2 Score for usage as scorer\n",
    "def own_f2_score(estimator, X_val, ground_truth):\n",
    "    prediction = estimator.predict(X_val)\n",
    "    return 0 if all(prediction == 0) else metrics.fbeta_score(ground_truth, prediction, beta=2)\n",
    "\n",
    "\n",
    "def report_best_scores(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                results['mean_test_score'][candidate],\n",
    "                results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading train data and add feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data:  (1503, 10) Shape of val data:  (376, 10)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"data/extended_train.csv\", sep=\"|\")\n",
    "df_y = df_train.fraud\n",
    "df_X = df_train.drop(['fraud'], axis=1)\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(df_X, df_y, test_size=0.2)\n",
    "\n",
    "print(\"Shape of train data: \", train_x.shape, \"Shape of val data: \", val_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a quantile scaled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data:  (1503, 10) Shape of val data:  (376, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>totalScannedLineItems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.243764</td>\n",
       "      <td>0.241806</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>7.777778e-01</td>\n",
       "      <td>6.111111e-01</td>\n",
       "      <td>0.821827</td>\n",
       "      <td>0.482265</td>\n",
       "      <td>0.260101</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.120172</td>\n",
       "      <td>0.937788</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>2.222222e-01</td>\n",
       "      <td>0.889656</td>\n",
       "      <td>0.889459</td>\n",
       "      <td>0.489475</td>\n",
       "      <td>0.851852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.158798</td>\n",
       "      <td>0.925902</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>4.444444e-01</td>\n",
       "      <td>6.111111e-01</td>\n",
       "      <td>0.748899</td>\n",
       "      <td>0.889105</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.501301</td>\n",
       "      <td>0.027299</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.111111e-01</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.549707</td>\n",
       "      <td>0.027109</td>\n",
       "      <td>0.694026</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.094837</td>\n",
       "      <td>0.205048</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>3.333333e-01</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>0.889331</td>\n",
       "      <td>0.779881</td>\n",
       "      <td>0.122261</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  \\\n",
       "547     0.444444                0.243764    0.241806       0.388889   \n",
       "1521    1.000000                0.120172    0.937788       0.777778   \n",
       "75      1.000000                0.158798    0.925902       0.388889   \n",
       "1602    0.444444                0.501301    0.027299       0.888889   \n",
       "234     0.444444                0.094837    0.205048       0.111111   \n",
       "\n",
       "      scansWithoutRegistration  quantityModifications  \\\n",
       "547               7.777778e-01           6.111111e-01   \n",
       "1521              1.000000e-07           2.222222e-01   \n",
       "75                4.444444e-01           6.111111e-01   \n",
       "1602              1.111111e-01           1.000000e-07   \n",
       "234               3.333333e-01           9.999999e-01   \n",
       "\n",
       "      scannedLineItemsPerSecond  valuePerSecond  lineItemVoidsPerPosition  \\\n",
       "547                    0.821827        0.482265                  0.260101   \n",
       "1521                   0.889656        0.889459                  0.489475   \n",
       "75                     0.748899        0.889105                  0.555556   \n",
       "1602                   0.549707        0.027109                  0.694026   \n",
       "234                    0.889331        0.779881                  0.122261   \n",
       "\n",
       "      totalScannedLineItems  \n",
       "547                0.777778  \n",
       "1521               0.851852  \n",
       "75                 0.333333  \n",
       "1602               0.555556  \n",
       "234                0.555556  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_quant_scaled_y = df_train.fraud\n",
    "df_wo_frauds = df_train.drop(['fraud'], axis=1)\n",
    "heads = list(df_wo_frauds.columns.values)\n",
    "\n",
    "qt = QuantileTransformer(n_quantiles=10, random_state=0)\n",
    "df_quant_scaled_x = pd.DataFrame(qt.fit_transform(df_wo_frauds), columns=heads)\n",
    "#df_quant_scaled['fraud'] = df_s_y\n",
    "train_qs_x, val_qs_x, train_qs_y, val_qs_y = train_test_split(df_quant_scaled_x, df_quant_scaled_y, test_size=0.2)\n",
    "\n",
    "print(\"Shape of train data: \", train_qs_x.shape, \"Shape of val data: \", val_qs_x.shape)\n",
    "train_qs_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_xgb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:  [45 55 20 20 25] Scaled Data:  [ 20 -10   5  65  75]\n",
      "Original Data:  [0.14900662 0.18272425 0.06666667 0.06666667 0.08333333] Scaled Data:  [ 0.06644518 -0.03322259  0.0166113   0.21666667  0.25      ]\n"
     ]
    }
   ],
   "source": [
    "default_xgb = xgb.XGBClassifier(n_jobs=6, verbosity=2)\n",
    "res_train1 = cross_validate(default_xgb, train_x,train_y, scoring=own_scorer, cv=5,n_jobs=-1)['test_score']\n",
    "res_train2 = cross_validate(default_xgb, train_qs_x,train_qs_y, scoring=own_scorer, cv=5,n_jobs=-1)['test_score']\n",
    "res_n_train1 = cross_validate(default_xgb, train_x,train_y, scoring=own_scorer_normalized, cv=5,n_jobs=-1)['test_score']\n",
    "res_n_train2 = cross_validate(default_xgb, train_qs_x,train_qs_y, scoring=own_scorer_normalized, cv=5,n_jobs=-1)['test_score']\n",
    "print(\"Original Data: \", res_train1,\"Scaled Data: \", res_train2)\n",
    "print(\"Original Data: \", res_n_train1,\"Scaled Data: \", res_n_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"learning_rate\": uniform(0.1, 0.7),\n",
    "    \"gamma\": uniform(0, 1),\n",
    "    \"max_depth\": randint(2, 6),  # default 3\n",
    "    \"n_estimators\": randint(300, 450),  # default 100\n",
    "    \"colsample_bytree\" : uniform(0.1,0.9)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_xgb = xgb.XGBClassifier(booster=\"gblinear\",objective='binary:logistic',n_jobs=6, verbosity=2)\n",
    "ftwo_scorer = metrics.make_scorer(metrics.fbeta_score, beta=0.5172)\n",
    "search = RandomizedSearchCV(xgb_model, scoring=ftwo_scorer, param_distributions=params, random_state=42, n_iter=1000,\n",
    "                            cv=3, verbose=1, n_jobs=-1, return_train_score=True,refit)\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
