{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with the Test split?\n",
    "This is what we want to do:\n",
    "\n",
    "1. predict batches with ...\n",
    ".TODO: create list of content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.) set run-config and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_SUBMISSION = False # will perform a test on a validation split if set to False\n",
    "\n",
    "TEST_BATCH_SIZE = 10000 # Number of Test entries to add to the training set for the next iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.) define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scores(y_test, y_pred):\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    confusion_matrix = (metrics.confusion_matrix(y_test, y_pred)).tolist()\n",
    "    dmc_score = np.sum(confusion_matrix * np.array([[0, -25], [-5, 5]]))\n",
    "    return accuracy, dmc_score, confusion_matrix\n",
    "\n",
    "def find_nearest_neighbor_index(row, dataset):\n",
    "    scaler = MinMaxScaler()\n",
    "    dataset_scaled = scaler.fit_transform(dataset)\n",
    "    row_scaled = scaler.transform([row])\n",
    "    diffs = [np.sum((row_scaled[0] - ds_row)**2) for ds_row in dataset_scaled]\n",
    "    return np.argmin(diffs)\n",
    "\n",
    "def get_classifier(name):\n",
    "    return {\n",
    "        'xgb': XGBClassifier(),\n",
    "        'svc': LinearSVC(C=0.8669055747631755, class_weight=None, dual=False,\n",
    "                 fit_intercept=True, intercept_scaling=1.1311617930050963,\n",
    "                 loss='squared_hinge', max_iter=20000, multi_class='ovr', penalty='l2',\n",
    "                 random_state=None, tol=0.0039333067038518875, verbose=0)\n",
    "    }[name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.) import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convention for variables names: datasetname_columntype_transformstatus_dataframeornot\n",
    "train_Xy_original_df = pd.read_csv(\"../data/train.csv\", sep=\"|\") if FINAL_SUBMISSION else pd.read_csv(\"../data/train_new.csv\", sep=\"|\")\n",
    "test_X_original_df   = pd.read_csv(\"../data/test.csv\", sep=\"|\") #.iloc[0:301] #TODO: For faster testing we use less data from the test set\n",
    "train_y_original_df = train_Xy_original_df[[\"fraud\"]].copy()\n",
    "train_X_original_df = train_Xy_original_df.copy().drop(\"fraud\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.) prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>totalScannedLineItems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.254645</td>\n",
       "      <td>0.884888</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.206897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.548087</td>\n",
       "      <td>0.589959</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.023569</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  \\\n",
       "0         0.6                0.254645    0.884888       0.363636   \n",
       "1         0.4                0.548087    0.589959       0.636364   \n",
       "\n",
       "   scansWithoutRegistration  quantityModifications  scannedLineItemsPerSecond  \\\n",
       "0                       0.8                    0.8                   0.000481   \n",
       "1                       0.6                    0.2                   0.000878   \n",
       "\n",
       "   valuePerSecond  lineItemVoidsPerPosition  totalScannedLineItems  \n",
       "0        0.001900                  0.051948               0.206897  \n",
       "1        0.000589                  0.023569               0.896552  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class dataTransformer:\n",
    "    \"\"\"\n",
    "    for scaling, data transformations (new features, one-hot encoding, categorical, ...)\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    def fitScaler(self, df):\n",
    "        self.scaler.fit(df.astype(np.float64))\n",
    "        return self\n",
    "        \n",
    "    def applyScale(self, df):\n",
    "        return pd.DataFrame(self.scaler.transform(df), df.index, df.columns)\n",
    "    \n",
    "    def inverseScale(self, df):\n",
    "        return pd.DataFrame(self.scaler.inverse_transform(df), df.index, df.columns)\n",
    "    \n",
    "    def addFeatures(self,df):\n",
    "        #TODO: Choose relevant features\n",
    "        df['totalScannedLineItems'] = df['scannedLineItemsPerSecond'] * df['totalScanTimeInSeconds']\n",
    "        #df['avgTimePerScan'] = 1/ df['scannedLineItemsPerSecond']\n",
    "        #df['avgValuePerScan'] = df['avgTimePerScan'] * df['valuePerSecond']\n",
    "        #df['withoutRegisPerPosition'] = df['scansWithoutRegistration'] / df['totalScannedLineItems'] #equivalent to lineItemVoidsPerPosition?\n",
    "        #df['quantiModPerPosition'] = df['quantityModifications'] / df['totalScannedLineItems']\n",
    "        #df['lineItemVoidsPerTotal'] = df['lineItemVoids'] / df['grandTotal']\n",
    "        #df['withoutRegisPerTotal'] = df['scansWithoutRegistration'] / df['grandTotal']\n",
    "        #df['quantiModPerTotal'] = df['quantityModifications'] / df['grandTotal']\n",
    "        #df['lineItemVoidsPerTime'] = df['lineItemVoids'] / df['totalScanTimeInSeconds']\n",
    "        #df['withoutRegisPerTime'] = df['scansWithoutRegistration'] / df['totalScanTimeInSeconds']\n",
    "        #df['quantiModPerTime'] = df['quantityModifications'] / df['totalScanTimeInSeconds']\n",
    "        #df['valuePerScannedLineItem'] = df['valuePerSecond'] / df['scannedLineItemsPerSecond']\n",
    "        return df\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        All in one: Apply all transform methods\n",
    "            1.) addFeatures\n",
    "            2.) applyScale\n",
    "        \"\"\"\n",
    "        return self.applyScale(self.addFeatures(df))\n",
    "    \n",
    "\n",
    "transformer = dataTransformer()\n",
    "transformer.fitScaler(transformer.addFeatures(train_X_original_df.append(test_X_original_df, sort=False)))\n",
    "train_X_transformed_df = transformer.transform(train_X_original_df.copy())\n",
    "test_X_transformed_df  = transformer.transform(test_X_original_df.copy())\n",
    "\n",
    "test_X_transformed_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.) iterative model training using pseudo-labeling\n",
    "predict batches of the test set, add them to the previous training set and use this new training set to predict the next batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test size: 498121 , with a batchsize of 10000  we will need 50 iterations:\n",
      "iteration 1 \twith batch from 0 \t to 10000 , training with 1503 samples\n",
      "iteration 2 \twith batch from 10000 \t to 20000 , training with 11503 samples\n",
      "iteration 3 \twith batch from 20000 \t to 30000 , training with 21503 samples\n",
      "iteration 4 \twith batch from 30000 \t to 40000 , training with 31503 samples\n",
      "iteration 5 \twith batch from 40000 \t to 50000 , training with 41503 samples\n",
      "iteration 6 \twith batch from 50000 \t to 60000 , training with 51503 samples\n",
      "iteration 7 \twith batch from 60000 \t to 70000 , training with 61503 samples\n",
      "iteration 8 \twith batch from 70000 \t to 80000 , training with 71503 samples\n",
      "iteration 9 \twith batch from 80000 \t to 90000 , training with 81503 samples\n",
      "iteration 10 \twith batch from 90000 \t to 100000 , training with 91503 samples\n",
      "iteration 11 \twith batch from 100000 \t to 110000 , training with 101503 samples\n",
      "iteration 12 \twith batch from 110000 \t to 120000 , training with 111503 samples\n",
      "iteration 13 \twith batch from 120000 \t to 130000 , training with 121503 samples\n",
      "iteration 14 \twith batch from 130000 \t to 140000 , training with 131503 samples\n",
      "iteration 15 \twith batch from 140000 \t to 150000 , training with 141503 samples\n",
      "iteration 16 \twith batch from 150000 \t to 160000 , training with 151503 samples\n",
      "iteration 17 \twith batch from 160000 \t to 170000 , training with 161503 samples\n",
      "iteration 18 \twith batch from 170000 \t to 180000 , training with 171503 samples\n",
      "iteration 19 \twith batch from 180000 \t to 190000 , training with 181503 samples\n",
      "iteration 20 \twith batch from 190000 \t to 200000 , training with 191503 samples\n",
      "iteration 21 \twith batch from 200000 \t to 210000 , training with 201503 samples\n",
      "iteration 22 \twith batch from 210000 \t to 220000 , training with 211503 samples\n",
      "iteration 23 \twith batch from 220000 \t to 230000 , training with 221503 samples\n",
      "iteration 24 \twith batch from 230000 \t to 240000 , training with 231503 samples\n",
      "iteration 25 \twith batch from 240000 \t to 250000 , training with 241503 samples\n",
      "iteration 26 \twith batch from 250000 \t to 260000 , training with 251503 samples\n",
      "iteration 27 \twith batch from 260000 \t to 270000 , training with 261503 samples\n",
      "iteration 28 \twith batch from 270000 \t to 280000 , training with 271503 samples\n",
      "iteration 29 \twith batch from 280000 \t to 290000 , training with 281503 samples\n",
      "iteration 30 \twith batch from 290000 \t to 300000 , training with 291503 samples\n",
      "iteration 31 \twith batch from 300000 \t to 310000 , training with 301503 samples\n",
      "iteration 32 \twith batch from 310000 \t to 320000 , training with 311503 samples\n",
      "iteration 33 \twith batch from 320000 \t to 330000 , training with 321503 samples\n",
      "iteration 34 \twith batch from 330000 \t to 340000 , training with 331503 samples\n",
      "iteration 35 \twith batch from 340000 \t to 350000 , training with 341503 samples\n",
      "iteration 36 \twith batch from 350000 \t to 360000 , training with 351503 samples\n",
      "iteration 37 \twith batch from 360000 \t to 370000 , training with 361503 samples\n",
      "iteration 38 \twith batch from 370000 \t to 380000 , training with 371503 samples\n",
      "iteration 39 \twith batch from 380000 \t to 390000 , training with 381503 samples\n",
      "iteration 40 \twith batch from 390000 \t to 400000 , training with 391503 samples\n",
      "iteration 41 \twith batch from 400000 \t to 410000 , training with 401503 samples\n",
      "iteration 42 \twith batch from 410000 \t to 420000 , training with 411503 samples\n",
      "iteration 43 \twith batch from 420000 \t to 430000 , training with 421503 samples\n",
      "iteration 44 \twith batch from 430000 \t to 440000 , training with 431503 samples\n",
      "iteration 45 \twith batch from 440000 \t to 450000 , training with 441503 samples\n",
      "iteration 46 \twith batch from 450000 \t to 460000 , training with 451503 samples\n",
      "iteration 47 \twith batch from 460000 \t to 470000 , training with 461503 samples\n",
      "iteration 48 \twith batch from 470000 \t to 480000 , training with 471503 samples\n",
      "iteration 49 \twith batch from 480000 \t to 490000 , training with 481503 samples\n",
      "iteration 50 \twith batch from 490000 \t to 498121 , training with 491503 samples\n",
      "training with pseudo labeling completed, last iteration used 499624 samples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>totalScannedLineItems</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.451913</td>\n",
       "      <td>0.665667</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  \\\n",
       "0         0.6                0.451913    0.665667       0.636364   \n",
       "\n",
       "   scansWithoutRegistration  quantityModifications  scannedLineItemsPerSecond  \\\n",
       "0                       0.4                    0.6                   0.000223   \n",
       "\n",
       "   valuePerSecond  lineItemVoidsPerPosition  totalScannedLineItems  fraud  \n",
       "0        0.000806                  0.106061               0.172414      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_extended_pltrain_for_batch(testbatch_X_transformed_df, pltrain_X_transformed_df, pltrain_y_original_df):\n",
    "    \n",
    "    #TODO: Use KNN to get best classifier\n",
    "    \n",
    "    # train a classificator on the pseudo labeled train (pltrain) dataset\n",
    "    clf = get_classifier('svc')\n",
    "    clf.fit(pltrain_X_transformed_df, pltrain_y_original_df.fraud)\n",
    "    \n",
    "    # predict labels for batch\n",
    "    testbatch_y_original = clf.predict(testbatch_X_transformed_df)\n",
    "    testbatch_Xy_transandorig_df = testbatch_X_transformed_df.assign(fraud = testbatch_y_original)\n",
    "    \n",
    "    # add batch to pseudo labeled train (pltrain) dataset. needs to ignore index as ids in test also start with 0\n",
    "    pltrainnew_X_transformed_df = pltrain_X_transformed_df.append(testbatch_X_transformed_df, ignore_index=True)\n",
    "    pltrainnew_y_original_df = pltrain_y_original_df.append(testbatch_Xy_transandorig_df[['fraud']], ignore_index=True)\n",
    "    return pltrainnew_X_transformed_df, pltrainnew_y_original_df\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"total test size:\",len(test_X_transformed_df),\", with a batchsize of\",TEST_BATCH_SIZE,\" we will need\",int(np.ceil(len(test_X_transformed_df)/TEST_BATCH_SIZE)),\"iterations:\")\n",
    "\n",
    "#initialize pseudo labeled train (pltrain) dataset with the transformed training data\n",
    "pltrain_X_transformed_df = train_X_transformed_df.copy()\n",
    "pltrain_y_original_df = train_y_original_df.copy()\n",
    "\n",
    "\n",
    "# iterate through fixed-size batches\n",
    "for i in range(TEST_BATCH_SIZE, len(test_X_transformed_df), TEST_BATCH_SIZE):\n",
    "    print(\"iteration\",int(i/TEST_BATCH_SIZE),\"\\twith batch from\",i-TEST_BATCH_SIZE,\"\\t to\", i,\", training with\",len(pltrain_y_original_df),\"samples\")\n",
    "    # get batch from test set\n",
    "    testbatch_X_transformed_df = test_X_transformed_df.iloc[i-TEST_BATCH_SIZE:i]\n",
    "    \n",
    "    # extend pseudo labeled train (pltrain) dataset by predicting the batch\n",
    "    pltrain_X_transformed_df, pltrain_y_original_df = get_extended_pltrain_for_batch(testbatch_X_transformed_df, pltrain_X_transformed_df, pltrain_y_original_df)\n",
    "    \n",
    "    #if i>len(df_test_X_transformed)-1000:\n",
    "    #    print(i)\n",
    "    #    display(df_test_X_transformed_batch.head(1))\n",
    "\n",
    "# use last few rows that cant fill up a complete batch as a smaller batch\n",
    "print(\"iteration\",int(i/TEST_BATCH_SIZE)+1,\"\\twith batch from\",i,\"\\t to\", len(test_X_transformed_df),\", training with\",len(pltrain_y_original_df),\"samples\")\n",
    "testbatch_X_transformed_df = test_X_transformed_df.iloc[i:len(test_X_transformed_df)]\n",
    "\n",
    "# extend pseudo labeled train (pltrain) dataset by predicting the small batch\n",
    "pltrain_X_transformed_df, pltrain_y_original_df = get_extended_pltrain_for_batch(testbatch_X_transformed_df, pltrain_X_transformed_df, pltrain_y_original_df)\n",
    "\n",
    "#combine x and y columns dataframes to one big dataframe\n",
    "pltrain_Xy_transandorig_df = pltrain_X_transformed_df.assign(fraud = pltrain_y_original_df.fraud.values)\n",
    "\n",
    "print(\"training with pseudo labeling completed, last iteration used\",len(pltrain_Xy_transandorig_df),\"samples.\")\n",
    "\n",
    "display(pltrain_Xy_transandorig_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.) pseudo-label the test set and create new classifier based on this\n",
    "first we predict the original test data labels using the new extended pltrain from above cell and second we use this test data labels to train a new classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>totalScannedLineItems</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.254645</td>\n",
       "      <td>0.884888</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  \\\n",
       "0         0.6                0.254645    0.884888       0.363636   \n",
       "\n",
       "   scansWithoutRegistration  quantityModifications  scannedLineItemsPerSecond  \\\n",
       "0                       0.8                    0.8                   0.000481   \n",
       "\n",
       "   valuePerSecond  lineItemVoidsPerPosition  totalScannedLineItems  fraud  \n",
       "0          0.0019                  0.051948               0.206897      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train a classificator on the pseudo labeled train (pltrain) dataset\n",
    "pltrain_clf = get_classifier('svc')\n",
    "pltrain_clf.fit(pltrain_X_transformed_df, pltrain_y_original_df.fraud)\n",
    "\n",
    "# predict labels for (transformed) original test set\n",
    "pltest_y_original = pltrain_clf.predict(test_X_transformed_df)\n",
    "\n",
    "# combine x and y columns dataframes to one big dataframe\n",
    "pltest_Xy_transandorig_df = test_X_transformed_df.assign(fraud = pltest_y_original)\n",
    "display(pltest_Xy_transandorig_df.head(1))\n",
    "\n",
    "# train a new classifier based on pltest\n",
    "pltest_clf = get_classifier('svc')\n",
    "pltest_clf.fit(test_X_transformed_df, pltest_y_original);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.) evaluate our new classifier with the original training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9906852960745176, 220, [[1419, 3], [11, 70]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainpred_y_original = pltest_clf.predict(train_X_transformed_df)\n",
    "calc_scores(train_y_original_df.fraud.values, trainpred_y_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.) combine the pseudo labeled test set with the original train data to train our final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--> already done in step 7\n",
    "final_clf = pltrain_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.) predict labels for the test set using our final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-->already done in step 7\n",
    "test_y_pred = pltest_y_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.) generate output file neeeded for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fraud\n",
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_y_pred, columns=[\"fraud\"]).to_csv(\"HS_Karlsruhe_1.csv\", index=False)\n",
    "pd.read_csv(\"HS_Karlsruhe_1.csv\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.) evaluate our new classifier with the validation set\n",
    "Now at the very end we can also test our final model on a validation split never used before. just for comparison. \n",
    "\n",
    "**For the final submission, the following code should will not be run and the full train (incl. this val split) set will be used above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9946808510638298, 95, [[353, 0], [2, 21]])\n"
     ]
    }
   ],
   "source": [
    "if not FINAL_SUBMISSION:\n",
    "    val_Xy_original_df = pd.read_csv(\"../data/val_new.csv\", sep=\"|\")\n",
    "    valpred_y_original = final_clf.predict(transformer.transform(val_Xy_original_df.drop(\"fraud\", axis=1)))\n",
    "    print(calc_scores(val_Xy_original_df.fraud.values, valpred_y_original))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
