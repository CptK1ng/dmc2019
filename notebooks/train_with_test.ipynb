{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with the Test split?\n",
    "This is what we want to do:\n",
    "\n",
    "1. predict batches with ...\n",
    ".TODO: create list of content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.) set run-config and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_SUBMISSION = False # will perform a test on a validation split if set to False\n",
    "\n",
    "TEST_BATCH_SIZE = 10000 # Number of Test entries to add to the training set for the next iteration\n",
    "ITER_PRINT_EVERY = 2 # Which Iterations to print (every nth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.) define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scores(y_test, y_pred):\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    confusion_matrix = (metrics.confusion_matrix(y_test, y_pred)).tolist()\n",
    "    dmc_score = np.sum(confusion_matrix * np.array([[0, -25], [-5, 5]]))\n",
    "    return accuracy, dmc_score, confusion_matrix\n",
    "\n",
    "def find_nearest_neighbor_index(row, dataset):\n",
    "    scaler = MinMaxScaler()\n",
    "    dataset_scaled = scaler.fit_transform(dataset)\n",
    "    row_scaled = scaler.transform([row])\n",
    "    diffs = [np.sum((row_scaled[0] - ds_row)**2) for ds_row in dataset_scaled]\n",
    "    return np.argmin(diffs)\n",
    "\n",
    "def get_classifier(name):\n",
    "    return {\n",
    "        'xgb': XGBClassifier(),\n",
    "        'svc': LinearSVC(C=0.8669055747631755, class_weight=None, dual=False,\n",
    "                 fit_intercept=True, intercept_scaling=1.1311617930050963,\n",
    "                 loss='squared_hinge', max_iter=20000, multi_class='ovr', penalty='l2',\n",
    "                 random_state=None, tol=0.0039333067038518875, verbose=0)\n",
    "    }[name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.) import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e6a28b2a0337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#convention for variables names: datasetname_columntype_transformstatus_dataframeornot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainandknn_Xy_original_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mFINAL_SUBMISSION\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/train_new.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_Xy_original_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknn_Xy_original_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mFINAL_SUBMISSION\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#small\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_X_original_df\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/test.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#.iloc[0:301] #TODO: For faster testing we use less data from the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_y_original_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_Xy_original_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fraud\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "#convention for variables names: datasetname_columntype_transformstatus_dataframeornot\n",
    "trainandknn_Xy_original_df = pd.read_csv(\"../data/train.csv\", sep=\"|\") if FINAL_SUBMISSION else pd.read_csv(\"../data/train_new.csv\", sep=\"|\")\n",
    "train_Xy_original_df, knn_Xy_original_df = train_test_split(df_train,train_size=0.8 if FINAL_SUBMISSION else 0.8**2) #small\n",
    "test_X_original_df   = pd.read_csv(\"../data/test.csv\", sep=\"|\") #.iloc[0:301] #TODO: For faster testing we use less data from the test set\n",
    "train_y_original_df = train_Xy_original_df[[\"fraud\"]].copy()\n",
    "train_X_original_df = train_Xy_original_df.copy().drop(\"fraud\", axis=1)\n",
    "knn_y_original_df = knn_Xy_original_df[[\"fraud\"]].copy()\n",
    "knn_X_original_df = knn_Xy_original_df.copy().drop(\"fraud\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.) prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataTransformer:\n",
    "    \"\"\"\n",
    "    for scaling, data transformations (new features, one-hot encoding, categorical, ...)\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    def fitScaler(self, df):\n",
    "        self.scaler.fit(df.astype(np.float64))\n",
    "        return self\n",
    "        \n",
    "    def applyScale(self, df):\n",
    "        return pd.DataFrame(self.scaler.transform(df), df.index, df.columns)\n",
    "    \n",
    "    def inverseScale(self, df):\n",
    "        return pd.DataFrame(self.scaler.inverse_transform(df), df.index, df.columns)\n",
    "    \n",
    "    def addFeatures(self,df):\n",
    "        #TODO: Choose relevant features\n",
    "        df['totalScannedLineItems'] = df['scannedLineItemsPerSecond'] * df['totalScanTimeInSeconds']\n",
    "        #df['avgTimePerScan'] = 1/ df['scannedLineItemsPerSecond']\n",
    "        #df['avgValuePerScan'] = df['avgTimePerScan'] * df['valuePerSecond']\n",
    "        #df['withoutRegisPerPosition'] = df['scansWithoutRegistration'] / df['totalScannedLineItems'] #equivalent to lineItemVoidsPerPosition?\n",
    "        #df['quantiModPerPosition'] = df['quantityModifications'] / df['totalScannedLineItems']\n",
    "        #df['lineItemVoidsPerTotal'] = df['lineItemVoids'] / df['grandTotal']\n",
    "        #df['withoutRegisPerTotal'] = df['scansWithoutRegistration'] / df['grandTotal']\n",
    "        #df['quantiModPerTotal'] = df['quantityModifications'] / df['grandTotal']\n",
    "        #df['lineItemVoidsPerTime'] = df['lineItemVoids'] / df['totalScanTimeInSeconds']\n",
    "        #df['withoutRegisPerTime'] = df['scansWithoutRegistration'] / df['totalScanTimeInSeconds']\n",
    "        #df['quantiModPerTime'] = df['quantityModifications'] / df['totalScanTimeInSeconds']\n",
    "        #df['valuePerScannedLineItem'] = df['valuePerSecond'] / df['scannedLineItemsPerSecond']\n",
    "        return df\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        All in one: Apply all transform methods\n",
    "            1.) addFeatures\n",
    "            2.) applyScale\n",
    "        \"\"\"\n",
    "        return self.applyScale(self.addFeatures(df))\n",
    "    \n",
    "\n",
    "transformer = dataTransformer()\n",
    "transformer.fitScaler(transformer.addFeatures(train_X_original_df.append(test_X_original_df, sort=False)))\n",
    "train_X_transformed_df = transformer.transform(train_X_original_df.copy())\n",
    "knn_X_transformed_df   = transformer.transform(knn_X_original_df.copy())\n",
    "test_X_transformed_df  = transformer.transform(test_X_original_df.copy())\n",
    "\n",
    "test_X_transformed_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 1/2.) train normally with all available classifiers for classifying knn split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnwithprobs_Xy_original_df = knn_Xy_original_df.copy()\n",
    "#TODO: save predict_proba to knnwithprobs_Xy_original_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.) iterative model training using pseudo-labeling\n",
    "predict batches of the test set, add them to the previous training set and use this new training set to predict the next batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extended_pltrain_for_batch(testbatch_X_transformed_df, pltrain_X_transformed_df, pltrain_y_original_df, transformer):\n",
    "    \n",
    "    #TODO: Use KNN (neighbour in following dataframe: knnwithprobs_Xy_original_df) to get best classifier: also use transformer.inverse_transform\n",
    "    \n",
    "    # train a classificator on the pseudo labeled train (pltrain) dataset\n",
    "    clf = get_classifier('svc')\n",
    "    clf.fit(pltrain_X_transformed_df, pltrain_y_original_df.fraud)\n",
    "    \n",
    "    # predict labels for batch\n",
    "    testbatch_y_original = clf.predict(testbatch_X_transformed_df)\n",
    "    testbatch_Xy_transandorig_df = testbatch_X_transformed_df.assign(fraud = testbatch_y_original)\n",
    "    \n",
    "    # add batch to pseudo labeled train (pltrain) dataset. needs to ignore index as ids in test also start with 0\n",
    "    pltrainnew_X_transformed_df = pltrain_X_transformed_df.append(testbatch_X_transformed_df, ignore_index=True)\n",
    "    pltrainnew_y_original_df = pltrain_y_original_df.append(testbatch_Xy_transandorig_df[['fraud']], ignore_index=True)\n",
    "    return pltrainnew_X_transformed_df, pltrainnew_y_original_df\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"total test size:\",len(test_X_transformed_df),\", with a batchsize of\",TEST_BATCH_SIZE,\" we will need\",int(np.ceil(len(test_X_transformed_df)/TEST_BATCH_SIZE)),\"iterations:\")\n",
    "\n",
    "#initialize pseudo labeled train (pltrain) dataset with the transformed training data\n",
    "pltrain_X_transformed_df = train_X_transformed_df.copy()\n",
    "pltrain_y_original_df = train_y_original_df.copy()\n",
    "\n",
    "\n",
    "# iterate through fixed-size batches\n",
    "for i in range(TEST_BATCH_SIZE, len(test_X_transformed_df), TEST_BATCH_SIZE):\n",
    "    if int(i/TEST_BATCH_SIZE) % ITER_PRINT_EVERY == 0:\n",
    "        print(\"iteration\",int(i/TEST_BATCH_SIZE),\"\\t/\",int(np.ceil(len(test_X_transformed_df)/TEST_BATCH_SIZE)),\"with batch from\",i-TEST_BATCH_SIZE,\"\\t to\", i,\", training with\",len(pltrain_y_original_df),\"samples\")\n",
    "    # get batch from test set\n",
    "    testbatch_X_transformed_df = test_X_transformed_df.iloc[i-TEST_BATCH_SIZE:i]\n",
    "    \n",
    "    # extend pseudo labeled train (pltrain) dataset by predicting the batch\n",
    "    pltrain_X_transformed_df, pltrain_y_original_df = get_extended_pltrain_for_batch(testbatch_X_transformed_df, pltrain_X_transformed_df, pltrain_y_original_df, transformer)\n",
    "    \n",
    "    #if i>len(df_test_X_transformed)-1000:\n",
    "    #    print(i)\n",
    "    #    display(df_test_X_transformed_batch.head(1))\n",
    "\n",
    "# use last few rows that cant fill up a complete batch as a smaller batch\n",
    "print(\"iteration\",int(i/TEST_BATCH_SIZE)+1,\"\\twith batch from\",i,\"\\t to\", len(test_X_transformed_df),\", training with\",len(pltrain_y_original_df),\"samples\")\n",
    "testbatch_X_transformed_df = test_X_transformed_df.iloc[i:len(test_X_transformed_df)]\n",
    "\n",
    "# extend pseudo labeled train (pltrain) dataset by predicting the small batch\n",
    "pltrain_X_transformed_df, pltrain_y_original_df = get_extended_pltrain_for_batch(testbatch_X_transformed_df, pltrain_X_transformed_df, pltrain_y_original_df, transformer)\n",
    "\n",
    "#combine x and y columns dataframes to one big dataframe\n",
    "pltrain_Xy_transandorig_df = pltrain_X_transformed_df.assign(fraud = pltrain_y_original_df.fraud.values)\n",
    "\n",
    "print(\"training with pseudo labeling completed, last iteration used\",len(pltrain_Xy_transandorig_df),\"samples.\")\n",
    "\n",
    "display(pltrain_Xy_transandorig_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.) pseudo-label the test set and create new classifier based on this\n",
    "first we predict the original test data labels using the new extended pltrain from above cell and second we use this test data labels to train a new classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a classificator on the pseudo labeled train (pltrain) dataset\n",
    "pltrain_clf = get_classifier('svc')\n",
    "pltrain_clf.fit(pltrain_X_transformed_df, pltrain_y_original_df.fraud)\n",
    "\n",
    "# predict labels for (transformed) original test set\n",
    "pltest_y_original = pltrain_clf.predict(test_X_transformed_df)\n",
    "\n",
    "# combine x and y columns dataframes to one big dataframe\n",
    "pltest_Xy_transandorig_df = test_X_transformed_df.assign(fraud = pltest_y_original)\n",
    "display(pltest_Xy_transandorig_df.head(1))\n",
    "\n",
    "# train a new classifier based on pltest\n",
    "pltest_clf = get_classifier('svc')\n",
    "pltest_clf.fit(test_X_transformed_df, pltest_y_original);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.) evaluate our new classifier with the original training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpred_y_original = pltest_clf.predict(train_X_transformed_df)\n",
    "calc_scores(train_y_original_df.fraud.values, trainpred_y_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.) combine the pseudo labeled test set with the original train data to train our final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--> already done in step 7\n",
    "final_clf = pltrain_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.) predict labels for the test set using our final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-->already done in step 7\n",
    "test_y_pred = pltest_y_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.) generate output file neeeded for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_y_pred, columns=[\"fraud\"]).to_csv(\"HS_Karlsruhe_1.csv\", index=False)\n",
    "pd.read_csv(\"HS_Karlsruhe_1.csv\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.) evaluate our new classifier with the validation set\n",
    "Now at the very end we can also test our final model on a validation split never used before. just for comparison. \n",
    "\n",
    "**For the final submission, the following code should will not be run and the full train (incl. this val split) set will be used above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FINAL_SUBMISSION:\n",
    "    val_Xy_original_df = pd.read_csv(\"../data/val_new.csv\", sep=\"|\")\n",
    "    valpred_y_original = final_clf.predict(transformer.transform(val_Xy_original_df.drop(\"fraud\", axis=1)))\n",
    "    print(calc_scores(val_Xy_original_df.fraud.values, valpred_y_original))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
