{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with the Test split?\n",
    "This is what we want to do:\n",
    "\n",
    "1. predict batches with ...\n",
    ".TODO: create list of content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.) set run-config and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_SUBMISSION = False # will perform a test on a validation split if set to False\n",
    "\n",
    "TEST_BATCH_SIZE = 200 # Number of Test entries to add to the training set for the next iteration\n",
    "ITER_PRINT_EVERY = 50 # Which Iterations to print (every nth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.) define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scores(y_test, y_pred):\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    confusion_matrix = (metrics.confusion_matrix(y_test, y_pred)).tolist()\n",
    "    dmc_score = np.sum(confusion_matrix * np.array([[0, -25], [-5, 5]]))\n",
    "    return accuracy, dmc_score, confusion_matrix\n",
    "\n",
    "def find_nearest_neighbor(row_scaled, dataset_scaled):  \n",
    "    diffs = [np.sum((row_scaled[0] - ds_row)**2) for ds_row in dataset_scaled]\n",
    "    idx = np.argmin(diffs)[0]\n",
    "    return idx, diffs[idx]\n",
    "\n",
    "def get_classifier(name):\n",
    "    return {\n",
    "        'xgb': XGBClassifier(),\n",
    "        'svc': LinearSVC(C=0.8669055747631755, class_weight=None, dual=False,\n",
    "                 fit_intercept=True, intercept_scaling=1.1311617930050963,\n",
    "                 loss='squared_hinge', max_iter=20000, multi_class='ovr', penalty='l2',\n",
    "                 random_state=None, tol=0.0039333067038518875, verbose=0)\n",
    "    }[name]\n",
    "\n",
    "def name_best_classifier_for_sample(idx, validation_set):\n",
    "    ground_truth = validation_set.iloc[idx].fraud\n",
    "    \n",
    "    # Both classifier predicted the calue correctly\n",
    "    if (validation_set.iloc[idx].lsvc_predict == ground_truth) and (validation_set.iloc[idx].xgb_predict == ground_truth):\n",
    "        if validation_set.iloc[idx].lsvc_proba > validation_set.iloc[idx].xgb_proba:\n",
    "            return \"lsvc\"\n",
    "        else:\n",
    "            return \"xgboost\"\n",
    "    # lsvc predicted correctly\n",
    "    elif (validation_set.iloc[idx].lsvc_predict == ground_truth) and (validation_set.iloc[idx].xgb_predict != ground_truth):\n",
    "        return \"lsvc\"\n",
    "    \n",
    "    # xgboost predicted correcltly\n",
    "    elif (validation_set.iloc[idx].lsvc_predict != ground_truth) and (validation_set.iloc[idx].xgb_predict == ground_truth):\n",
    "        return \"xgboost\"\n",
    "    \n",
    "    # If No classifier predicted the knn correct, None is returned\n",
    "    else: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.) Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/Projects/dmc2019/venv/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "trainandknn_Xy_original_df = pd.read_csv(\"../data/train.csv\", sep=\"|\") if FINAL_SUBMISSION else pd.read_csv(\"../data/train_new.csv\", sep=\"|\")\n",
    "train_Xy_original_df, knn_Xy_original_df = train_test_split(trainandknn_Xy_original_df,train_size=0.75) # if FINAL_SUBMISSION else 0.8**2) #small\n",
    "test_X_original_df  = pd.read_csv(\"../data/test.csv\", sep=\"|\") #.iloc[0:301] #TODO: For faster testing we use less data from the test set\n",
    "\n",
    "#Only for test routines\n",
    "val_Xy_original_df = pd.read_csv(\"../data/val_new.csv\", sep=\"|\")\n",
    "train_Xy_complete_original_df = pd.read_csv(\"../data/train.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.) Prepare Input X and Label Y Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convention for variables names: datasetname_columntype_transformstatus_dataframeornot\n",
    "train_y_original_df = train_Xy_original_df[[\"fraud\"]].copy()\n",
    "train_X_original_df = train_Xy_original_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "knn_y_original_df = knn_Xy_original_df[[\"fraud\"]].copy()\n",
    "knn_X_original_df = knn_Xy_original_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "# Only for test routie#nes\n",
    "val_y_originial_df = val_Xy_original_df[[\"fraud\"]].copy()\n",
    "val_X_originial_df = val_Xy_original_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "train_y_complete_complete = train_Xy_complete_original_df[[\"fraud\"]].copy()\n",
    "train_X_complete_complete = train_Xy_complete_original_df.copy().drop(\"fraud\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.) DataTransformer Class and data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>totalScannedLineItems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.254645</td>\n",
       "      <td>0.884888</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.206897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.548087</td>\n",
       "      <td>0.589959</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.023569</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  \\\n",
       "0         0.6                0.254645    0.884888       0.363636   \n",
       "1         0.4                0.548087    0.589959       0.636364   \n",
       "\n",
       "   scansWithoutRegistration  quantityModifications  scannedLineItemsPerSecond  \\\n",
       "0                       0.8                    0.8                   0.000481   \n",
       "1                       0.6                    0.2                   0.000878   \n",
       "\n",
       "   valuePerSecond  lineItemVoidsPerPosition  totalScannedLineItems  \n",
       "0        0.001900                  0.051948               0.206897  \n",
       "1        0.000589                  0.023569               0.896552  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DataTransformer:\n",
    "    \"\"\"\n",
    "    for scaling, data transformations (new features, one-hot encoding, categorical, ...)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scaler):\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def fit_scaler(self, df):\n",
    "        df_tmp = df.copy()\n",
    "        self.scaler.fit(df_tmp.astype(np.float64))\n",
    "        return self\n",
    "        \n",
    "    def apply_scaler(self, df):\n",
    "        return pd.DataFrame(self.scaler.transform(df), df.index, df.columns)\n",
    "    \n",
    "    def inverse_scale(self, df):\n",
    "        df_tmp = df.copy()\n",
    "        return pd.DataFrame(self.scaler.inverse_transform(df_tmp), df_tmp.index, df_tmp.columns)\n",
    "    \n",
    "    def add_features(self,df):\n",
    "        #TODO: Choose relevant features\n",
    "        df_tmp = df.copy()\n",
    "        df_tmp['totalScannedLineItems'] = df_tmp['scannedLineItemsPerSecond'] * df_tmp['totalScanTimeInSeconds']\n",
    "        #df['avgTimePerScan'] = 1/ df['scannedLineItemsPerSecond']\n",
    "        #df['avgValuePerScan'] = df['avgTimePerScan'] * df['valuePerSecond']\n",
    "        #df['withoutRegisPerPosition'] = df['scansWithoutRegistration'] / df['totalScannedLineItems'] #equivalent to lineItemVoidsPerPosition?\n",
    "        #df['quantiModPerPosition'] = df['quantityModifications'] / df['totalScannedLineItems']\n",
    "        #df['lineItemVoidsPerTotal'] = df['lineItemVoids'] / df['grandTotal']\n",
    "        #df['withoutRegisPerTotal'] = df['scansWithoutRegistration'] / df['grandTotal']\n",
    "        #df['quantiModPerTotal'] = df['quantityModifications'] / df['grandTotal']\n",
    "        #df['lineItemVoidsPerTime'] = df['lineItemVoids'] / df['totalScanTimeInSeconds']\n",
    "        #df['withoutRegisPerTime'] = df['scansWithoutRegistration'] / df['totalScanTimeInSeconds']\n",
    "        #df['quantiModPerTime'] = df['quantityModifications'] / df['totalScanTimeInSeconds']\n",
    "        #df['valuePerScannedLineItem'] = df['valuePerSecond'] / df['scannedLineItemsPerSecond']\n",
    "        return df_tmp\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        All in one: Apply all transform methods\n",
    "            1.) addFeatures\n",
    "            2.) apply_scaler\n",
    "        \"\"\"\n",
    "        df_tmp = df.copy()\n",
    "        return self.apply_scaler(self.add_features(df_tmp))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>totalScannedLineItems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.254645</td>\n",
       "      <td>0.884888</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.206897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.548087</td>\n",
       "      <td>0.589959</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.023569</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  \\\n",
       "0         0.6                0.254645    0.884888       0.363636   \n",
       "1         0.4                0.548087    0.589959       0.636364   \n",
       "\n",
       "   scansWithoutRegistration  quantityModifications  scannedLineItemsPerSecond  \\\n",
       "0                       0.8                    0.8                   0.000481   \n",
       "1                       0.6                    0.2                   0.000878   \n",
       "\n",
       "   valuePerSecond  lineItemVoidsPerPosition  totalScannedLineItems  \n",
       "0        0.001900                  0.051948               0.206897  \n",
       "1        0.000589                  0.023569               0.896552  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "transformer = DataTransformer(scaler)\n",
    "\n",
    "# Adding new Features to train and test set\n",
    "train_X_unscaled_df = transformer.add_features(train_X_original_df)\n",
    "test_X_unscaled_df = transformer.add_features(test_X_original_df)\n",
    "knn_X_unscaled_df = transformer.add_features(knn_X_original_df)\n",
    "\n",
    "val_X_unscaled_df = transformer.add_features(val_X_originial_df)\n",
    "train_X_complete_unscaled_df = transformer.add_features(train_X_original_df) \n",
    "\n",
    "transformer.fit_scaler(transformer.add_features(train_X_complete_unscaled.append(test_X_unscaled_df, sort=False)))\n",
    "train_X_scaled_df = transformer.apply_scaler(train_X_unscaled_df)\n",
    "knn_X_scaled_df   = transformer.apply_scaler(knn_X_unscaled_df)\n",
    "\n",
    "\n",
    "test_X_scaled_df  = transformer.apply_scaler(test_X_unscaled_df)\n",
    "val_X_scaled_df = transformer.apply_scaler(val_X_unscaled_df)\n",
    "train_X_complete_scaled_df = transformer.apply_scaler(train_X_complete_unscaled_df)\n",
    "\n",
    "test_X_scaled_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name     | New Features | Scaled |\n",
    "|----------|--------------|--------|\n",
    "| orig     |      [ ]     |   [ ]  |\n",
    "| unscaled |      [X]     |   [ ]  |\n",
    "| scaled   |      [X]     |   [X]  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 1/2.) train normally with all available classifiers for classifying knn split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnwithprobs_Xy_df = knn_X_scaled_df.copy()\n",
    "#TODO: save predict_proba to knnwithprobs_Xy_original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_routine(lsvc_classifier, xgboost_classifier, data_dict ,data_transformer, step):\n",
    "    \n",
    "    #Predict on ValidationSet\n",
    "    lsvc_val_acc, lsvc_val_dmc, lsvc_val_conf_mat = calc_scores(data_dict['val_y'],lsvc_pred_val)\n",
    "    xgb_val_acc, xgb_val_dmc, xgb_val_conf_mat = calc_scores(data_dict['val_y'],lsvc_pred_val)\n",
    "    \n",
    "    lsvc_pred_val = fitted_lsvc_classifier.predict(data_dict['val_X_transformed'])\n",
    "    xgb_pred_val = fitted_xgboost_classifier.predict(data_dict['val_X'])\n",
    "    \n",
    "    #Predict on original full size (~1900 samples) just trained on test_data    \n",
    "    lsvc_pred_train = data_dict['train_X_transformed']\n",
    "    xgb_pred_train = data_dict['train_X']\n",
    "    lsvc_train_acc, lsvc_train_dmc, lsvc_train_conf_mat = calc_scores(data_dict['val_y'],lsvc_pred_val)\n",
    "    xgb_train_acc, xgb_train_dmc, xgb_train_conf_mat = calc_scores(data_dict['val_y'],lsvc_pred_val)\n",
    "    \n",
    "    results = {\"lin_svc\":{\n",
    "                    \"val\": {\n",
    "                        \"dmc_score\": lsvc_val_dmc,\n",
    "                        \"conf_matrix\": lsvc_val_conf_mat\n",
    "                    },\n",
    "                    \"train\": {\n",
    "                        \"dmc_score\": lsvc_train_dmc,\n",
    "                        \"conf_matrix\": lsvc_train_conf_mat\n",
    "                    }\n",
    "                },\n",
    "                \"xgboost\": {\n",
    "                    \"val\": {\n",
    "                        \"dmc_score\": xgb_val_dmc,\n",
    "                        \"conf_matrix\": xgb_val_conf_mat\n",
    "                    },\n",
    "                    \"train\": {\n",
    "                        \"dmc_score\": xgb_train_dmc,\n",
    "                        \"conf_matrix\": xgb_train_conf_mat\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(xgboost, linear_svc, data_to_predict, data_knn_with_probs, transformer):\n",
    "    prediction = []\n",
    "    data_knn_transformed = transformer.apply_scaler(data_knn.copy().drop(columns=[\"xgb_predict\", \"xgb_proba\", \"lsvc_predict\", \"lsvc_proba\"])) \n",
    "    data_to_predict_transformed = transformer.apply_scaler(data_to_predict,copy())\n",
    "    data_to_predict_original = data_to_predict.copy()\n",
    "\n",
    "    for index, row in data_to_predict_orig.iterrows():\n",
    "        if sample.trustLevel >= 3:\n",
    "            prediction.append(0)            \n",
    "    \n",
    "        else:             \n",
    "            idx_knn, distance_knn = find_nearest_neighbor(data_to_predict_transformed.iloc[index], data_knn_transformed)\n",
    "            # If distance to knn is to big, classify them directly\n",
    "            if distance > 0.15:\n",
    "                xgb_pred = xgboost_classifier.predict([data_to_predict_original.iloc[index]])[0]\n",
    "                xgb_prob = max(np.ravel(xgboost_classifier.predict_proba([data_to_predict_original.iloc[index]])))\n",
    "\n",
    "                lsvc_pred = lsvc_classifier.predict([data_to_predict_transformed.iloc[index]])[0]\n",
    "                lsvc_prob = max(np.ravel(lsvc_classifier.predict_proba([data_to_predict_transformed.iloc[index]])))\n",
    "                # If both classified them equal, take one of both\n",
    "                if xgb_pred == lsvc_pred:\n",
    "                    prediction.append(xgb_pred)\n",
    "\n",
    "                #if classification is not equal, take the one with higher probability\n",
    "                elif xgb_prob > lsvc_prob:\n",
    "                    prediction.append(xgb_pred)\n",
    "                else: \n",
    "                    prediction.append(lsvc_pred)\n",
    "                    \n",
    "            # If distance is smaller than 0.15, use knn    \n",
    "            else:    \n",
    "                best_classifier = best_classifier_for_sample(idx, data_knn_with_probs)\n",
    "                if best_classifier == \"xgboost\":\n",
    "                    prediction.append(xgboost_classifier.predict([data_to_predict_original.iloc[index]])[0])\n",
    "\n",
    "                elif best_classifier == \"lsvc\":\n",
    "                    prediction.append(lsvc_classifier.predict([data_to_predict_transformed.iloc[index]])[0])\n",
    "\n",
    "                elif best_classifier is None:\n",
    "                    return None\n",
    "                \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_supervised_learning_procedure(test_X):\n",
    "    for i in range(TEST_BATCH_SIZE, len(test_X_transformed), TEST_BATCH_SIZE):\n",
    "        if int(i/TEST_BATCH_SIZE) % ITER_PRINT_EVERY == 0:\n",
    "            print(\"iteration\",int(i/TEST_BATCH_SIZE),\"\\t/\",int(np.ceil(len(test_X_transformed)/TEST_BATCH_SIZE)),\"with batch from\",i-TEST_BATCH_SIZE,\"\\t to\", i,\", training with\",len(pltrain_y_original_df),\"samples\")\n",
    "        # get batch from test set\n",
    "        testbatch_X_df = test_X.iloc[i-TEST_BATCH_SIZE:i]\n",
    "        \n",
    "        \n",
    "        # extend pseudo labeled train (pltrain) dataset by predicting the batch\n",
    "        pltrain_X_transformed_df, pltrain_y_original_df = get_extended_pltrain_for_batch(testbatch_X_df, pltrain_X_df, pltrain_y_original_df, transformer)\n",
    "        \n",
    "    \n",
    "    # use last few rows that cant fill up a complete batch as a smaller batch\n",
    "    print(\"iteration\",int(i/TEST_BATCH_SIZE)+1,\"\\twith batch from\",i,\"\\t to\", len(test_X_transformed_df),\", training with\",len(pltrain_y_original_df),\"samples\")\n",
    "    testbatch_X_transformed_df = test_X_transformed_df.iloc[i:len(test_X_transformed_df)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.) iterative model training using pseudo-labeling\n",
    "predict batches of the test set, add them to the previous training set and use this new training set to predict the next batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test size: 498121 , with a batchsize of 200  we will need 2491 iterations:\n",
      "iteration 50 \t/ 2491 with batch from 9800 \t to 10000 , training with 10927 samples\n",
      "iteration 100 \t/ 2491 with batch from 19800 \t to 20000 , training with 20927 samples\n",
      "iteration 150 \t/ 2491 with batch from 29800 \t to 30000 , training with 30927 samples\n",
      "iteration 200 \t/ 2491 with batch from 39800 \t to 40000 , training with 40927 samples\n",
      "iteration 250 \t/ 2491 with batch from 49800 \t to 50000 , training with 50927 samples\n",
      "iteration 300 \t/ 2491 with batch from 59800 \t to 60000 , training with 60927 samples\n",
      "iteration 350 \t/ 2491 with batch from 69800 \t to 70000 , training with 70927 samples\n",
      "iteration 400 \t/ 2491 with batch from 79800 \t to 80000 , training with 80927 samples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f84dfcc0b337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# extend pseudo labeled train (pltrain) dataset by predicting the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mpltrain_X_transformed_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpltrain_y_original_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_extended_pltrain_for_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestbatch_X_transformed_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpltrain_X_transformed_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpltrain_y_original_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#if i>len(df_test_X_transformed)-1000:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-f84dfcc0b337>\u001b[0m in \u001b[0;36mget_extended_pltrain_for_batch\u001b[0;34m(testbatch_X_transformed_df, pltrain_X_transformed_df, pltrain_y_original_df, transformer)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# train a classificator on the pseudo labeled train (pltrain) dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'svc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpltrain_X_transformed_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpltrain_y_original_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfraud\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# predict labels for batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/dmc2019/venv/lib/python3.5/site-packages/sklearn/svm/classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             self.loss, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"crammer_singer\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/dmc2019/venv/lib/python3.5/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    924\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_extended_pltrain_for_batch(testbatch_X_unscaled_df, pltrain_X_unscaled_df, pltrain_y_df, transformer):\n",
    "    \n",
    "    #TODO: Use KNN (neighbour in following dataframe: knnwithprobs_Xy_original_df) to get best classifier: also use transformer.inverse_transform\n",
    "    \n",
    "    # train a classificator on the pseudo labeled train (pltrain) dataset\n",
    "    clf = get_classifier('svc')\n",
    "    clf.fit(pltrain_X_originial_df, pltrain_y_original_df.fraud)\n",
    "    \n",
    "    # predict labels for batch\n",
    "    testbatch_y_original = clf.predict(testbatch_X_transformed_df)\n",
    "    testbatch_Xy_transandorig_df = testbatch_X_transformed_df.assign(fraud = testbatch_y_original)\n",
    "    \n",
    "    # add batch to pseudo labeled train (pltrain) dataset. needs to ignore index as ids in test also start with 0\n",
    "    pltrainnew_X_transformed_df = pltrain_X_transformed_df.append(testbatch_X_transformed_df, ignore_index=True)\n",
    "    pltrainnew_y_original_df = pltrain_y_original_df.append(testbatch_Xy_transandorig_df[['fraud']], ignore_index=True)\n",
    "    return pltrainnew_X_transformed_df, pltrainnew_y_original_df\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"total test size:\",len(test_X_transformed_df),\", with a batchsize of\",TEST_BATCH_SIZE,\" we will need\",int(np.ceil(len(test_X_transformed_df)/TEST_BATCH_SIZE)),\"iterations:\")\n",
    "\n",
    "#initialize pseudo labeled train (pltrain) dataset with the transformed training data\n",
    "pltrain_X_transformed_df = train_X_transformed_df.copy()\n",
    "pltrain_y_original_df = train_y_original_df.copy()\n",
    "\n",
    "\n",
    "# iterate through fixed-size batches\n",
    "for i in range(TEST_BATCH_SIZE, len(test_X_transformed_df), TEST_BATCH_SIZE):\n",
    "    if int(i/TEST_BATCH_SIZE) % ITER_PRINT_EVERY == 0:\n",
    "        print(\"iteration\",int(i/TEST_BATCH_SIZE),\"\\t/\",int(np.ceil(len(test_X_transformed_df)/TEST_BATCH_SIZE)),\"with batch from\",i-TEST_BATCH_SIZE,\"\\t to\", i,\", training with\",len(pltrain_y_original_df),\"samples\")\n",
    "    # get batch from test set\n",
    "    testbatch_X_transformed_df = test_X_transformed_df.iloc[i-TEST_BATCH_SIZE:i]\n",
    "    \n",
    "    # extend pseudo labeled train (pltrain) dataset by predicting the batch\n",
    "    pltrain_X_transformed_df, pltrain_y_original_df = get_extended_pltrain_for_batch(testbatch_X_transformed_df, pltrain_X_transformed_df, pltrain_y_original_df, transformer)\n",
    "    \n",
    "    #if i>len(df_test_X_transformed)-1000:\n",
    "    #    print(i)\n",
    "    #    display(df_test_X_transformed_batch.head(1))\n",
    "\n",
    "# use last few rows that cant fill up a complete batch as a smaller batch\n",
    "print(\"iteration\",int(i/TEST_BATCH_SIZE)+1,\"\\twith batch from\",i,\"\\t to\", len(test_X_transformed_df),\", training with\",len(pltrain_y_original_df),\"samples\")\n",
    "testbatch_X_transformed_df = test_X_transformed_df.iloc[i:len(test_X_transformed_df)]\n",
    "\n",
    "# extend pseudo labeled train (pltrain) dataset by predicting the small batch\n",
    "pltrain_X_transformed_df, pltrain_y_original_df = get_extended_pltrain_for_batch(testbatch_X_transformed_df, pltrain_X_transformed_df, pltrain_y_original_df, transformer)\n",
    "\n",
    "#combine x and y columns dataframes to one big dataframe\n",
    "pltrain_Xy_transandorig_df = pltrain_X_transformed_df.assign(fraud = pltrain_y_original_df.fraud.values)\n",
    "\n",
    "print(\"training with pseudo labeling completed, last iteration used\",len(pltrain_Xy_transandorig_df),\"samples.\")\n",
    "\n",
    "display(pltrain_Xy_transandorig_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.) pseudo-label the test set and create new classifier based on this\n",
    "first we predict the original test data labels using the new extended pltrain from above cell and second we use this test data labels to train a new classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a classificator on the pseudo labeled train (pltrain) dataset\n",
    "pltrain_clf = get_classifier('svc')\n",
    "pltrain_clf.fit(pltrain_X_transformed_df, pltrain_y_original_df.fraud)\n",
    "\n",
    "# predict labels for (transformed) original test set\n",
    "pltest_y_original = pltrain_clf.predict(test_X_transformed_df)\n",
    "\n",
    "# combine x and y columns dataframes to one big dataframe\n",
    "pltest_Xy_transandorig_df = test_X_transformed_df.assign(fraud = pltest_y_original)\n",
    "display(pltest_Xy_transandorig_df.head(1))\n",
    "\n",
    "# train a new classifier based on pltest\n",
    "pltest_clf = get_classifier('svc')\n",
    "pltest_clf.fit(test_X_transformed_df, pltest_y_original);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.) evaluate our new classifier with the original training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpred_y_original = pltest_clf.predict(train_X_transformed_df)\n",
    "calc_scores(train_y_original_df.fraud.values, trainpred_y_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.) combine the pseudo labeled test set with the original train data to train our final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--> already done in step 7\n",
    "final_clf = pltrain_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.) predict labels for the test set using our final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-->already done in step 7\n",
    "test_y_pred = pltest_y_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.) generate output file neeeded for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_y_pred, columns=[\"fraud\"]).to_csv(\"HS_Karlsruhe_1.csv\", index=False)\n",
    "pd.read_csv(\"HS_Karlsruhe_1.csv\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.) evaluate our new classifier with the validation set\n",
    "Now at the very end we can also test our final model on a validation split never used before. just for comparison. \n",
    "\n",
    "**For the final submission, the following code should will not be run and the full train (incl. this val split) set will be used above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FINAL_SUBMISSION:\n",
    "    val_Xy_original_df = pd.read_csv(\"../data/val_new.csv\", sep=\"|\")\n",
    "    valpred_y_original = final_clf.predict(transformer.transform(val_Xy_original_df.drop(\"fraud\", axis=1)))\n",
    "    print(calc_scores(val_Xy_original_df.fraud.values, valpred_y_original))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
