{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with the Test split?\n",
    "This is what we want to do:\n",
    "\n",
    "1. predict batches with ...\n",
    ".TODO: create list of content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.) set run-config and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_SUBMISSION = False # will perform a test on a validation split if set to False\n",
    "\n",
    "TEST_BATCH_SIZE = 50 # Number of Test entries to add to the training set for the next iteration\n",
    "ITER_PRINT_EVERY = 50 # Which Iterations to print (every nth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors, KDTree\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.) define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scores(y_test, y_pred):\n",
    "    y_test_tmp = y_test.copy()\n",
    "    accuracy = metrics.accuracy_score(y_test_tmp, y_pred)\n",
    "    confusion_matrix = (metrics.confusion_matrix(y_test_tmp, y_pred)).tolist()\n",
    "    dmc_score = np.sum(confusion_matrix * np.array([[0, -25], [-5, 5]]))\n",
    "    return accuracy, dmc_score, confusion_matrix\n",
    "\n",
    "def get_classifier(name):\n",
    "    return {\n",
    "            'xgb': XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "                colsample_bytree=1, disable_default_eval_metric=1,\n",
    "                eta=0.1769182150877735, eval_metric='aucpr',\n",
    "                gamma=1.8285912697052542, reg_lambda=0.4149772770711012,\n",
    "                learning_rate=0.1, max_bin=254, max_delta_step=7.2556696256684035,\n",
    "                max_depth=3, min_child_weight=1.0317712458399741, missing=None,\n",
    "                n_estimators=445, n_jobs=-1, objective='binary:logistic', random_state=0, reg_alpha=0,\n",
    "                scale_pos_weight=1,silent=True,\n",
    "                subsample=1, tree_method='gpu_hist', verbosity=2),\n",
    "            'svc': SVC(C=56.98164719395536, cache_size=8000, class_weight=None, coef0=0.0,  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
    "                kernel='linear', max_iter=-1, probability=True, random_state=None,\n",
    "                shrinking=True, tol=0.002848504943774676, verbose=0)}[name]\n",
    "\n",
    "def get_best_classifier_for_sample(idx, validation_set):\n",
    "    ground_truth = validation_set.iloc[idx].fraud\n",
    "    \n",
    "    # Both classifier predicted the calue correctly\n",
    "    if (validation_set.iloc[idx].lsvc_predict == ground_truth) and (validation_set.iloc[idx].xgb_predict == ground_truth):\n",
    "        if validation_set.iloc[idx].lsvc_proba > validation_set.iloc[idx].xgb_proba:\n",
    "            return \"lsvc\"\n",
    "        else:\n",
    "            return \"xgboost\"\n",
    "    # lsvc predicted correctly\n",
    "    elif (validation_set.iloc[idx].lsvc_predict == ground_truth) and (validation_set.iloc[idx].xgb_predict != ground_truth):\n",
    "        return \"lsvc\"\n",
    "    \n",
    "    # xgboost predicted correcltly\n",
    "    elif (validation_set.iloc[idx].lsvc_predict != ground_truth) and (validation_set.iloc[idx].xgb_predict == ground_truth):\n",
    "        return \"xgboost\"\n",
    "    \n",
    "    # If No classifier predicted the knn correct, None is returned\n",
    "    else: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Search Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNLookup():\n",
    "\n",
    "    def __init__(self, knn_data):\n",
    "\n",
    "        #self.knn = NearestNeighbors(n_neighbors=1)\n",
    "        #self.knn.fit(knn_data.values.tolist())\n",
    "\n",
    "        self.tree = KDTree(knn_data)\n",
    "\n",
    "    def refit(self, knn_data):\n",
    "        self.tree = KDTree(knn_data)\n",
    "\n",
    "    def find_nearest_neighbor(self, row_scaled, dataset_scaled):\n",
    "        diffs = [np.sum((row_scaled - ds_row) ** 2) for idx, ds_row in dataset_scaled.iterrows()]\n",
    "        idx = np.argmin(diffs)\n",
    "        # print(idx,\"----\", diffs[idx])\n",
    "        return idx, diffs[idx]\n",
    "\n",
    "    def find_nearest_neighbor2(self, row_scaled, dataset_scaled):\n",
    "        dist, ind = self.tree.query([row_scaled.values], k=1)\n",
    "        return np.ravel(ind)[0], np.ravel(dist)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.) Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainandknn_Xy_original_df = pd.read_csv(\"../data/train.csv\", sep=\"|\") if FINAL_SUBMISSION else pd.read_csv(\"../data/train_new.csv\", sep=\"|\")\n",
    "train_Xy_original_df, knn_Xy_original_df = train_test_split(trainandknn_Xy_original_df,train_size=0.75) # if FINAL_SUBMISSION else 0.8**2) #small\n",
    "test_X_original_df  = pd.read_csv(\"../data/test.csv\", sep=\"|\") #.iloc[0:301] #TODO: For faster testing we use less data from the test set\n",
    "\n",
    "#Only for test routines\n",
    "val_Xy_original_df = pd.read_csv(\"../data/val_new.csv\", sep=\"|\")\n",
    "train_complete_Xy_original_df = pd.read_csv(\"../data/train.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.) Prepare Input X and Label Y Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convention for variables names: datasetname_columntype_transformstatus_dataframeornot\n",
    "train_y_original_df = train_Xy_original_df[[\"fraud\"]].copy()\n",
    "train_X_original_df = train_Xy_original_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "knn_y_original_df = knn_Xy_original_df[[\"fraud\"]].copy()\n",
    "knn_X_original_df = knn_Xy_original_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "# Only for test routie#nes\n",
    "val_y_originial_df = val_Xy_original_df[[\"fraud\"]].copy()\n",
    "val_X_originial_df = val_Xy_original_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "train_complete_y_originial_df = train_complete_Xy_original_df[[\"fraud\"]].copy()\n",
    "train_complete_X_originial_df = train_complete_Xy_original_df.copy().drop(\"fraud\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.401809</td>\n",
       "      <td>932.153273</td>\n",
       "      <td>50.864492</td>\n",
       "      <td>5.469931</td>\n",
       "      <td>4.904204</td>\n",
       "      <td>2.525279</td>\n",
       "      <td>0.058138</td>\n",
       "      <td>0.201746</td>\n",
       "      <td>0.745404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.709404</td>\n",
       "      <td>530.144640</td>\n",
       "      <td>28.940202</td>\n",
       "      <td>3.451169</td>\n",
       "      <td>3.139697</td>\n",
       "      <td>1.695472</td>\n",
       "      <td>0.278512</td>\n",
       "      <td>1.242135</td>\n",
       "      <td>1.327241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>474.500000</td>\n",
       "      <td>25.965000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008384</td>\n",
       "      <td>0.027787</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>932.000000</td>\n",
       "      <td>51.210000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.016317</td>\n",
       "      <td>0.054498</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1397.000000</td>\n",
       "      <td>77.285000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.032594</td>\n",
       "      <td>0.107313</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1831.000000</td>\n",
       "      <td>99.960000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>37.870000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        trustLevel  totalScanTimeInSeconds   grandTotal  lineItemVoids  \\\n",
       "count  1879.000000             1879.000000  1879.000000    1879.000000   \n",
       "mean      3.401809              932.153273    50.864492       5.469931   \n",
       "std       1.709404              530.144640    28.940202       3.451169   \n",
       "min       1.000000                2.000000     0.010000       0.000000   \n",
       "25%       2.000000              474.500000    25.965000       2.000000   \n",
       "50%       3.000000              932.000000    51.210000       5.000000   \n",
       "75%       5.000000             1397.000000    77.285000       8.000000   \n",
       "max       6.000000             1831.000000    99.960000      11.000000   \n",
       "\n",
       "       scansWithoutRegistration  quantityModifications  \\\n",
       "count               1879.000000            1879.000000   \n",
       "mean                   4.904204               2.525279   \n",
       "std                    3.139697               1.695472   \n",
       "min                    0.000000               0.000000   \n",
       "25%                    2.000000               1.000000   \n",
       "50%                    5.000000               3.000000   \n",
       "75%                    8.000000               4.000000   \n",
       "max                   10.000000               5.000000   \n",
       "\n",
       "       scannedLineItemsPerSecond  valuePerSecond  lineItemVoidsPerPosition  \n",
       "count                1879.000000     1879.000000               1879.000000  \n",
       "mean                    0.058138        0.201746                  0.745404  \n",
       "std                     0.278512        1.242135                  1.327241  \n",
       "min                     0.000548        0.000007                  0.000000  \n",
       "25%                     0.008384        0.027787                  0.160000  \n",
       "50%                     0.016317        0.054498                  0.350000  \n",
       "75%                     0.032594        0.107313                  0.666667  \n",
       "max                     6.666667       37.870000                 11.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_complete_X_originial_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.) DataTransformer Class and data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer:\n",
    "    \"\"\"\n",
    "    for scaling, data transformations (new features, one-hot encoding, categorical, ...)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scaler):\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def fit_scaler(self, df):\n",
    "        df_tmp = df.copy()\n",
    "        self.scaler.fit(df_tmp.astype(np.float64))\n",
    "        return self\n",
    "        \n",
    "    def apply_scaler(self, df):\n",
    "        df_temp = df.copy()\n",
    "        return pd.DataFrame(self.scaler.transform(df_temp),df_temp.index, df_temp.columns)\n",
    "    \n",
    "    def inverse_scale(self, df):\n",
    "        df_tmp = df.copy()\n",
    "        return pd.DataFrame(self.scaler.inverse_transform(df_tmp), df_tmp.index, df_tmp.columns)\n",
    "    \n",
    "    def add_features(self,df):\n",
    "        #TODO: Choose relevant features\n",
    "        df_tmp = df.copy()\n",
    "        df_tmp['totalScannedLineItems'] = df_tmp['scannedLineItemsPerSecond'] * df_tmp['totalScanTimeInSeconds']\n",
    "        #df['avgTimePerScan'] = 1/ df['scannedLineItemsPerSecond']\n",
    "        #df['avgValuePerScan'] = df['avgTimePerScan'] * df['valuePerSecond']\n",
    "        #df['withoutRegisPerPosition'] = df['scansWithoutRegistration'] / df['totalScannedLineItems'] #equivalent to lineItemVoidsPerPosition?\n",
    "        #df['quantiModPerPosition'] = df['quantityModifications'] / df['totalScannedLineItems']\n",
    "        #df['lineItemVoidsPerTotal'] = df['lineItemVoids'] / df['grandTotal']\n",
    "        #df['withoutRegisPerTotal'] = df['scansWithoutRegistration'] / df['grandTotal']\n",
    "        #df['quantiModPerTotal'] = df['quantityModifications'] / df['grandTotal']\n",
    "        #df['lineItemVoidsPerTime'] = df['lineItemVoids'] / df['totalScanTimeInSeconds']\n",
    "        #df['withoutRegisPerTime'] = df['scansWithoutRegistration'] / df['totalScanTimeInSeconds']\n",
    "        #df['quantiModPerTime'] = df['quantityModifications'] / df['totalScanTimeInSeconds']\n",
    "        #df['valuePerScannedLineItem'] = df['valuePerSecond'] / df['scannedLineItemsPerSecond']\n",
    "        return df_tmp\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        All in one: Apply all transform methods\n",
    "            1.) addFeatures\n",
    "            2.) apply_scaler\n",
    "        \"\"\"\n",
    "        df_tmp = df.copy()\n",
    "        return self.apply_scaler(self.add_features(df_tmp))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>totalScannedLineItems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.254645</td>\n",
       "      <td>0.884888</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.206897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.548087</td>\n",
       "      <td>0.589959</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.023569</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  \\\n",
       "0         0.6                0.254645    0.884888       0.363636   \n",
       "1         0.4                0.548087    0.589959       0.636364   \n",
       "\n",
       "   scansWithoutRegistration  quantityModifications  scannedLineItemsPerSecond  \\\n",
       "0                       0.8                    0.8                   0.000481   \n",
       "1                       0.6                    0.2                   0.000878   \n",
       "\n",
       "   valuePerSecond  lineItemVoidsPerPosition  totalScannedLineItems  \n",
       "0        0.001900                  0.051948               0.206897  \n",
       "1        0.000589                  0.023569               0.896552  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "transformer = DataTransformer(scaler)\n",
    "\n",
    "# Adding new Features to train and test set\n",
    "train_X_unscaled_df = transformer.add_features(train_X_original_df)\n",
    "test_X_unscaled_df = transformer.add_features(test_X_original_df)\n",
    "knn_X_unscaled_df = transformer.add_features(knn_X_original_df)\n",
    "\n",
    "val_X_unscaled_df = transformer.add_features(val_X_originial_df)\n",
    "train_complete_X_unscaled_df = transformer.add_features(train_complete_X_originial_df) \n",
    "\n",
    "transformer.fit_scaler(transformer.add_features(train_complete_X_unscaled_df.append(test_X_unscaled_df, sort=False)))\n",
    "train_X_scaled_df = transformer.apply_scaler(train_X_unscaled_df)\n",
    "knn_X_scaled_df   = transformer.apply_scaler(knn_X_unscaled_df)\n",
    "\n",
    "\n",
    "test_X_scaled_df  = transformer.apply_scaler(test_X_unscaled_df)\n",
    "val_X_scaled_df = transformer.apply_scaler(val_X_unscaled_df)\n",
    "train_complete_X_scaled_df = transformer.apply_scaler(train_complete_X_unscaled_df)\n",
    "\n",
    "\n",
    "# labels\n",
    "train_y_df = train_y_original_df.copy()\n",
    "val_y_df = val_y_originial_df.copy()\n",
    "knn_y_df = knn_y_original_df.copy()\n",
    "train_complete_y_df = train_complete_y_originial_df.copy()\n",
    "\n",
    "test_X_scaled_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>totalScannedLineItems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.00000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1.879000e+03</td>\n",
       "      <td>1.879000e+03</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1.879000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.480362</td>\n",
       "      <td>0.508827</td>\n",
       "      <td>0.508696</td>\n",
       "      <td>0.497266</td>\n",
       "      <td>0.49042</td>\n",
       "      <td>0.505056</td>\n",
       "      <td>1.919748e-03</td>\n",
       "      <td>2.023325e-03</td>\n",
       "      <td>0.067764</td>\n",
       "      <td>4.959718e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.341881</td>\n",
       "      <td>0.289697</td>\n",
       "      <td>0.289431</td>\n",
       "      <td>0.313743</td>\n",
       "      <td>0.31397</td>\n",
       "      <td>0.339094</td>\n",
       "      <td>9.283904e-03</td>\n",
       "      <td>1.245747e-02</td>\n",
       "      <td>0.120658</td>\n",
       "      <td>3.002556e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.985311e-08</td>\n",
       "      <td>6.808611e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.081668e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.258743</td>\n",
       "      <td>0.259676</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.612577e-04</td>\n",
       "      <td>2.786768e-04</td>\n",
       "      <td>0.014545</td>\n",
       "      <td>2.413793e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.508743</td>\n",
       "      <td>0.512151</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>5.257051e-04</td>\n",
       "      <td>5.465632e-04</td>\n",
       "      <td>0.031818</td>\n",
       "      <td>4.827586e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.762842</td>\n",
       "      <td>0.772927</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.80000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.068281e-03</td>\n",
       "      <td>1.076256e-03</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>7.586207e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.222081e-01</td>\n",
       "      <td>3.798014e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        trustLevel  totalScanTimeInSeconds   grandTotal  lineItemVoids  \\\n",
       "count  1879.000000             1879.000000  1879.000000    1879.000000   \n",
       "mean      0.480362                0.508827     0.508696       0.497266   \n",
       "std       0.341881                0.289697     0.289431       0.313743   \n",
       "min       0.000000                0.000546     0.000100       0.000000   \n",
       "25%       0.200000                0.258743     0.259676       0.181818   \n",
       "50%       0.400000                0.508743     0.512151       0.454545   \n",
       "75%       0.800000                0.762842     0.772927       0.727273   \n",
       "max       1.000000                1.000000     0.999700       1.000000   \n",
       "\n",
       "       scansWithoutRegistration  quantityModifications  \\\n",
       "count                1879.00000            1879.000000   \n",
       "mean                    0.49042               0.505056   \n",
       "std                     0.31397               0.339094   \n",
       "min                     0.00000               0.000000   \n",
       "25%                     0.20000               0.200000   \n",
       "50%                     0.50000               0.600000   \n",
       "75%                     0.80000               0.800000   \n",
       "max                     1.00000               1.000000   \n",
       "\n",
       "       scannedLineItemsPerSecond  valuePerSecond  lineItemVoidsPerPosition  \\\n",
       "count               1.879000e+03    1.879000e+03               1879.000000   \n",
       "mean                1.919748e-03    2.023325e-03                  0.067764   \n",
       "std                 9.283904e-03    1.245747e-02                  0.120658   \n",
       "min                 5.985311e-08    6.808611e-08                  0.000000   \n",
       "25%                 2.612577e-04    2.786768e-04                  0.014545   \n",
       "50%                 5.257051e-04    5.465632e-04                  0.031818   \n",
       "75%                 1.068281e-03    1.076256e-03                  0.060606   \n",
       "max                 2.222081e-01    3.798014e-01                  1.000000   \n",
       "\n",
       "       totalScannedLineItems  \n",
       "count           1.879000e+03  \n",
       "mean            4.959718e-01  \n",
       "std             3.002556e-01  \n",
       "min             2.081668e-17  \n",
       "25%             2.413793e-01  \n",
       "50%             4.827586e-01  \n",
       "75%             7.586207e-01  \n",
       "max             1.000000e+00  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_complete_X_scaled_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name     | New Features | Scaled |\n",
    "|----------|--------------|--------|\n",
    "| orig     |      [ ]     |   [ ]  |\n",
    "| unscaled |      [X]     |   [ ]  |\n",
    "| scaled   |      [X]     |   [X]  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 1/2.) train normally with all available classifiers for classifying knn split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnwithprobs_Xy_df = knn_X_unscaled_df.copy()\n",
    "knnwithprobs_Xy_df['fraud'] = knn_y_df\n",
    "#TODO: save predict_proba to knnwithprobs_Xy_original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_routine(data_dict, data_transformer, knn_lookup):\n",
    "    pltrain_X_unscaled_df = data_dict['pltrain_X_unscaled_df']\n",
    "    pltrain_X_scaled_df = transformer.apply_scaler(pltrain_X_unscaled_df)\n",
    "    pltrain_y_df = data_dict['pltrain_y_df']\n",
    "\n",
    "    lsvc_pltrain = get_classifier(\"svc\")\n",
    "    xgb_pltrain = get_classifier(\"xgb\")\n",
    "\n",
    "    lsvc_pltrain.fit(pltrain_X_scaled_df.values, pltrain_y_df.values)\n",
    "    xgb_pltrain.fit(pltrain_X_unscaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "    # Predict on ValidationSet\n",
    "    lsvc_pred_val = lsvc_pltrain.predict(data_dict['val_X_scaled_df'].values)\n",
    "    xgb_pred_val = xgb_pltrain.predict(data_dict['val_X_unscaled_df'].values)\n",
    "\n",
    "    own_classifier_pred_val = classify(lsvc_pltrain, xgb_pltrain, data_dict['val_X_unscaled_df'],\n",
    "                                       data_dict['knnwithprob_Xy_unscaled_df'], data_transformer, knn_lookup)\n",
    "\n",
    "    lsvc_val_acc, lsvc_val_dmc, lsvc_val_conf_mat = calc_scores(data_dict['val_y_df'].values, lsvc_pred_val)\n",
    "    xgb_val_acc, xgb_val_dmc, xgb_val_conf_mat = calc_scores(data_dict['val_y_df'].values, xgb_pred_val)\n",
    "    own_classifier_val_acc, own_classifier_val_dmc, own_classifier_val_conf_mat = calc_scores(\n",
    "        data_dict['val_y_df'], own_classifier_pred_val)\n",
    "\n",
    "    lsvc_pltrain = get_classifier(\"svc\")\n",
    "    xgb_pltrain = get_classifier(\"xgb\")\n",
    "\n",
    "    lsvc_pltrain.fit(data_dict['pltrain_X_scaled_df'].values, data_dict['pltrain_y_df'].values)\n",
    "    xgb_pltrain.fit(data_dict['pltrain_X_unscaled_df'].values, data_dict['pltrain_y_df'].values)\n",
    "\n",
    "    # Predict on original full size (~1900 samples) just trained on test_data\n",
    "    lsvc_pred_train = lsvc_pltrain.predict(data_dict['train_complete_X_scaled_df'].values)\n",
    "    xgb_pred_train = xgb_pltrain.predict(data_dict['train_complete_X_unscaled_df'].values)\n",
    "    own_classifier_pred_train = classify(lsvc_pltrain, xgb_pltrain, data_dict['train_complete_X_unscaled_df'],\n",
    "                                         data_dict['knnwithprob_Xy_unscaled_df'], data_transformer, knn_lookup)\n",
    "\n",
    "\n",
    "    lsvc_train_acc, lsvc_train_dmc, lsvc_train_conf_mat = calc_scores(data_dict['train_complete_y_df'], lsvc_pred_train)\n",
    "    xgb_train_acc, xgb_train_dmc, xgb_train_conf_mat = calc_scores(data_dict['train_complete_y_df'], xgb_pred_train)\n",
    "    own_classifier_train_acc, own_classifier_train_dmc, own_classifier_train_conf_mat = calc_scores(\n",
    "        data_dict['train_complete_y_df'], own_classifier_pred_train)\n",
    "\n",
    "    results = {\"lin_svc\": {\n",
    "        \"val\": {\n",
    "            \"dmc_score\": lsvc_val_dmc,\n",
    "            \"conf_matrix\": lsvc_val_conf_mat\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"dmc_score\": lsvc_train_dmc,\n",
    "            \"conf_matrix\": lsvc_train_conf_mat\n",
    "        }\n",
    "    },\n",
    "        \"xgboost\": {\n",
    "            \"val\": {\n",
    "                \"dmc_score\": xgb_val_dmc,\n",
    "                \"conf_matrix\": xgb_val_conf_mat\n",
    "            },\n",
    "            \"train\": {\n",
    "                \"dmc_score\": xgb_train_dmc,\n",
    "                \"conf_matrix\": xgb_train_conf_mat\n",
    "            }\n",
    "        },\n",
    "        \"own_classifier\": {\n",
    "            \"val\": {\n",
    "                \"dmc_score\": own_classifier_val_dmc,\n",
    "                \"conf_matrix\": own_classifier_val_conf_mat\n",
    "            },\n",
    "            \"train\": {\n",
    "                \"dmc_score\": own_classifier_train_dmc,\n",
    "                \"conf_matrix\": own_classifier_train_conf_mat\n",
    "            }\n",
    "        },\n",
    "\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(xgboost_fitted, linear_svc_fitted, data_to_predict, data_knn_with_probs, transformer, knn_lookup):\n",
    "    prediction = []\n",
    "    # print(data_knn_with_probs.head())\n",
    "    data_knn_X_scaled = transformer.apply_scaler(\n",
    "        data_knn_with_probs.copy().drop(columns=[\"fraud\", \"xgb_predict\", \"xgb_proba\", \"lsvc_predict\", \"lsvc_proba\"]))\n",
    "    data_to_predict_scaled = transformer.apply_scaler(data_to_predict.copy())\n",
    "    data_to_predict_unscaled = data_to_predict.copy()\n",
    "\n",
    "    knn_time = []\n",
    "    pred_time = []\n",
    "\n",
    "    for i, row in data_to_predict_unscaled.iterrows():\n",
    "        if row.trustLevel >= 3:\n",
    "            prediction.append(0)\n",
    "\n",
    "        else:\n",
    "\n",
    "            start_knn = time.time()\n",
    "            idx_knn, distance_knn = knn_lookup.find_nearest_neighbor2(data_to_predict_scaled.iloc[i], data_knn_X_scaled)\n",
    "            knn_time.append(time.time() - start_knn)\n",
    "            # If distance to knn is to big, classify them directly\n",
    "            if distance_knn > 0.5:\n",
    "\n",
    "                pred_start = time.time()\n",
    "                xgb_pred = xgboost_fitted.predict([data_to_predict_unscaled.iloc[i].values])[0]\n",
    "                xgb_prob = max(np.ravel(xgboost_fitted.predict_proba([data_to_predict_unscaled.iloc[i].values])))\n",
    "\n",
    "                lsvc_pred = linear_svc_fitted.predict([data_to_predict_scaled.iloc[i]])[0]\n",
    "                lsvc_prob = max(np.ravel(linear_svc_fitted.predict_proba([data_to_predict_scaled.iloc[i].values])))\n",
    "\n",
    "                pred_time.append(time.time() - pred_start)\n",
    "                # If both classified them equal, take one of both\n",
    "                if xgb_pred == lsvc_pred:\n",
    "                    prediction.append(xgb_pred)\n",
    "\n",
    "                # if classification is not equal, take the one with higher probability\n",
    "                elif xgb_prob > lsvc_prob:\n",
    "                    prediction.append(xgb_pred)\n",
    "                else:\n",
    "                    prediction.append(lsvc_pred)\n",
    "\n",
    "            # If distance is smaller than 0.15, use knn    \n",
    "            else:\n",
    "                best_classifier = get_best_classifier_for_sample(idx_knn, data_knn_with_probs)\n",
    "                if best_classifier == \"xgboost\":\n",
    "                    pred_start = time.time()\n",
    "                    prediction.append(xgboost_fitted.predict([data_to_predict_unscaled.iloc[i].values])[0])\n",
    "                    pred_end = time.time()\n",
    "\n",
    "\n",
    "                elif best_classifier == \"lsvc\":\n",
    "                    pred_start = time.time()\n",
    "                    prediction.append(linear_svc_fitted.predict([data_to_predict_scaled.iloc[i].values])[0])\n",
    "                    pred_end = time.time()\n",
    "\n",
    "                elif best_classifier is None:\n",
    "                    # TODO What should we do in this case?\n",
    "                    prediction.append(0)\n",
    "                pred_time.append(pred_end - pred_start)\n",
    "    #print(\"Time for KNN Search: {}  Time For Prediction: {}\".format(np.sum(knn_time), np.sum(pred_time)))\n",
    "    return pd.DataFrame({\"fraud\": prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_supervised_learning_procedure(test_X_unscaled, train_X_unscaled, train_y, test_data_dict, transformer,\n",
    "                                       knn_Xy_df):\n",
    "    test_data_dict = test_data_dict\n",
    "\n",
    "    # initialize pseudo labeled train (pltrain) dataset with the transformed training data\n",
    "    pltrain_X_unscaled_df = train_X_unscaled.copy()\n",
    "    train_X_scaled_df = transformer.apply_scaler(train_X_unscaled)\n",
    "\n",
    "    pltrain_y_df = train_y.copy()\n",
    "    train_X_scaled_len = len(train_X_unscaled)\n",
    "    print(\"{} available train data before pseudo labeling\".format(train_X_scaled_len))\n",
    "\n",
    "    train_X_scaled_df = transformer.apply_scaler(train_X_unscaled)\n",
    "\n",
    "    linear_svc_initial = get_classifier('svc')\n",
    "    linear_svc_initial.fit(train_X_scaled_df.values, pltrain_y_df.values)\n",
    "    xgboost_initial = get_classifier('xgb')\n",
    "    xgboost_initial.fit(train_X_unscaled.values, pltrain_y_df.values)\n",
    "\n",
    "    knnwithprobs_X_unscaled_df = knn_Xy_df.copy().drop(columns=['fraud'])\n",
    "    knnwithprobs_X_scaled_df = transformer.apply_scaler(knnwithprobs_X_unscaled_df)\n",
    "    knn_lookup = KNNLookup(knnwithprobs_X_unscaled_df)\n",
    "\n",
    "    knnwithprobs_Xy_df['xgb_predict'] = xgboost_initial.predict(knnwithprobs_X_unscaled_df.values)\n",
    "    knnwithprobs_Xy_df['xgb_proba'] = [round(max(x), 3) for x in\n",
    "                                       xgboost_initial.predict_proba(knnwithprobs_X_unscaled_df.values)]\n",
    "    knnwithprobs_Xy_df['lsvc_predict'] = linear_svc_initial.predict(knnwithprobs_X_scaled_df.values)\n",
    "    knnwithprobs_Xy_df['lsvc_proba'] = [round(max(x), 3) for x in\n",
    "                                        linear_svc_initial.predict_proba(knnwithprobs_X_scaled_df.values)]\n",
    "\n",
    "    for i in range(TEST_BATCH_SIZE, len(test_X_unscaled), TEST_BATCH_SIZE):\n",
    "        if int(i / TEST_BATCH_SIZE) % ITER_PRINT_EVERY == 0:\n",
    "            print(\"iteration\", int(i / TEST_BATCH_SIZE), \"\\t/\", int(np.ceil(len(test_X_unscaled) / TEST_BATCH_SIZE)),\n",
    "                  \"with batch from\", i - TEST_BATCH_SIZE, \"\\t to\", i, \", training with\", len(pltrain_y_df), \"samples\")\n",
    "        # get batch from test set\n",
    "        testbatch_X_df = test_X_unscaled.iloc[i - TEST_BATCH_SIZE:i].copy().reset_index(drop=True)\n",
    "\n",
    "        # extend pseudo labeled train (pltrain) dataset by predicting the batch\n",
    "        pltrain_X_unscaled_df, pltrain_y_df = get_extended_pltrain_for_batch(testbatch_X_df, pltrain_X_unscaled_df,\n",
    "                                                                             pltrain_y_df, knnwithprobs_Xy_df,\n",
    "                                                                             transformer, knn_lookup)\n",
    "        pltrain_X_scaled_df = transformer.apply_scaler(pltrain_X_unscaled_df)\n",
    "        test_data_dict['pltrain_X_unscaled_df'] = pltrain_X_unscaled_df.copy()\n",
    "        test_data_dict['pltrain_X_scaled_df'] = pltrain_X_scaled_df.copy()\n",
    "        test_data_dict['pltrain_y_df'] = pltrain_y_df.copy()\n",
    "\n",
    "        linear_svc = get_classifier('svc')\n",
    "\n",
    "        # print(\"scaled X: \", len(pltrain_X_scaled_df),\"Y: \",  len(pltrain_y_df))\n",
    "        linear_svc.fit(pltrain_X_scaled_df, pltrain_y_df)\n",
    "\n",
    "        xgboost = get_classifier('xgb')\n",
    "        xgboost.fit(pltrain_X_unscaled_df, pltrain_y_df)\n",
    "\n",
    "        knnwithprobs_Xy_df_tmp = knnwithprobs_Xy_df.copy().drop(\n",
    "            columns=['fraud', 'xgb_predict', 'xgb_proba', 'lsvc_predict', 'lsvc_proba'])\n",
    "        knnwithprobs_Xy_df['xgb_predict'] = xgboost.predict(knnwithprobs_Xy_df_tmp)\n",
    "        knnwithprobs_Xy_df['xgb_proba'] = [round(max(x), 3) for x in xgboost.predict_proba(knnwithprobs_Xy_df_tmp)]\n",
    "        knnwithprobs_Xy_df['lsvc_predict'] = linear_svc.predict(transformer.apply_scaler(knnwithprobs_Xy_df_tmp))\n",
    "        knnwithprobs_Xy_df['lsvc_proba'] = [round(max(x), 3) for x in\n",
    "                                            linear_svc.predict_proba(transformer.apply_scaler(knnwithprobs_Xy_df_tmp))]\n",
    "        test_data_dict['knnwithprob_Xy_unscaled_df'] = knnwithprobs_Xy_df\n",
    "\n",
    "        print(\"Test Results: \", test_routine(data_dict, transformer, knn_lookup))\n",
    "\n",
    "    # use last few rows that cant fill up a complete batch as a smaller batch\n",
    "    print(\"iteration\", int(i / TEST_BATCH_SIZE) + 1, \"\\twith batch from\", i, \"\\t to\", len(test_X_unscaled),\n",
    "          \", training with\", len(pltrain_X_unscaled_df), \"samples\")\n",
    "    testbatch_X_transformed_df = test_X_unscaled.iloc[i:len(test_X_unscaled)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.) iterative model training using pseudo-labeling\n",
    "predict batches of the test set, add them to the previous training set and use this new training set to predict the next batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extended_pltrain_for_batch(testbatch_X_unscaled_df, pltrain_X_unscaled_df, pltrain_y_df,\n",
    "                                   knnwithprobs_Xy_unscaled, transformer, knn_lookup):\n",
    "    # train a classificator on the pseudo labeled train (pltrain) dataset\n",
    "    pltrain_X_scaled_df = transformer.apply_scaler(pltrain_X_unscaled_df)\n",
    "    linear_svc = get_classifier('svc')\n",
    "    linear_svc.fit(pltrain_X_scaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "    xgboost = get_classifier('xgb')\n",
    "    xgboost.fit(pltrain_X_unscaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "    testbatch_y = classify(xgboost, linear_svc, testbatch_X_unscaled_df, knnwithprobs_Xy_unscaled, transformer,\n",
    "                           knn_lookup)\n",
    "\n",
    "    # add batch to pseudo labeled train (pltrain) dataset. needs to ignore index as ids in test also start with 0\n",
    "    pltrainnew_X_unscaled_df = pltrain_X_unscaled_df.append(testbatch_X_unscaled_df, ignore_index=True)\n",
    "    pltrainnew_y_df = pltrain_y_df.append(testbatch_y,\n",
    "                                          ignore_index=True)  # pltrainnew_Xy_unscaled_df[['fraud']], ignore_index=True)\n",
    "    # print(\"Length pltainnew X: {} --- Length pltain y: {}\".format(len(pltrainnew_X_unscaled_df), pltrain_y_df.columns))\n",
    "    # print(pltrainnew_y_df.tail())\n",
    "    return pltrainnew_X_unscaled_df, pltrainnew_y_df\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1127 available train data before pseudo labeling\n",
      "Test Results:  {'xgboost': {'train': {'conf_matrix': [[1771, 4], [11, 93]], 'dmc_score': 310}, 'val': {'conf_matrix': [[351, 2], [7, 16]], 'dmc_score': -5}}, 'lin_svc': {'train': {'conf_matrix': [[1770, 5], [9, 95]], 'dmc_score': 305}, 'val': {'conf_matrix': [[352, 1], [4, 19]], 'dmc_score': 50}}, 'own_classifier': {'train': {'conf_matrix': [[1200, 575], [0, 104]], 'dmc_score': -13855}, 'val': {'conf_matrix': [[235, 118], [0, 23]], 'dmc_score': -2835}}}\n",
      "Test Results:  {'xgboost': {'train': {'conf_matrix': [[1771, 4], [11, 93]], 'dmc_score': 310}, 'val': {'conf_matrix': [[351, 2], [7, 16]], 'dmc_score': -5}}, 'lin_svc': {'train': {'conf_matrix': [[1772, 3], [8, 96]], 'dmc_score': 365}, 'val': {'conf_matrix': [[353, 0], [2, 21]], 'dmc_score': 95}}, 'own_classifier': {'train': {'conf_matrix': [[1200, 575], [0, 104]], 'dmc_score': -13855}, 'val': {'conf_matrix': [[235, 118], [0, 23]], 'dmc_score': -2835}}}\n",
      "Test Results:  {'xgboost': {'train': {'conf_matrix': [[1771, 4], [11, 93]], 'dmc_score': 310}, 'val': {'conf_matrix': [[351, 2], [6, 17]], 'dmc_score': 5}}, 'lin_svc': {'train': {'conf_matrix': [[1772, 3], [8, 96]], 'dmc_score': 365}, 'val': {'conf_matrix': [[353, 0], [2, 21]], 'dmc_score': 95}}, 'own_classifier': {'train': {'conf_matrix': [[1200, 575], [0, 104]], 'dmc_score': -13855}, 'val': {'conf_matrix': [[235, 118], [0, 23]], 'dmc_score': -2835}}}\n",
      "Test Results:  {'xgboost': {'train': {'conf_matrix': [[1772, 3], [11, 93]], 'dmc_score': 335}, 'val': {'conf_matrix': [[351, 2], [7, 16]], 'dmc_score': -5}}, 'lin_svc': {'train': {'conf_matrix': [[1769, 6], [8, 96]], 'dmc_score': 290}, 'val': {'conf_matrix': [[352, 1], [2, 21]], 'dmc_score': 70}}, 'own_classifier': {'train': {'conf_matrix': [[1200, 575], [0, 104]], 'dmc_score': -13855}, 'val': {'conf_matrix': [[235, 118], [0, 23]], 'dmc_score': -2835}}}\n",
      "Test Results:  {'xgboost': {'train': {'conf_matrix': [[1772, 3], [11, 93]], 'dmc_score': 335}, 'val': {'conf_matrix': [[351, 2], [7, 16]], 'dmc_score': -5}}, 'lin_svc': {'train': {'conf_matrix': [[1771, 4], [7, 97]], 'dmc_score': 350}, 'val': {'conf_matrix': [[353, 0], [2, 21]], 'dmc_score': 95}}, 'own_classifier': {'train': {'conf_matrix': [[1200, 575], [0, 104]], 'dmc_score': -13855}, 'val': {'conf_matrix': [[235, 118], [0, 23]], 'dmc_score': -2835}}}\n",
      "Test Results:  {'xgboost': {'train': {'conf_matrix': [[1771, 4], [12, 92]], 'dmc_score': 300}, 'val': {'conf_matrix': [[351, 2], [7, 16]], 'dmc_score': -5}}, 'lin_svc': {'train': {'conf_matrix': [[1769, 6], [7, 97]], 'dmc_score': 300}, 'val': {'conf_matrix': [[352, 1], [2, 21]], 'dmc_score': 70}}, 'own_classifier': {'train': {'conf_matrix': [[1200, 575], [0, 104]], 'dmc_score': -13855}, 'val': {'conf_matrix': [[235, 118], [0, 23]], 'dmc_score': -2835}}}\n",
      "Test Results:  {'xgboost': {'train': {'conf_matrix': [[1771, 4], [10, 94]], 'dmc_score': 320}, 'val': {'conf_matrix': [[351, 2], [6, 17]], 'dmc_score': 5}}, 'lin_svc': {'train': {'conf_matrix': [[1769, 6], [7, 97]], 'dmc_score': 300}, 'val': {'conf_matrix': [[352, 1], [2, 21]], 'dmc_score': 70}}, 'own_classifier': {'train': {'conf_matrix': [[1200, 575], [0, 104]], 'dmc_score': -13855}, 'val': {'conf_matrix': [[235, 118], [0, 23]], 'dmc_score': -2835}}}\n",
      "Test Results:  {'xgboost': {'train': {'conf_matrix': [[1772, 3], [10, 94]], 'dmc_score': 345}, 'val': {'conf_matrix': [[352, 1], [6, 17]], 'dmc_score': 30}}, 'lin_svc': {'train': {'conf_matrix': [[1769, 6], [7, 97]], 'dmc_score': 300}, 'val': {'conf_matrix': [[352, 1], [2, 21]], 'dmc_score': 70}}, 'own_classifier': {'train': {'conf_matrix': [[1200, 575], [0, 104]], 'dmc_score': -13855}, 'val': {'conf_matrix': [[235, 118], [0, 23]], 'dmc_score': -2835}}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6b98730efdf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_y_df'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_y_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msemi_supervised_learning_procedure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X_unscaled_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X_unscaled_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknnwithprobs_Xy_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-b882ca730bba>\u001b[0m in \u001b[0;36msemi_supervised_learning_procedure\u001b[0;34m(test_X_unscaled, train_X_unscaled, train_y, test_data_dict, transformer, knn_Xy_df)\u001b[0m\n\u001b[1;32m     39\u001b[0m         pltrain_X_unscaled_df, pltrain_y_df = get_extended_pltrain_for_batch(testbatch_X_df, pltrain_X_unscaled_df,\n\u001b[1;32m     40\u001b[0m                                                                              \u001b[0mpltrain_y_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknnwithprobs_Xy_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                                                                              transformer, knn_lookup)\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mpltrain_X_scaled_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_scaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpltrain_X_unscaled_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mtest_data_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pltrain_X_unscaled_df'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpltrain_X_unscaled_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-682999846a63>\u001b[0m in \u001b[0;36mget_extended_pltrain_for_batch\u001b[0;34m(testbatch_X_unscaled_df, pltrain_X_unscaled_df, pltrain_y_df, knnwithprobs_Xy_unscaled, transformer, knn_lookup)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mxgboost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xgb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mxgboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpltrain_X_unscaled_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpltrain_y_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     testbatch_y = classify(xgboost, linear_svc, testbatch_X_unscaled_df, knnwithprobs_Xy_unscaled, transformer,\n",
      "\u001b[0;32m~/Projects/dmc2019/venv/lib/python3.5/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    711\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/dmc2019/venv/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/dmc2019/venv/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrabit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_world_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mrabit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mnboost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/dmc2019/venv/lib/python3.5/site-packages/xgboost/rabit.py\u001b[0m in \u001b[0;36mget_world_size\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mTotal\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRabitGetWorldSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_dict = dict()\n",
    "data_dict['train_complete_X_unscaled_df'] = train_complete_X_unscaled_df.copy()\n",
    "data_dict['train_complete_X_scaled_df'] = transformer.apply_scaler(train_complete_X_unscaled_df)\n",
    "data_dict['train_complete_y_df'] = train_complete_y_df.copy()\n",
    "\n",
    "data_dict['val_X_unscaled_df'] = val_X_unscaled_df.copy()\n",
    "data_dict['val_X_scaled_df'] = transformer.apply_scaler(val_X_unscaled_df)\n",
    "data_dict['val_y_df'] = val_y_df.copy()\n",
    "\n",
    "semi_supervised_learning_procedure(test_X_unscaled_df, train_X_unscaled_df, train_y_df, data_dict, transformer, knnwithprobs_Xy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_complete_X_unscaled_df.copy().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.) pseudo-label the test set and create new classifier based on this\n",
    "first we predict the original test data labels using the new extended pltrain from above cell and second we use this test data labels to train a new classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.) evaluate our new classifier with the original training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.) combine the pseudo labeled test set with the original train data to train our final classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.) predict labels for the test set using our final classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.) generate output file neeeded for submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.) evaluate our new classifier with the validation set\n",
    "Now at the very end we can also test our final model on a validation split never used before. just for comparison. \n",
    "\n",
    "**For the final submission, the following code should will not be run and the full train (incl. this val split) set will be used above**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
