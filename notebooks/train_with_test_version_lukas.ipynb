{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with the Test split?\n",
    "This is what we want to do:\n",
    "\n",
    "1. predict batches with ...\n",
    ".TODO: create list of content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.) set run-config and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_SUBMISSION = False # will perform a test on a validation split if set to False\n",
    "\n",
    "TEST_BATCH_SIZE = 250 # Number of Test entries to add to the training set for the next iteration\n",
    "ITER_PRINT_EVERY = 2 # Which Iterations to print (every nth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors, KDTree\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.) define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scores(y_test, y_pred):\n",
    "    y_test_tmp = y_test.copy()\n",
    "    accuracy = metrics.accuracy_score(y_test_tmp, y_pred)\n",
    "    confusion_matrix = (metrics.confusion_matrix(y_test_tmp, y_pred)).tolist()\n",
    "    dmc_score = np.sum(confusion_matrix * np.array([[0, -25], [-5, 5]]))\n",
    "    return accuracy, dmc_score, confusion_matrix\n",
    "\n",
    "def get_classifier(name):\n",
    "    return {\n",
    "            'xgb': XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "                colsample_bytree=1, disable_default_eval_metric=1,\n",
    "                eta=0.1769182150877735, eval_metric='aucpr',\n",
    "                gamma=1.8285912697052542, reg_lambda=0.4149772770711012,\n",
    "                learning_rate=0.1, max_bin=254, max_delta_step=7.2556696256684035,\n",
    "                max_depth=3, min_child_weight=1.0317712458399741, missing=None,\n",
    "                n_estimators=445, n_jobs=-1, objective='binary:logistic', random_state=0, reg_alpha=0,\n",
    "                scale_pos_weight=1,silent=True,\n",
    "                subsample=1, tree_method='gpu_hist', verbosity=2, seed=42),\n",
    "            'svc': SVC(C=56.98164719395536, cache_size=8000, class_weight=None, coef0=0.0,  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
    "                kernel='linear', max_iter=-1, probability=True, random_state=None,\n",
    "                shrinking=True, tol=0.002848504943774676, verbose=0)}[name]\n",
    "\n",
    "def get_best_classifier_for_sample(idx, validation_set):\n",
    "    ground_truth = validation_set.iloc[idx].fraud\n",
    "    \n",
    "    # Both classifier predicted the calue correctly\n",
    "    if (validation_set.iloc[idx].lsvc_predict == ground_truth) and (validation_set.iloc[idx].xgb_predict == ground_truth):\n",
    "        if validation_set.iloc[idx].lsvc_proba > validation_set.iloc[idx].xgb_proba:\n",
    "            return \"lsvc\"\n",
    "        else:\n",
    "            return \"xgboost\"\n",
    "    # lsvc predicted correctly\n",
    "    elif (validation_set.iloc[idx].lsvc_predict == ground_truth) and (validation_set.iloc[idx].xgb_predict != ground_truth):\n",
    "        return \"lsvc\"\n",
    "    \n",
    "    # xgboost predicted correcltly\n",
    "    elif (validation_set.iloc[idx].lsvc_predict != ground_truth) and (validation_set.iloc[idx].xgb_predict == ground_truth):\n",
    "        return \"xgboost\"\n",
    "    \n",
    "    # If No classifier predicted the knn correct, None is returned\n",
    "    else: \n",
    "        if validation_set.iloc[idx].xgb_predict == 0:\n",
    "            return 1\n",
    "        else: return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Search Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNLookup():\n",
    "\n",
    "    def __init__(self, knn_data):\n",
    "\n",
    "        #self.knn = NearestNeighbors(n_neighbors=1)\n",
    "        #self.knn.fit(knn_data.values.tolist())\n",
    "\n",
    "        self.tree = KDTree(knn_data)\n",
    "\n",
    "    def refit(self, knn_data):\n",
    "        self.tree = KDTree(knn_data)\n",
    "\n",
    "    def find_nearest_neighbor(self, row_scaled, dataset_scaled):\n",
    "        diffs = [np.sum((row_scaled - ds_row) ** 2) for idx, ds_row in dataset_scaled.iterrows()]\n",
    "        idx = np.argmin(diffs)\n",
    "        return idx, diffs[idx]\n",
    "\n",
    "    def find_nearest_neighbor2(self, row_scaled, dataset_scaled):\n",
    "        dist, ind = self.tree.query([row_scaled.values], k=1)\n",
    "        return np.ravel(ind)[0], np.ravel(dist)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.) Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainandknn_Xy_original_df = pd.read_csv(\"../data/train.csv\", sep=\"|\") if FINAL_SUBMISSION else pd.read_csv(\"../data/train_new.csv\", sep=\"|\")\n",
    "train_Xy_original_df, knn_Xy_original_df = train_test_split(trainandknn_Xy_original_df,train_size=0.75) # if FINAL_SUBMISSION else 0.8**2) #small\n",
    "test_X_original_df  = pd.read_csv(\"../data/test.csv\", sep=\"|\") .iloc[0:5000] #TODO: For faster testing we use less data from the test set\n",
    "test_final_X_df = pd.read_csv(\"../data/test.csv\", sep=\"|\")\n",
    "\n",
    "#Only for test routines\n",
    "val_Xy_original_df = pd.read_csv(\"../data/val_new.csv\", sep=\"|\")\n",
    "train_complete_Xy_original_df = pd.read_csv(\"../data/train.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.) Prepare Input X and Label Y Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convention for variables names: datasetname_columntype_transformstatus_dataframeornot\n",
    "train_y_original_df = train_Xy_original_df[[\"fraud\"]].copy()\n",
    "train_X_original_df = train_Xy_original_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "knn_y_original_df = knn_Xy_original_df[[\"fraud\"]].copy()\n",
    "knn_X_original_df = knn_Xy_original_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "# Only for test routie#nes\n",
    "val_y_originial_df = val_Xy_original_df[[\"fraud\"]].copy()\n",
    "val_X_originial_df = val_Xy_original_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "train_complete_y_originial_df = train_complete_Xy_original_df[[\"fraud\"]].copy()\n",
    "train_complete_X_originial_df = train_complete_Xy_original_df.copy().drop(\"fraud\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.) DataTransformer Class and data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer:\n",
    "    \"\"\"\n",
    "    for scaling, data transformations (new features, one-hot encoding, categorical, ...)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scaler):\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def fit_scaler(self, df):\n",
    "        df_tmp = df.copy()\n",
    "        self.scaler.fit(df_tmp.astype(np.float64))\n",
    "        return self\n",
    "        \n",
    "    def apply_scaler(self, df):\n",
    "        df_temp = df.copy()\n",
    "        return pd.DataFrame(self.scaler.transform(df_temp),df_temp.index, df_temp.columns)\n",
    "    \n",
    "    def inverse_scale(self, df):\n",
    "        df_tmp = df.copy()\n",
    "        return pd.DataFrame(self.scaler.inverse_transform(df_tmp), df_tmp.index, df_tmp.columns)\n",
    "    \n",
    "    def add_features(self,df):\n",
    "        #TODO: Choose relevant features\n",
    "        df_tmp = df.copy()\n",
    "        df_tmp['totalScannedLineItems'] = df_tmp['scannedLineItemsPerSecond'] * df_tmp['totalScanTimeInSeconds']\n",
    "        #df['avgTimePerScan'] = 1/ df['scannedLineItemsPerSecond']\n",
    "        #df['avgValuePerScan'] = df['avgTimePerScan'] * df['valuePerSecond']\n",
    "        #df['withoutRegisPerPosition'] = df['scansWithoutRegistration'] / df['totalScannedLineItems'] #equivalent to lineItemVoidsPerPosition?\n",
    "        #df['quantiModPerPosition'] = df['quantityModifications'] / df['totalScannedLineItems']\n",
    "        #df['lineItemVoidsPerTotal'] = df['lineItemVoids'] / df['grandTotal']\n",
    "        #df['withoutRegisPerTotal'] = df['scansWithoutRegistration'] / df['grandTotal']\n",
    "        #df['quantiModPerTotal'] = df['quantityModifications'] / df['grandTotal']\n",
    "        #df['lineItemVoidsPerTime'] = df['lineItemVoids'] / df['totalScanTimeInSeconds']\n",
    "        #df['withoutRegisPerTime'] = df['scansWithoutRegistration'] / df['totalScanTimeInSeconds']\n",
    "        #df['quantiModPerTime'] = df['quantityModifications'] / df['totalScanTimeInSeconds']\n",
    "        #df['valuePerScannedLineItem'] = df['valuePerSecond'] / df['scannedLineItemsPerSecond']\n",
    "        return df_tmp\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        All in one: Apply all transform methods\n",
    "            1.) addFeatures\n",
    "            2.) apply_scaler\n",
    "        \"\"\"\n",
    "        df_tmp = df.copy()\n",
    "        return self.apply_scaler(self.add_features(df_tmp))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>totalScannedLineItems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.309503</td>\n",
       "      <td>-0.859188</td>\n",
       "      <td>1.325981</td>\n",
       "      <td>-0.430787</td>\n",
       "      <td>0.955184</td>\n",
       "      <td>0.867967</td>\n",
       "      <td>-0.087878</td>\n",
       "      <td>-0.021945</td>\n",
       "      <td>-0.121216</td>\n",
       "      <td>-0.981527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.283553</td>\n",
       "      <td>0.160516</td>\n",
       "      <td>0.303538</td>\n",
       "      <td>0.438411</td>\n",
       "      <td>0.319350</td>\n",
       "      <td>-0.891584</td>\n",
       "      <td>-0.070574</td>\n",
       "      <td>-0.096731</td>\n",
       "      <td>-0.354895</td>\n",
       "      <td>1.335418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  \\\n",
       "0    0.309503               -0.859188    1.325981      -0.430787   \n",
       "1   -0.283553                0.160516    0.303538       0.438411   \n",
       "\n",
       "   scansWithoutRegistration  quantityModifications  scannedLineItemsPerSecond  \\\n",
       "0                  0.955184               0.867967                  -0.087878   \n",
       "1                  0.319350              -0.891584                  -0.070574   \n",
       "\n",
       "   valuePerSecond  lineItemVoidsPerPosition  totalScannedLineItems  \n",
       "0       -0.021945                 -0.121216              -0.981527  \n",
       "1       -0.096731                 -0.354895               1.335418  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "transformer = DataTransformer(scaler)\n",
    "\n",
    "# Adding new Features to train and test set\n",
    "train_X_unscaled_df = transformer.add_features(train_X_original_df)\n",
    "test_X_unscaled_df = transformer.add_features(test_X_original_df)\n",
    "knn_X_unscaled_df = transformer.add_features(knn_X_original_df)\n",
    "\n",
    "val_X_unscaled_df = transformer.add_features(val_X_originial_df)\n",
    "train_complete_X_unscaled_df = transformer.add_features(train_complete_X_originial_df) \n",
    "\n",
    "transformer.fit_scaler(transformer.add_features(train_complete_X_unscaled_df.append(test_X_unscaled_df, sort=False)))\n",
    "train_X_scaled_df = transformer.apply_scaler(train_X_unscaled_df)\n",
    "knn_X_scaled_df   = transformer.apply_scaler(knn_X_unscaled_df)\n",
    "\n",
    "\n",
    "test_X_scaled_df  = transformer.apply_scaler(test_X_unscaled_df)\n",
    "val_X_scaled_df = transformer.apply_scaler(val_X_unscaled_df)\n",
    "train_complete_X_scaled_df = transformer.apply_scaler(train_complete_X_unscaled_df)\n",
    "\n",
    "\n",
    "# labels\n",
    "train_y_df = train_y_original_df.copy()\n",
    "val_y_df = val_y_originial_df.copy()\n",
    "knn_y_df = knn_y_original_df.copy()\n",
    "train_complete_y_df = train_complete_y_originial_df.copy()\n",
    "\n",
    "test_final_X_df = transformer.add_features(test_final_X_df)\n",
    "\n",
    "\n",
    "test_X_scaled_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name     | New Features | Scaled |\n",
    "|----------|--------------|--------|\n",
    "| orig     |      [ ]     |   [ ]  |\n",
    "| unscaled |      [X]     |   [ ]  |\n",
    "| scaled   |      [X]     |   [X]  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 1/2.) train normally with all available classifiers for classifying knn split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnwithprobs_Xy_df = knn_X_unscaled_df.copy()\n",
    "knnwithprobs_Xy_df['fraud'] = knn_y_df\n",
    "#TODO: save predict_proba to knnwithprobs_Xy_original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_routine(data_dict, data_transformer, knn_lookup):\n",
    "    pltrain_X_unscaled_df = data_dict['pltrain_X_unscaled_df']\n",
    "    pltrain_X_scaled_df = transformer.apply_scaler(pltrain_X_unscaled_df)\n",
    "    pltrain_y_df = data_dict['pltrain_y_df']\n",
    "\n",
    "    lsvc_pltrain = get_classifier(\"svc\")\n",
    "    xgb_pltrain = get_classifier(\"xgb\")\n",
    "\n",
    "    lsvc_pltrain.fit(pltrain_X_scaled_df.values, pltrain_y_df.values)\n",
    "    xgb_pltrain.fit(pltrain_X_unscaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "    # Predict on ValidationSet\n",
    "    lsvc_pred_val = lsvc_pltrain.predict(data_dict['val_X_scaled_df'].values)\n",
    "    xgb_pred_val = xgb_pltrain.predict(data_dict['val_X_unscaled_df'].values)\n",
    "\n",
    "    own_classifier_pred_val = classify(xgb_pltrain, lsvc_pltrain,  data_dict['val_X_unscaled_df'],\n",
    "                                       data_dict['knnwithprob_Xy_unscaled_df'], data_transformer, knn_lookup)\n",
    "\n",
    "    lsvc_val_acc, lsvc_val_dmc, lsvc_val_conf_mat = calc_scores(data_dict['val_y_df'].values, lsvc_pred_val)\n",
    "    xgb_val_acc, xgb_val_dmc, xgb_val_conf_mat = calc_scores(data_dict['val_y_df'].values, xgb_pred_val)\n",
    "    own_classifier_val_acc, own_classifier_val_dmc, own_classifier_val_conf_mat = calc_scores(\n",
    "        data_dict['val_y_df'], own_classifier_pred_val)\n",
    "\n",
    "    lsvc_pltrain = get_classifier(\"svc\")\n",
    "    xgb_pltrain = get_classifier(\"xgb\")\n",
    "\n",
    "    lsvc_pltrain.fit(data_dict['test_X_scaled_df'].values, data_dict['test_y_df'].values)\n",
    "    xgb_pltrain.fit(data_dict['test_X_unscaled_df'].values, data_dict['test_y_df'].values)\n",
    "\n",
    "    # Predict on original full size (~1900 samples) just trained on test_data\n",
    "    lsvc_pred_train = lsvc_pltrain.predict(data_dict['train_complete_X_scaled_df'].values)\n",
    "    xgb_pred_train = xgb_pltrain.predict(data_dict['train_complete_X_unscaled_df'].values)\n",
    "    own_classifier_pred_train = classify(xgb_pltrain,lsvc_pltrain, data_dict['train_complete_X_unscaled_df'],\n",
    "                                         data_dict['knnwithprob_Xy_unscaled_df'], data_transformer, knn_lookup)\n",
    "\n",
    "\n",
    "    lsvc_train_acc, lsvc_train_dmc, lsvc_train_conf_mat = calc_scores(data_dict['train_complete_y_df'], lsvc_pred_train)\n",
    "    xgb_train_acc, xgb_train_dmc, xgb_train_conf_mat = calc_scores(data_dict['train_complete_y_df'], xgb_pred_train)\n",
    "    own_classifier_train_acc, own_classifier_train_dmc, own_classifier_train_conf_mat = calc_scores(\n",
    "        data_dict['train_complete_y_df'], own_classifier_pred_train)\n",
    "\n",
    "    results = {\"lin_svc\": {\n",
    "        \"val\": {\n",
    "            \"dmc_score\": lsvc_val_dmc,\n",
    "            \"conf_matrix\": lsvc_val_conf_mat\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"dmc_score\": lsvc_train_dmc,\n",
    "            \"conf_matrix\": lsvc_train_conf_mat\n",
    "        }\n",
    "    },\n",
    "        \"xgboost\": {\n",
    "            \"val\": {\n",
    "                \"dmc_score\": xgb_val_dmc,\n",
    "                \"conf_matrix\": xgb_val_conf_mat\n",
    "            },\n",
    "            \"train\": {\n",
    "                \"dmc_score\": xgb_train_dmc,\n",
    "                \"conf_matrix\": xgb_train_conf_mat\n",
    "            }\n",
    "        },\n",
    "        \"own_classifier\": {\n",
    "            \"val\": {\n",
    "                \"dmc_score\": own_classifier_val_dmc,\n",
    "                \"conf_matrix\": own_classifier_val_conf_mat\n",
    "            },\n",
    "            \"train\": {\n",
    "                \"dmc_score\": own_classifier_train_dmc,\n",
    "                \"conf_matrix\": own_classifier_train_conf_mat\n",
    "            }\n",
    "        },\n",
    "\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(xgboost_fitted, linear_svc_fitted, data_to_predict, data_knn_with_probs, transformer, knn_lookup):\n",
    "    prediction = []\n",
    "    data_knn_X_scaled = transformer.apply_scaler(\n",
    "        data_knn_with_probs.copy().drop(columns=[\"fraud\", \"xgb_predict\", \"xgb_proba\", \"lsvc_predict\", \"lsvc_proba\"]))\n",
    "    data_to_predict_scaled = transformer.apply_scaler(data_to_predict.copy())\n",
    "    data_to_predict_unscaled = data_to_predict.copy()\n",
    "\n",
    "    knn_time = []\n",
    "    pred_time = []\n",
    "    tolerance = 0.5\n",
    "    \n",
    "    #Check which scaler was used for preprocessing\n",
    "    if str(type(transformer.scaler)) == \"<class 'sklearn.preprocessing.data.StandardScaler'>\":\n",
    "        tolerance = 1.73\n",
    "    \n",
    "    \n",
    "    for i, row in data_to_predict_unscaled.iterrows():\n",
    "        if row.trustLevel >= 3:\n",
    "            prediction.append(0)\n",
    "\n",
    "        else:\n",
    "\n",
    "            start_knn = time.time()\n",
    "            idx_knn, distance_knn = knn_lookup.find_nearest_neighbor2(data_to_predict_scaled.iloc[i], data_knn_X_scaled)\n",
    "            knn_time.append(time.time() - start_knn)\n",
    "            # If distance to knn is to big, classify them directly\n",
    "            if distance_knn > 0.5:\n",
    "\n",
    "                pred_start = time.time()\n",
    "                xgb_pred = xgboost_fitted.predict([data_to_predict_unscaled.iloc[i].values])[0]\n",
    "                xgb_prob = max(np.ravel(xgboost_fitted.predict_proba([data_to_predict_unscaled.iloc[i].values])))\n",
    "\n",
    "                lsvc_pred = linear_svc_fitted.predict([data_to_predict_scaled.iloc[i]])[0]\n",
    "                lsvc_prob = max(np.ravel(linear_svc_fitted.predict_proba([data_to_predict_scaled.iloc[i].values])))\n",
    "\n",
    "                pred_time.append(time.time() - pred_start)\n",
    "                # If both classified them equal, take one of both\n",
    "                if xgb_pred == lsvc_pred:\n",
    "                    prediction.append(xgb_pred)\n",
    "\n",
    "                # if classification is not equal, take the one with higher probability\n",
    "                elif xgb_prob > lsvc_prob:\n",
    "                    prediction.append(xgb_pred)\n",
    "                else:\n",
    "                    prediction.append(lsvc_pred)\n",
    "\n",
    "            # If distance is smaller than 0.15, use knn    \n",
    "            else:\n",
    "                best_classifier = get_best_classifier_for_sample(idx_knn, data_knn_with_probs)\n",
    "                if isinstance(best_classifier, int):\n",
    "                    prediction.append(best_classifier)\n",
    "                    \n",
    "                elif best_classifier == \"xgboost\":\n",
    "                    pred_start = time.time()\n",
    "                    prediction.append(xgboost_fitted.predict([data_to_predict_unscaled.iloc[i].values])[0])\n",
    "                    pred_end = time.time()\n",
    "\n",
    "\n",
    "                elif best_classifier == \"lsvc\":\n",
    "                    pred_start = time.time()\n",
    "                    prediction.append(linear_svc_fitted.predict([data_to_predict_scaled.iloc[i].values])[0])\n",
    "                    pred_end = time.time()\n",
    "\n",
    "                elif best_classifier is None:\n",
    "                    prediction.append(0)\n",
    "                pred_time.append(pred_end - pred_start)\n",
    "    return pd.DataFrame({\"fraud\": prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_supervised_learning_procedure(test_X_unscaled, train_X_unscaled, train_y, test_data_dict, transformer):\n",
    "    test_data_dict = test_data_dict\n",
    "\n",
    "    # initialize pseudo labeled train (pltrain) dataset with the transformed training data\n",
    "    pltrain_X_unscaled_df = train_X_unscaled.copy()\n",
    "    train_X_scaled_df = transformer.apply_scaler(train_X_unscaled)\n",
    "\n",
    "    pltrain_y_df = train_y.copy()\n",
    "    train_X_scaled_len = len(train_X_unscaled)\n",
    "    print(\"{} available train data before pseudo labeling\".format(train_X_scaled_len))\n",
    "\n",
    "    train_X_scaled_df = transformer.apply_scaler(train_X_unscaled)\n",
    "\n",
    "    linear_svc_initial = get_classifier('svc')\n",
    "    linear_svc_initial.fit(train_X_scaled_df.values, pltrain_y_df.values)\n",
    "    xgboost_initial = get_classifier('xgb')\n",
    "    xgboost_initial.fit(train_X_unscaled.values, pltrain_y_df.values)\n",
    "    \n",
    "    knnwithprob_Xy_unscaled_df = test_data_dict['knnwithprob_Xy_unscaled_df']\n",
    "    knnwithprobs_X_unscaled_df = knnwithprob_Xy_unscaled_df.copy().drop(columns=['fraud'])\n",
    "    knnwithprobs_X_scaled_df = transformer.apply_scaler(knnwithprobs_X_unscaled_df.copy())\n",
    "    knn_lookup = KNNLookup(knnwithprobs_X_unscaled_df)\n",
    "    \n",
    "    knnwithprob_Xy_unscaled_df['xgb_predict'] = xgboost_initial.predict(knnwithprobs_X_unscaled_df.values)\n",
    "    knnwithprob_Xy_unscaled_df['xgb_proba'] = [round(max(x), 3) for x in\n",
    "                                       xgboost_initial.predict_proba(knnwithprobs_X_unscaled_df.values)]\n",
    "    knnwithprob_Xy_unscaled_df['lsvc_predict'] = linear_svc_initial.predict(knnwithprobs_X_scaled_df.values)\n",
    "    knnwithprob_Xy_unscaled_df['lsvc_proba'] = [round(max(x), 3) for x in\n",
    "                                        linear_svc_initial.predict_proba(knnwithprobs_X_scaled_df.values)]\n",
    "    results = []\n",
    "    for i in range(TEST_BATCH_SIZE, len(test_X_unscaled), TEST_BATCH_SIZE):\n",
    "        if int(i / TEST_BATCH_SIZE) % ITER_PRINT_EVERY == 0:\n",
    "            print(\"iteration\", int(i / TEST_BATCH_SIZE), \"\\t/\", int(np.ceil(len(test_X_unscaled) / TEST_BATCH_SIZE)),\n",
    "                  \"with batch from\", i - TEST_BATCH_SIZE, \"\\t to\", i, \", training with\", len(pltrain_y_df), \"samples\")\n",
    "        # get batch from test set\n",
    "        testbatch_X_df = test_X_unscaled.iloc[i - TEST_BATCH_SIZE:i].copy().reset_index(drop=True)\n",
    "\n",
    "        # extend pseudo labeled train (pltrain) dataset by predicting the batch\n",
    "        pltrain_X_unscaled_df, pltrain_y_df = get_extended_pltrain_for_batch(testbatch_X_df, pltrain_X_unscaled_df,\n",
    "                                                                             pltrain_y_df, knnwithprob_Xy_unscaled_df,\n",
    "                                                                             transformer, knn_lookup)\n",
    "        pltrain_X_scaled_df = transformer.apply_scaler(pltrain_X_unscaled_df)\n",
    "        test_data_dict['pltrain_X_unscaled_df'] = pltrain_X_unscaled_df.copy()\n",
    "        test_data_dict['pltrain_X_scaled_df'] = pltrain_X_scaled_df.copy()\n",
    "        test_data_dict['pltrain_y_df'] = pltrain_y_df.copy()\n",
    "\n",
    "        linear_svc = get_classifier('svc')\n",
    "\n",
    "        linear_svc.fit(pltrain_X_scaled_df, pltrain_y_df)\n",
    "\n",
    "        xgboost = get_classifier('xgb')\n",
    "        xgboost.fit(pltrain_X_unscaled_df, pltrain_y_df)\n",
    "\n",
    "        knnwithprob_Xy_unscaled_df_tmp = knnwithprob_Xy_unscaled_df.copy().drop(\n",
    "            columns=['fraud', 'xgb_predict', 'xgb_proba', 'lsvc_predict', 'lsvc_proba'])\n",
    "        \n",
    "        knnwithprob_Xy_unscaled_df['xgb_predict'] = xgboost.predict(knnwithprob_Xy_unscaled_df_tmp)\n",
    "        knnwithprob_Xy_unscaled_df['xgb_proba'] = [round(max(x), 3) for x in xgboost.predict_proba(knnwithprob_Xy_unscaled_df_tmp)]\n",
    "        knnwithprob_Xy_unscaled_df['lsvc_predict'] = linear_svc.predict(transformer.apply_scaler(knnwithprob_Xy_unscaled_df_tmp))\n",
    "        knnwithprob_Xy_unscaled_df['lsvc_proba'] = [round(max(x), 3) for x in\n",
    "                                            linear_svc.predict_proba(transformer.apply_scaler(knnwithprob_Xy_unscaled_df_tmp))]\n",
    "        test_data_dict['knnwithprob_Xy_unscaled_df'] = knnwithprob_Xy_unscaled_df\n",
    "        test_data_dict['test_X_unscaled_df'] = pltrain_X_unscaled_df.iloc[train_X_scaled_len:]\n",
    "        test_data_dict['test_X_scaled_df'] = pltrain_X_scaled_df.iloc[train_X_scaled_len:]\n",
    "        test_data_dict['test_y_df'] = pltrain_y_df.iloc[train_X_scaled_len:]\n",
    "        \n",
    "        res = test_routine(data_dict, transformer, knn_lookup)\n",
    "        \n",
    "        print(\"XGBoost: PLTrain auf Val: {} --- PLTest auf Train: {} || LinearSVC:  PLTrain auf Val: {} --- PLTest auf Train: {} || Own Classifier:  PLTrain auf Val: {} --- PLTest auf Train: {}\".format(\n",
    "            res['xgboost']['val']['dmc_score'],res['xgboost']['train']['dmc_score'],res['lin_svc']['val']['dmc_score'],res['lin_svc']['train']['dmc_score'],res['own_classifier']['val']['dmc_score'],res['own_classifier']['train']['dmc_score']))\n",
    "        \n",
    "        \n",
    "        results.append(res)\n",
    "\n",
    "    \"\"\"\n",
    "    # use last few rows that cant fill up a complete batch as a smaller batch\n",
    "    print(\"iteration\", int(i / TEST_BATCH_SIZE) + 1, \"\\twith batch from\", i, \"\\t to\", len(test_X_unscaled),\n",
    "          \", training with\", len(pltrain_X_unscaled_df), \"samples\")\n",
    "    testbatch_X_transformed_df = test_X_unscaled.iloc[i:len(test_X_unscaled)]\n",
    "    \"\"\"\n",
    "    xgb_final = get_classifier(\"xgb\")\n",
    "    lsvc_final = get_classifier(\"svc\")\n",
    "    \n",
    "    xgb_final.fit(pltrain_X_unscaled_df.values, pltrain_y_df.values)\n",
    "    lsvc_final.fit(pltrain_X_scaled_df.values, pltrain_y_df.values)\n",
    "    final_prediction = classify(xgb_final, lsvc_final, test_final_X_df.iloc[:10000], data_dict['knnwithprob_Xy_unscaled_df'], transformer, knn_lookup)\n",
    "    return results, final_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.) iterative model training using pseudo-labeling\n",
    "predict batches of the test set, add them to the previous training set and use this new training set to predict the next batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extended_pltrain_for_batch(testbatch_X_unscaled_df, pltrain_X_unscaled_df, pltrain_y_df,\n",
    "                                   knnwithprobs_Xy_unscaled, transformer, knn_lookup):\n",
    "    # train a classificator on the pseudo labeled train (pltrain) dataset\n",
    "    pltrain_X_scaled_df = transformer.apply_scaler(pltrain_X_unscaled_df)\n",
    "    linear_svc = get_classifier('svc')\n",
    "    linear_svc.fit(pltrain_X_scaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "    xgboost = get_classifier('xgb')\n",
    "    xgboost.fit(pltrain_X_unscaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "    testbatch_y = classify(xgboost, linear_svc, testbatch_X_unscaled_df, knnwithprobs_Xy_unscaled, transformer,\n",
    "                           knn_lookup)\n",
    "\n",
    "    # add batch to pseudo labeled train (pltrain) dataset. needs to ignore index as ids in test also start with 0\n",
    "    pltrainnew_X_unscaled_df = pltrain_X_unscaled_df.append(testbatch_X_unscaled_df, ignore_index=True)\n",
    "    pltrainnew_y_df = pltrain_y_df.append(testbatch_y,\n",
    "                                          ignore_index=True)  # pltrainnew_Xy_unscaled_df[['fraud']], ignore_index=True)\n",
    "\n",
    "    return pltrainnew_X_unscaled_df, pltrainnew_y_df\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1127 available train data before pseudo labeling\n",
      "XGBoost: PLTrain auf Val: 30 --- PLTest auf Train: -505 || LinearSVC:  PLTrain auf Val: 80 --- PLTest auf Train: -90 || Own Classifier:  PLTrain auf Val: 75 --- PLTest auf Train: -155\n",
      "iteration 2 \t/ 20 with batch from 250 \t to 500 , training with 1377 samples\n",
      "XGBoost: PLTrain auf Val: -15 --- PLTest auf Train: -5 || LinearSVC:  PLTrain auf Val: 35 --- PLTest auf Train: 95 || Own Classifier:  PLTrain auf Val: -10 --- PLTest auf Train: 70\n",
      "XGBoost: PLTrain auf Val: -45 --- PLTest auf Train: 10 || LinearSVC:  PLTrain auf Val: -40 --- PLTest auf Train: 70 || Own Classifier:  PLTrain auf Val: -50 --- PLTest auf Train: 80\n",
      "iteration 4 \t/ 20 with batch from 750 \t to 1000 , training with 1877 samples\n",
      "XGBoost: PLTrain auf Val: -55 --- PLTest auf Train: -15 || LinearSVC:  PLTrain auf Val: -50 --- PLTest auf Train: 150 || Own Classifier:  PLTrain auf Val: -45 --- PLTest auf Train: 45\n",
      "XGBoost: PLTrain auf Val: -65 --- PLTest auf Train: 45 || LinearSVC:  PLTrain auf Val: -35 --- PLTest auf Train: 105 || Own Classifier:  PLTrain auf Val: -55 --- PLTest auf Train: 75\n",
      "iteration 6 \t/ 20 with batch from 1250 \t to 1500 , training with 2377 samples\n",
      "XGBoost: PLTrain auf Val: -40 --- PLTest auf Train: 10 || LinearSVC:  PLTrain auf Val: 0 --- PLTest auf Train: 185 || Own Classifier:  PLTrain auf Val: -55 --- PLTest auf Train: 60\n",
      "XGBoost: PLTrain auf Val: -40 --- PLTest auf Train: 50 || LinearSVC:  PLTrain auf Val: -35 --- PLTest auf Train: 230 || Own Classifier:  PLTrain auf Val: -55 --- PLTest auf Train: 145\n",
      "iteration 8 \t/ 20 with batch from 1750 \t to 2000 , training with 2877 samples\n",
      "XGBoost: PLTrain auf Val: -30 --- PLTest auf Train: 40 || LinearSVC:  PLTrain auf Val: -60 --- PLTest auf Train: 195 || Own Classifier:  PLTrain auf Val: -55 --- PLTest auf Train: 135\n",
      "XGBoost: PLTrain auf Val: -30 --- PLTest auf Train: 40 || LinearSVC:  PLTrain auf Val: -60 --- PLTest auf Train: 195 || Own Classifier:  PLTrain auf Val: -55 --- PLTest auf Train: 120\n",
      "iteration 10 \t/ 20 with batch from 2250 \t to 2500 , training with 3377 samples\n",
      "XGBoost: PLTrain auf Val: -30 --- PLTest auf Train: 80 || LinearSVC:  PLTrain auf Val: -60 --- PLTest auf Train: 160 || Own Classifier:  PLTrain auf Val: -45 --- PLTest auf Train: 150\n",
      "XGBoost: PLTrain auf Val: -30 --- PLTest auf Train: 115 || LinearSVC:  PLTrain auf Val: -60 --- PLTest auf Train: 180 || Own Classifier:  PLTrain auf Val: -35 --- PLTest auf Train: 135\n",
      "iteration 12 \t/ 20 with batch from 2750 \t to 3000 , training with 3877 samples\n",
      "XGBoost: PLTrain auf Val: -40 --- PLTest auf Train: 140 || LinearSVC:  PLTrain auf Val: -25 --- PLTest auf Train: 195 || Own Classifier:  PLTrain auf Val: -55 --- PLTest auf Train: 160\n",
      "XGBoost: PLTrain auf Val: -40 --- PLTest auf Train: 135 || LinearSVC:  PLTrain auf Val: -35 --- PLTest auf Train: 185 || Own Classifier:  PLTrain auf Val: -45 --- PLTest auf Train: 140\n",
      "iteration 14 \t/ 20 with batch from 3250 \t to 3500 , training with 4377 samples\n",
      "XGBoost: PLTrain auf Val: -40 --- PLTest auf Train: 150 || LinearSVC:  PLTrain auf Val: -45 --- PLTest auf Train: 185 || Own Classifier:  PLTrain auf Val: -55 --- PLTest auf Train: 125\n",
      "XGBoost: PLTrain auf Val: -65 --- PLTest auf Train: 115 || LinearSVC:  PLTrain auf Val: -45 --- PLTest auf Train: 195 || Own Classifier:  PLTrain auf Val: -55 --- PLTest auf Train: 130\n",
      "iteration 16 \t/ 20 with batch from 3750 \t to 4000 , training with 4877 samples\n",
      "XGBoost: PLTrain auf Val: -55 --- PLTest auf Train: 135 || LinearSVC:  PLTrain auf Val: -45 --- PLTest auf Train: 195 || Own Classifier:  PLTrain auf Val: -55 --- PLTest auf Train: 155\n",
      "XGBoost: PLTrain auf Val: -55 --- PLTest auf Train: 150 || LinearSVC:  PLTrain auf Val: -55 --- PLTest auf Train: 195 || Own Classifier:  PLTrain auf Val: -45 --- PLTest auf Train: 200\n",
      "iteration 18 \t/ 20 with batch from 4250 \t to 4500 , training with 5377 samples\n",
      "XGBoost: PLTrain auf Val: -65 --- PLTest auf Train: 130 || LinearSVC:  PLTrain auf Val: -55 --- PLTest auf Train: 195 || Own Classifier:  PLTrain auf Val: -55 --- PLTest auf Train: 115\n",
      "XGBoost: PLTrain auf Val: -75 --- PLTest auf Train: 145 || LinearSVC:  PLTrain auf Val: -55 --- PLTest auf Train: 195 || Own Classifier:  PLTrain auf Val: -55 --- PLTest auf Train: 150\n"
     ]
    }
   ],
   "source": [
    "data_dict = dict()\n",
    "data_dict['train_complete_X_unscaled_df'] = train_complete_X_unscaled_df.copy()\n",
    "data_dict['train_complete_X_scaled_df'] = transformer.apply_scaler(train_complete_X_unscaled_df)\n",
    "data_dict['train_complete_y_df'] = train_complete_y_df.copy()\n",
    "\n",
    "data_dict['knnwithprob_Xy_unscaled_df'] = transformer.add_features(knn_Xy_original_df)\n",
    "data_dict['val_X_unscaled_df'] = val_X_unscaled_df.copy()\n",
    "data_dict['val_X_scaled_df'] = transformer.apply_scaler(val_X_unscaled_df)\n",
    "data_dict['val_y_df'] = val_y_df.copy()\n",
    "\n",
    "res, fin_pred = semi_supervised_learning_procedure(test_X_unscaled_df, train_X_unscaled_df, train_y_df, data_dict, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>totalScannedLineItems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.401809</td>\n",
       "      <td>932.153273</td>\n",
       "      <td>50.864492</td>\n",
       "      <td>5.469931</td>\n",
       "      <td>4.904204</td>\n",
       "      <td>2.525279</td>\n",
       "      <td>0.058138</td>\n",
       "      <td>0.201746</td>\n",
       "      <td>0.745404</td>\n",
       "      <td>15.383183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.709404</td>\n",
       "      <td>530.144640</td>\n",
       "      <td>28.940202</td>\n",
       "      <td>3.451169</td>\n",
       "      <td>3.139697</td>\n",
       "      <td>1.695472</td>\n",
       "      <td>0.278512</td>\n",
       "      <td>1.242135</td>\n",
       "      <td>1.327241</td>\n",
       "      <td>8.707411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>474.500000</td>\n",
       "      <td>25.965000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008384</td>\n",
       "      <td>0.027787</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>932.000000</td>\n",
       "      <td>51.210000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.016317</td>\n",
       "      <td>0.054498</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1397.000000</td>\n",
       "      <td>77.285000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.032594</td>\n",
       "      <td>0.107313</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1831.000000</td>\n",
       "      <td>99.960000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>37.870000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        trustLevel  totalScanTimeInSeconds   grandTotal  lineItemVoids  \\\n",
       "count  1879.000000             1879.000000  1879.000000    1879.000000   \n",
       "mean      3.401809              932.153273    50.864492       5.469931   \n",
       "std       1.709404              530.144640    28.940202       3.451169   \n",
       "min       1.000000                2.000000     0.010000       0.000000   \n",
       "25%       2.000000              474.500000    25.965000       2.000000   \n",
       "50%       3.000000              932.000000    51.210000       5.000000   \n",
       "75%       5.000000             1397.000000    77.285000       8.000000   \n",
       "max       6.000000             1831.000000    99.960000      11.000000   \n",
       "\n",
       "       scansWithoutRegistration  quantityModifications  \\\n",
       "count               1879.000000            1879.000000   \n",
       "mean                   4.904204               2.525279   \n",
       "std                    3.139697               1.695472   \n",
       "min                    0.000000               0.000000   \n",
       "25%                    2.000000               1.000000   \n",
       "50%                    5.000000               3.000000   \n",
       "75%                    8.000000               4.000000   \n",
       "max                   10.000000               5.000000   \n",
       "\n",
       "       scannedLineItemsPerSecond  valuePerSecond  lineItemVoidsPerPosition  \\\n",
       "count                1879.000000     1879.000000               1879.000000   \n",
       "mean                    0.058138        0.201746                  0.745404   \n",
       "std                     0.278512        1.242135                  1.327241   \n",
       "min                     0.000548        0.000007                  0.000000   \n",
       "25%                     0.008384        0.027787                  0.160000   \n",
       "50%                     0.016317        0.054498                  0.350000   \n",
       "75%                     0.032594        0.107313                  0.666667   \n",
       "max                     6.666667       37.870000                 11.000000   \n",
       "\n",
       "       totalScannedLineItems  \n",
       "count            1879.000000  \n",
       "mean               15.383183  \n",
       "std                 8.707411  \n",
       "min                 1.000000  \n",
       "25%                 8.000000  \n",
       "50%                15.000000  \n",
       "75%                23.000000  \n",
       "max                30.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_complete_X_unscaled_df.copy().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.201283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              fraud\n",
       "count  10000.000000\n",
       "mean       0.042300\n",
       "std        0.201283\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        0.000000\n",
       "max        1.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_pred.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.) pseudo-label the test set and create new classifier based on this\n",
    "first we predict the original test data labels using the new extended pltrain from above cell and second we use this test data labels to train a new classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.) evaluate our new classifier with the original training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.) combine the pseudo labeled test set with the original train data to train our final classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.) predict labels for the test set using our final classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.) generate output file neeeded for submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.) evaluate our new classifier with the validation set\n",
    "Now at the very end we can also test our final model on a validation split never used before. just for comparison. \n",
    "\n",
    "**For the final submission, the following code should will not be run and the full train (incl. this val split) set will be used above**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
