{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/train_new.csv\", sep=\"|\")\n",
    "df_val = pd.read_csv(\"../data/val_new.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmc_score_function(label, y_pred):\n",
    "    loss = 0.0\n",
    "    for index, item in enumerate(label):\n",
    "        if label[index] == 0: \n",
    "            if y_pred[index] == 0:\n",
    "                loss += 0.0\n",
    "            else:\n",
    "                loss -= 25.0\n",
    "        else:\n",
    "            if y_pred[index] == 0:\n",
    "                loss -= 5.0\n",
    "            else:\n",
    "                loss += 5.0\n",
    "    return loss\n",
    "\n",
    "def calculate_metrics(x, y, model, tp=\"scikit\"):\n",
    "    scores = {}\n",
    "    pred = None\n",
    "    if tp is \"scikit\":\n",
    "        scores['acc'] = model.score(x,y)\n",
    "        pred = model.predict(x)\n",
    "    elif tp is \"keras\":\n",
    "        pred = model.predict_classes(x)\n",
    "    scores['dmc'] = dmc_score_function(pred,y)\n",
    "    scores['f2'] = fbeta_score(pred, y, 2, average='binary')\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_frauds_balanced = df_train[df_train.fraud!=1].sample(n=df_train[df_train.fraud==1].count()[0])\n",
    "df_50_50 = df_wo_frauds_balanced.append(df_train[df_train.fraud==1],ignore_index=True).sample(frac=1)\n",
    "df_50_50.count()\n",
    "#Validation dataset\n",
    "df_val_y = df_val.fraud\n",
    "df_val_x = df_val.drop(['fraud'], axis=1)\n",
    "\n",
    "#Train balanced\n",
    "df_train_balanced_y = df_50_50.fraud\n",
    "df_train_balanced_x = df_50_50.drop(['fraud'], axis=1)\n",
    "\n",
    "#Train unbalanced\n",
    "df_train_unbalanced_y = df_train.fraud\n",
    "df_train_unbalanced_x = df_train.drop(['fraud'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "`class sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logistic regression unbalanced': {'dmc': -270.0, 'f2': 0.632183908045977, 'acc': 0.9547872340425532}, 'logistic regression balanced': {'dmc': -105.0, 'f2': 0.3951890034364261, 'acc': 0.8829787234042553}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/Projects/dmc2019/venv/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/lukas/Projects/dmc2019/venv/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "res = dict()\n",
    "\n",
    "log_reg_balanced = LogisticRegression().fit(df_train_balanced_x, df_train_balanced_y)\n",
    "log_reg_unbalanced = LogisticRegression().fit(df_train_unbalanced_x, df_train_unbalanced_y)\n",
    "\n",
    "res['logistic regression balanced'] = calculate_metrics(df_val_x, df_val_y, log_reg_balanced)\n",
    "res['logistic regression unbalanced'] = calculate_metrics(df_val_x, df_val_y, log_reg_unbalanced)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis\n",
    "`class sklearn.discriminant_analysis.LinearDiscriminantAnalysis(solver=’svd’, shrinkage=None, priors=None, n_components=None, store_covariance=False, tol=0.0001)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda_balanced = LinearDiscriminantAnalysis().fit(df_train_balanced_x, df_train_balanced_y)\n",
    "lda_unbalanced = LinearDiscriminantAnalysis().fit(df_train_unbalanced_x, df_train_unbalanced_y)\n",
    "res['lda balanced'] = calculate_metrics(df_val_x, df_val_y, lda_balanced)\n",
    "res['lda unbalanced'] = calculate_metrics(df_val_x, df_val_y, lda_unbalanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN \n",
    "`class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights=’uniform’, algorithm=’auto’, leaf_size=30, p=2, metric=’minkowski’, metric_params=None, n_jobs=None, **kwargs)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_balanced = KNeighborsClassifier().fit(df_train_balanced_x, df_train_balanced_y)\n",
    "knn_unbalanced = KNeighborsClassifier().fit(df_train_unbalanced_x, df_train_unbalanced_y)\n",
    "res['knn balanced'] = calculate_metrics(df_val_x, df_val_y, knn_balanced)\n",
    "res['knn unbalanced'] = calculate_metrics(df_val_x, df_val_y, knn_unbalanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier\n",
    "`class sklearn.svm.SVC(C=1.0, kernel=’rbf’, degree=3, gamma=’auto_deprecated’, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/Projects/dmc2019/venv/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/home/lukas/Projects/dmc2019/venv/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/home/lukas/Projects/dmc2019/venv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_balanced = SVC().fit(df_train_balanced_x, df_train_balanced_y)\n",
    "svc_unbalanced = SVC().fit(df_train_unbalanced_x, df_train_unbalanced_y)\n",
    "res['svc balanced'] = calculate_metrics(df_val_x, df_val_y, svc_balanced)\n",
    "res['svc unbalanced'] = calculate_metrics(df_val_x, df_val_y, svc_unbalanced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/Projects/dmc2019/venv/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/home/lukas/Projects/dmc2019/venv/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/home/lukas/Projects/dmc2019/venv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_balanced = SVC().fit(df_train_balanced_x, df_train_balanced_y)\n",
    "rfc_unbalanced = SVC().fit(df_train_unbalanced_x, df_train_unbalanced_y)\n",
    "res['rfc balanced'] = calculate_metrics(df_val_x, df_val_y, rfc_balanced)\n",
    "res['rfc unbalanced'] = calculate_metrics(df_val_x, df_val_y, rfc_unbalanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost \n",
    "[XGBoost Webseite](https://xgboost.readthedocs.io/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "#params = param = {'max_depth':3, 'eta':1, 'gamma': 1.0, 'min_child_weight' : 1, 'objective':'binary:logistic' }\n",
    "xgb_balanced = XGBClassifier().fit(df_train_balanced_x,df_train_balanced_y)\n",
    "xgb_unbalanced = XGBClassifier().fit(df_train_unbalanced_x,df_train_unbalanced_y)\n",
    "#bst = xgb.train(param, xgb_train_x_balanced, 3)\n",
    "res['xgb balanced'] = calculate_metrics(df_val_x, df_val_y, xgb_balanced)\n",
    "res['xgb unbalanced'] = calculate_metrics(df_val_x, df_val_y, xgb_unbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(376, 9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(df_val_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1503/1503 [==============================] - 0s 103us/step - loss: 1.4070 - acc: 0.7911\n",
      "Epoch 2/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.9879 - acc: 0.8789\n",
      "Epoch 3/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.8967 - acc: 0.9175\n",
      "Epoch 4/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.8581 - acc: 0.9335\n",
      "Epoch 5/100\n",
      "1503/1503 [==============================] - 0s 19us/step - loss: 1.0309 - acc: 0.8629\n",
      "Epoch 6/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.8042 - acc: 0.9441\n",
      "Epoch 7/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.7185 - acc: 0.9348\n",
      "Epoch 8/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 1.0343 - acc: 0.8689\n",
      "Epoch 9/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.7940 - acc: 0.9461\n",
      "Epoch 10/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.6980 - acc: 0.9381\n",
      "Epoch 11/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.6491 - acc: 0.9408\n",
      "Epoch 12/100\n",
      "1503/1503 [==============================] - 0s 20us/step - loss: 0.8332 - acc: 0.9175\n",
      "Epoch 13/100\n",
      "1503/1503 [==============================] - 0s 23us/step - loss: 0.7271 - acc: 0.9461\n",
      "Epoch 14/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 1.3055 - acc: 0.8729\n",
      "Epoch 15/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.6694 - acc: 0.9355\n",
      "Epoch 16/100\n",
      "1503/1503 [==============================] - 0s 23us/step - loss: 0.5276 - acc: 0.9401\n",
      "Epoch 17/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.5831 - acc: 0.9434\n",
      "Epoch 18/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.8987 - acc: 0.8796\n",
      "Epoch 19/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.7945 - acc: 0.9461\n",
      "Epoch 20/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.8923 - acc: 0.8776\n",
      "Epoch 21/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.7864 - acc: 0.9461\n",
      "Epoch 22/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.6046 - acc: 0.9288\n",
      "Epoch 23/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.7607 - acc: 0.8995\n",
      "Epoch 24/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.8068 - acc: 0.9461\n",
      "Epoch 25/100\n",
      "1503/1503 [==============================] - 0s 20us/step - loss: 0.9420 - acc: 0.8756\n",
      "Epoch 26/100\n",
      "1503/1503 [==============================] - 0s 20us/step - loss: 0.8010 - acc: 0.9461\n",
      "Epoch 27/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.7938 - acc: 0.8849\n",
      "Epoch 28/100\n",
      "1503/1503 [==============================] - 0s 23us/step - loss: 0.7944 - acc: 0.9461\n",
      "Epoch 29/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.7118 - acc: 0.9015\n",
      "Epoch 30/100\n",
      "1503/1503 [==============================] - 0s 23us/step - loss: 0.7791 - acc: 0.9461\n",
      "Epoch 31/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.9412 - acc: 0.8796\n",
      "Epoch 32/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.7645 - acc: 0.9461\n",
      "Epoch 33/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.9002 - acc: 0.8796\n",
      "Epoch 34/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.7568 - acc: 0.9461\n",
      "Epoch 35/100\n",
      "1503/1503 [==============================] - 0s 23us/step - loss: 0.5219 - acc: 0.9348\n",
      "Epoch 36/100\n",
      "1503/1503 [==============================] - 0s 24us/step - loss: 0.7155 - acc: 0.9395\n",
      "Epoch 37/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.5843 - acc: 0.9235\n",
      "Epoch 38/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.7769 - acc: 0.9461\n",
      "Epoch 39/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.9022 - acc: 0.8709\n",
      "Epoch 40/100\n",
      "1503/1503 [==============================] - 0s 20us/step - loss: 0.7833 - acc: 0.9461\n",
      "Epoch 41/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.7437 - acc: 0.8882\n",
      "Epoch 42/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.7906 - acc: 0.9461\n",
      "Epoch 43/100\n",
      "1503/1503 [==============================] - 0s 20us/step - loss: 0.6274 - acc: 0.9401\n",
      "Epoch 44/100\n",
      "1503/1503 [==============================] - 0s 20us/step - loss: 0.9331 - acc: 0.8862\n",
      "Epoch 45/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.6677 - acc: 0.9461\n",
      "Epoch 46/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.7268 - acc: 0.9208\n",
      "Epoch 47/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.7429 - acc: 0.9228\n",
      "Epoch 48/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.8930 - acc: 0.8829\n",
      "Epoch 49/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.7568 - acc: 0.9461\n",
      "Epoch 50/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.6485 - acc: 0.8889\n",
      "Epoch 51/100\n",
      "1503/1503 [==============================] - 0s 20us/step - loss: 0.7928 - acc: 0.9461\n",
      "Epoch 52/100\n",
      "1503/1503 [==============================] - 0s 24us/step - loss: 0.5508 - acc: 0.9448\n",
      "Epoch 53/100\n",
      "1503/1503 [==============================] - 0s 23us/step - loss: 0.8483 - acc: 0.8842\n",
      "Epoch 54/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.6497 - acc: 0.9468\n",
      "Epoch 55/100\n",
      "1503/1503 [==============================] - 0s 24us/step - loss: 0.7702 - acc: 0.8969\n",
      "Epoch 56/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.7297 - acc: 0.9128\n",
      "Epoch 57/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.7277 - acc: 0.9461\n",
      "Epoch 58/100\n",
      "1503/1503 [==============================] - 0s 20us/step - loss: 0.6936 - acc: 0.9148\n",
      "Epoch 59/100\n",
      "1503/1503 [==============================] - 0s 23us/step - loss: 0.7763 - acc: 0.8849\n",
      "Epoch 60/100\n",
      "1503/1503 [==============================] - 0s 19us/step - loss: 0.7858 - acc: 0.9461\n",
      "Epoch 61/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.7132 - acc: 0.8962\n",
      "Epoch 62/100\n",
      "1503/1503 [==============================] - 0s 20us/step - loss: 0.7522 - acc: 0.9461\n",
      "Epoch 63/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.7692 - acc: 0.8942\n",
      "Epoch 64/100\n",
      "1503/1503 [==============================] - 0s 23us/step - loss: 0.6380 - acc: 0.9421\n",
      "Epoch 65/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.4580 - acc: 0.9388\n",
      "Epoch 66/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.9070 - acc: 0.8849\n",
      "Epoch 67/100\n",
      "1503/1503 [==============================] - 0s 20us/step - loss: 0.6497 - acc: 0.9434\n",
      "Epoch 68/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.6666 - acc: 0.9142\n",
      "Epoch 69/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.8131 - acc: 0.8869\n",
      "Epoch 70/100\n",
      "1503/1503 [==============================] - 0s 20us/step - loss: 0.7768 - acc: 0.9461\n",
      "Epoch 71/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.6570 - acc: 0.9341\n",
      "Epoch 72/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.5466 - acc: 0.9301\n",
      "Epoch 73/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.7006 - acc: 0.9215\n",
      "Epoch 74/100\n",
      "1503/1503 [==============================] - 0s 20us/step - loss: 0.6622 - acc: 0.9428\n",
      "Epoch 75/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.7514 - acc: 0.8909\n",
      "Epoch 76/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.7348 - acc: 0.9461\n",
      "Epoch 77/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.8054 - acc: 0.8949\n",
      "Epoch 78/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.6607 - acc: 0.9401\n",
      "Epoch 79/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.5751 - acc: 0.9295\n",
      "Epoch 80/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.8746 - acc: 0.8916\n",
      "Epoch 81/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.6901 - acc: 0.9461\n",
      "Epoch 82/100\n",
      "1503/1503 [==============================] - 0s 20us/step - loss: 0.7632 - acc: 0.9009\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1503/1503 [==============================] - 0s 19us/step - loss: 0.7034 - acc: 0.9228\n",
      "Epoch 84/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.6240 - acc: 0.9022\n",
      "Epoch 85/100\n",
      "1503/1503 [==============================] - 0s 23us/step - loss: 0.7658 - acc: 0.9461\n",
      "Epoch 86/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.6802 - acc: 0.9281\n",
      "Epoch 87/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.6163 - acc: 0.9261\n",
      "Epoch 88/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.9383 - acc: 0.8809\n",
      "Epoch 89/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.6665 - acc: 0.9261\n",
      "Epoch 90/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.6407 - acc: 0.9261\n",
      "Epoch 91/100\n",
      "1503/1503 [==============================] - 0s 23us/step - loss: 0.8513 - acc: 0.8882\n",
      "Epoch 92/100\n",
      "1503/1503 [==============================] - 0s 19us/step - loss: 0.7537 - acc: 0.9461\n",
      "Epoch 93/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.5148 - acc: 0.9341\n",
      "Epoch 94/100\n",
      "1503/1503 [==============================] - 0s 19us/step - loss: 0.8815 - acc: 0.8862\n",
      "Epoch 95/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.5752 - acc: 0.9268\n",
      "Epoch 96/100\n",
      "1503/1503 [==============================] - 0s 21us/step - loss: 0.6768 - acc: 0.9461\n",
      "Epoch 97/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.9528 - acc: 0.8902\n",
      "Epoch 98/100\n",
      "1503/1503 [==============================] - 0s 22us/step - loss: 0.6009 - acc: 0.9002\n",
      "Epoch 99/100\n",
      "1503/1503 [==============================] - 0s 20us/step - loss: 0.7448 - acc: 0.9461\n",
      "Epoch 100/100\n",
      "1503/1503 [==============================] - 0s 23us/step - loss: 0.5829 - acc: 0.8922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6524728048>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import fbeta_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "df_train = pd.read_csv(\"../data/train_new.csv\", sep=\"|\")\n",
    "df_val = pd.read_csv(\"../data/val_new.csv\", sep=\"|\")\n",
    "\n",
    "# Data Preparation\n",
    "df_wo_frauds_balanced = df_train[df_train.fraud != 1].sample(n=df_train[df_train.fraud == 1].count()[0])\n",
    "df_50_50 = df_wo_frauds_balanced.append(df_train[df_train.fraud == 1], ignore_index=True).sample(frac=1)\n",
    "df_50_50.count()\n",
    "\n",
    "# Validation dataset\n",
    "df_val_y = df_val.fraud\n",
    "df_val_x = df_val.drop(['fraud'], axis=1)\n",
    "\n",
    "# Train balanced\n",
    "df_train_balanced_y = df_50_50.fraud\n",
    "df_train_balanced_x = df_50_50.drop(['fraud'], axis=1)\n",
    "\n",
    "# Train unbalanced\n",
    "df_train_unbalanced_y = np.array(df_train.fraud)\n",
    "df_train_unbalanced_x = np.array(df_train.drop(['fraud'], axis=1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=9))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(df_train_unbalanced_x, df_train_unbalanced_y, epochs=15, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 1.7002 - acc: 0.6358\n",
      "Epoch 2/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 1.0249 - acc: 0.6605\n",
      "Epoch 3/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 1.6106 - acc: 0.5432\n",
      "Epoch 4/500\n",
      "162/162 [==============================] - 0s 111us/step - loss: 0.6992 - acc: 0.8210\n",
      "Epoch 5/500\n",
      "162/162 [==============================] - 0s 111us/step - loss: 1.3480 - acc: 0.6173\n",
      "Epoch 6/500\n",
      "162/162 [==============================] - 0s 109us/step - loss: 1.3372 - acc: 0.6605\n",
      "Epoch 7/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 2.0288 - acc: 0.5185\n",
      "Epoch 8/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 0.7390 - acc: 0.7840\n",
      "Epoch 9/500\n",
      "162/162 [==============================] - 0s 110us/step - loss: 1.0824 - acc: 0.6852\n",
      "Epoch 10/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 1.3023 - acc: 0.6667\n",
      "Epoch 11/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 0.6051 - acc: 0.7840\n",
      "Epoch 12/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 1.3603 - acc: 0.6173\n",
      "Epoch 13/500\n",
      "162/162 [==============================] - 0s 107us/step - loss: 0.8365 - acc: 0.7284\n",
      "Epoch 14/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.9702 - acc: 0.8457\n",
      "Epoch 15/500\n",
      "162/162 [==============================] - 0s 112us/step - loss: 1.4512 - acc: 0.6420\n",
      "Epoch 16/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 0.2287 - acc: 0.9198\n",
      "Epoch 17/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 2.1645 - acc: 0.5864\n",
      "Epoch 18/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 1.4869 - acc: 0.5741\n",
      "Epoch 19/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.9070 - acc: 0.7531\n",
      "Epoch 20/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 1.6310 - acc: 0.5864\n",
      "Epoch 21/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 0.5133 - acc: 0.8148\n",
      "Epoch 22/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 1.3677 - acc: 0.8086\n",
      "Epoch 23/500\n",
      "162/162 [==============================] - 0s 95us/step - loss: 0.7850 - acc: 0.7593\n",
      "Epoch 24/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 2.2412 - acc: 0.4691\n",
      "Epoch 25/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 0.2132 - acc: 0.9444\n",
      "Epoch 26/500\n",
      "162/162 [==============================] - 0s 95us/step - loss: 0.9480 - acc: 0.7778\n",
      "Epoch 27/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.7358 - acc: 0.8086\n",
      "Epoch 28/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 2.5577 - acc: 0.6543\n",
      "Epoch 29/500\n",
      "162/162 [==============================] - 0s 109us/step - loss: 0.8705 - acc: 0.6975\n",
      "Epoch 30/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.5626 - acc: 0.7654\n",
      "Epoch 31/500\n",
      "162/162 [==============================] - 0s 90us/step - loss: 1.1634 - acc: 0.7654\n",
      "Epoch 32/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.9144 - acc: 0.7716\n",
      "Epoch 33/500\n",
      "162/162 [==============================] - 0s 112us/step - loss: 1.9411 - acc: 0.6543\n",
      "Epoch 34/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 1.2613 - acc: 0.6728\n",
      "Epoch 35/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.8202 - acc: 0.7407\n",
      "Epoch 36/500\n",
      "162/162 [==============================] - 0s 111us/step - loss: 2.3298 - acc: 0.6173\n",
      "Epoch 37/500\n",
      "162/162 [==============================] - 0s 86us/step - loss: 0.5858 - acc: 0.7593\n",
      "Epoch 38/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 1.4162 - acc: 0.6852\n",
      "Epoch 39/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 1.3659 - acc: 0.6790\n",
      "Epoch 40/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 1.2778 - acc: 0.6481\n",
      "Epoch 41/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 2.6311 - acc: 0.6543\n",
      "Epoch 42/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.2850 - acc: 0.8642\n",
      "Epoch 43/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 0.8596 - acc: 0.7160\n",
      "Epoch 44/500\n",
      "162/162 [==============================] - 0s 91us/step - loss: 0.9850 - acc: 0.7531\n",
      "Epoch 45/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 1.1738 - acc: 0.7284\n",
      "Epoch 46/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 1.7840 - acc: 0.6358\n",
      "Epoch 47/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.8302 - acc: 0.7284\n",
      "Epoch 48/500\n",
      "162/162 [==============================] - 0s 109us/step - loss: 0.7101 - acc: 0.7099\n",
      "Epoch 49/500\n",
      "162/162 [==============================] - 0s 107us/step - loss: 1.6914 - acc: 0.6235\n",
      "Epoch 50/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.9730 - acc: 0.6667\n",
      "Epoch 51/500\n",
      "162/162 [==============================] - 0s 113us/step - loss: 0.2687 - acc: 0.8889\n",
      "Epoch 52/500\n",
      "162/162 [==============================] - 0s 117us/step - loss: 1.0964 - acc: 0.7099\n",
      "Epoch 53/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 0.9903 - acc: 0.6728\n",
      "Epoch 54/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 0.9140 - acc: 0.6605\n",
      "Epoch 55/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 0.6040 - acc: 0.8148\n",
      "Epoch 56/500\n",
      "162/162 [==============================] - 0s 81us/step - loss: 0.9207 - acc: 0.8272\n",
      "Epoch 57/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 1.6831 - acc: 0.6543\n",
      "Epoch 58/500\n",
      "162/162 [==============================] - 0s 95us/step - loss: 1.3999 - acc: 0.6728\n",
      "Epoch 59/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.4376 - acc: 0.7963\n",
      "Epoch 60/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 2.1230 - acc: 0.7531\n",
      "Epoch 61/500\n",
      "162/162 [==============================] - 0s 109us/step - loss: 0.4658 - acc: 0.8086\n",
      "Epoch 62/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 0.5112 - acc: 0.8272\n",
      "Epoch 63/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 1.4407 - acc: 0.7099\n",
      "Epoch 64/500\n",
      "162/162 [==============================] - 0s 91us/step - loss: 2.2680 - acc: 0.7222\n",
      "Epoch 65/500\n",
      "162/162 [==============================] - 0s 107us/step - loss: 0.9962 - acc: 0.7037\n",
      "Epoch 66/500\n",
      "162/162 [==============================] - 0s 109us/step - loss: 0.4167 - acc: 0.8457\n",
      "Epoch 67/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 0.9896 - acc: 0.7407\n",
      "Epoch 68/500\n",
      "162/162 [==============================] - 0s 91us/step - loss: 1.3184 - acc: 0.7160\n",
      "Epoch 69/500\n",
      "162/162 [==============================] - 0s 91us/step - loss: 0.8545 - acc: 0.6667\n",
      "Epoch 70/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 1.2858 - acc: 0.7778\n",
      "Epoch 71/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 0.5103 - acc: 0.8086\n",
      "Epoch 72/500\n",
      "162/162 [==============================] - 0s 111us/step - loss: 1.2107 - acc: 0.6852\n",
      "Epoch 73/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 0.9468 - acc: 0.7531\n",
      "Epoch 74/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 1.0020 - acc: 0.7346\n",
      "Epoch 75/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 1.1696 - acc: 0.7160\n",
      "Epoch 76/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 1.1130 - acc: 0.7346\n",
      "Epoch 77/500\n",
      "162/162 [==============================] - 0s 108us/step - loss: 1.6745 - acc: 0.7099\n",
      "Epoch 78/500\n",
      "162/162 [==============================] - 0s 92us/step - loss: 1.6076 - acc: 0.6358\n",
      "Epoch 79/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.4108 - acc: 0.8765\n",
      "Epoch 80/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 1.1457 - acc: 0.7407\n",
      "Epoch 81/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 1.4365 - acc: 0.7840\n",
      "Epoch 82/500\n",
      "162/162 [==============================] - 0s 107us/step - loss: 1.1844 - acc: 0.7160\n",
      "Epoch 83/500\n",
      "162/162 [==============================] - 0s 109us/step - loss: 0.8370 - acc: 0.7593\n",
      "Epoch 84/500\n",
      "162/162 [==============================] - 0s 109us/step - loss: 0.9656 - acc: 0.6667\n",
      "Epoch 85/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 1.0361 - acc: 0.6481\n",
      "Epoch 86/500\n",
      "162/162 [==============================] - 0s 114us/step - loss: 0.6603 - acc: 0.7037\n",
      "Epoch 87/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 0.5325 - acc: 0.8395\n",
      "Epoch 88/500\n",
      "162/162 [==============================] - 0s 123us/step - loss: 1.1856 - acc: 0.7160\n",
      "Epoch 89/500\n",
      "162/162 [==============================] - 0s 108us/step - loss: 0.6908 - acc: 0.8148\n",
      "Epoch 90/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 1.0774 - acc: 0.7531\n",
      "Epoch 91/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 0.9314 - acc: 0.8086\n",
      "Epoch 92/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 1.3688 - acc: 0.7037\n",
      "Epoch 93/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 1.1701 - acc: 0.7037\n",
      "Epoch 94/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 0.5106 - acc: 0.8395\n",
      "Epoch 95/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.6938 - acc: 0.7963\n",
      "Epoch 96/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 1.5040 - acc: 0.6975\n",
      "Epoch 97/500\n",
      "162/162 [==============================] - 0s 110us/step - loss: 0.6588 - acc: 0.7716\n",
      "Epoch 98/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 1.3006 - acc: 0.6667\n",
      "Epoch 99/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 0.6336 - acc: 0.8025\n",
      "Epoch 100/500\n",
      "162/162 [==============================] - 0s 93us/step - loss: 1.7080 - acc: 0.6790\n",
      "Epoch 101/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 1.3580 - acc: 0.6914\n",
      "Epoch 102/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 0.4083 - acc: 0.8704\n",
      "Epoch 103/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 1.0684 - acc: 0.7531\n",
      "Epoch 104/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.2743 - acc: 0.8580\n",
      "Epoch 105/500\n",
      "162/162 [==============================] - 0s 117us/step - loss: 1.8948 - acc: 0.6852\n",
      "Epoch 106/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 1.1298 - acc: 0.7716\n",
      "Epoch 107/500\n",
      "162/162 [==============================] - 0s 113us/step - loss: 0.5836 - acc: 0.8457\n",
      "Epoch 108/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 0.8269 - acc: 0.8272\n",
      "Epoch 109/500\n",
      "162/162 [==============================] - 0s 108us/step - loss: 2.4227 - acc: 0.7531\n",
      "Epoch 110/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 0.6244 - acc: 0.8333\n",
      "Epoch 111/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 1.2799 - acc: 0.7901\n",
      "Epoch 112/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 1.3769 - acc: 0.7160\n",
      "Epoch 113/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.6310 - acc: 0.7963\n",
      "Epoch 114/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 1.6377 - acc: 0.7037\n",
      "Epoch 115/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 0.1977 - acc: 0.9321\n",
      "Epoch 116/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 0.9541 - acc: 0.7840\n",
      "Epoch 117/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.9161 - acc: 0.7593\n",
      "Epoch 118/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 1.2542 - acc: 0.7407\n",
      "Epoch 119/500\n",
      "162/162 [==============================] - 0s 93us/step - loss: 0.2751 - acc: 0.8889\n",
      "Epoch 120/500\n",
      "162/162 [==============================] - 0s 89us/step - loss: 1.2339 - acc: 0.7346\n",
      "Epoch 121/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 1.3277 - acc: 0.6852\n",
      "Epoch 122/500\n",
      "162/162 [==============================] - 0s 89us/step - loss: 0.2867 - acc: 0.8951\n",
      "Epoch 123/500\n",
      "162/162 [==============================] - 0s 95us/step - loss: 1.8925 - acc: 0.5926\n",
      "Epoch 124/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 1.2909 - acc: 0.7593\n",
      "Epoch 125/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.1856 - acc: 0.9444\n",
      "Epoch 126/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 0.9004 - acc: 0.8086\n",
      "Epoch 127/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 1.1249 - acc: 0.6914\n",
      "Epoch 128/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.1817 - acc: 0.9136\n",
      "Epoch 129/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 1.4753 - acc: 0.7099\n",
      "Epoch 130/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 0.9191 - acc: 0.7284\n",
      "Epoch 131/500\n",
      "162/162 [==============================] - 0s 107us/step - loss: 0.6061 - acc: 0.7963\n",
      "Epoch 132/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 1.2899 - acc: 0.6914\n",
      "Epoch 133/500\n",
      "162/162 [==============================] - 0s 86us/step - loss: 1.5372 - acc: 0.7654\n",
      "Epoch 134/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 0.5173 - acc: 0.8333\n",
      "Epoch 135/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 1.2422 - acc: 0.7840\n",
      "Epoch 136/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 1.2559 - acc: 0.7346\n",
      "Epoch 137/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.4997 - acc: 0.8086\n",
      "Epoch 138/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 1.0814 - acc: 0.7963\n",
      "Epoch 139/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 1.1822 - acc: 0.7531\n",
      "Epoch 140/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 1.0524 - acc: 0.7037\n",
      "Epoch 141/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 0.5011 - acc: 0.8333\n",
      "Epoch 142/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 1.0723 - acc: 0.7160\n",
      "Epoch 143/500\n",
      "162/162 [==============================] - 0s 109us/step - loss: 0.7736 - acc: 0.7778\n",
      "Epoch 144/500\n",
      "162/162 [==============================] - 0s 108us/step - loss: 0.2982 - acc: 0.8580\n",
      "Epoch 145/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 1.5518 - acc: 0.6111\n",
      "Epoch 146/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 0.8585 - acc: 0.8148\n",
      "Epoch 147/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.2678 - acc: 0.9136\n",
      "Epoch 148/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 1.1852 - acc: 0.7407\n",
      "Epoch 149/500\n",
      "162/162 [==============================] - 0s 117us/step - loss: 1.0210 - acc: 0.7469\n",
      "Epoch 150/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.6257 - acc: 0.8272\n",
      "Epoch 151/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 1.1051 - acc: 0.8210\n",
      "Epoch 152/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 0.4303 - acc: 0.8086\n",
      "Epoch 153/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.9318 - acc: 0.7593\n",
      "Epoch 154/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.5902 - acc: 0.8272\n",
      "Epoch 155/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 0.6713 - acc: 0.8086\n",
      "Epoch 156/500\n",
      "162/162 [==============================] - 0s 107us/step - loss: 0.8703 - acc: 0.8704\n",
      "Epoch 157/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 0.3411 - acc: 0.8765\n",
      "Epoch 158/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 1.4490 - acc: 0.7037\n",
      "Epoch 159/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.3705 - acc: 0.8580\n",
      "Epoch 160/500\n",
      "162/162 [==============================] - 0s 112us/step - loss: 1.2682 - acc: 0.7654\n",
      "Epoch 161/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 1.2308 - acc: 0.6296\n",
      "Epoch 162/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.6920 - acc: 0.7840\n",
      "Epoch 163/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 0.1787 - acc: 0.9321\n",
      "Epoch 164/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 1.0673 - acc: 0.7284\n",
      "Epoch 165/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 1.2906 - acc: 0.6667\n",
      "Epoch 166/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 87us/step - loss: 1.5278 - acc: 0.7593\n",
      "Epoch 167/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.3021 - acc: 0.8580\n",
      "Epoch 168/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.1513 - acc: 0.9568\n",
      "Epoch 169/500\n",
      "162/162 [==============================] - 0s 90us/step - loss: 0.2767 - acc: 0.9012\n",
      "Epoch 170/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 1.8444 - acc: 0.6111\n",
      "Epoch 171/500\n",
      "162/162 [==============================] - 0s 113us/step - loss: 0.6808 - acc: 0.8086\n",
      "Epoch 172/500\n",
      "162/162 [==============================] - 0s 95us/step - loss: 0.9253 - acc: 0.7407\n",
      "Epoch 173/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 0.6461 - acc: 0.8148\n",
      "Epoch 174/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.9044 - acc: 0.7716\n",
      "Epoch 175/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 1.0009 - acc: 0.7531\n",
      "Epoch 176/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 1.1474 - acc: 0.6914\n",
      "Epoch 177/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 1.1628 - acc: 0.6605\n",
      "Epoch 178/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.7251 - acc: 0.7593\n",
      "Epoch 179/500\n",
      "162/162 [==============================] - 0s 87us/step - loss: 0.7566 - acc: 0.8025\n",
      "Epoch 180/500\n",
      "162/162 [==============================] - 0s 93us/step - loss: 0.7307 - acc: 0.7716\n",
      "Epoch 181/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 1.1701 - acc: 0.7840\n",
      "Epoch 182/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.4305 - acc: 0.8519\n",
      "Epoch 183/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 1.6068 - acc: 0.6605\n",
      "Epoch 184/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.4959 - acc: 0.8395\n",
      "Epoch 185/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.8129 - acc: 0.7778\n",
      "Epoch 186/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 0.3573 - acc: 0.8951\n",
      "Epoch 187/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 0.9853 - acc: 0.8519\n",
      "Epoch 188/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 1.5239 - acc: 0.6235\n",
      "Epoch 189/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 1.1804 - acc: 0.8457\n",
      "Epoch 190/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 0.2617 - acc: 0.9136\n",
      "Epoch 191/500\n",
      "162/162 [==============================] - 0s 89us/step - loss: 1.0584 - acc: 0.7099\n",
      "Epoch 192/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 0.6830 - acc: 0.7963\n",
      "Epoch 193/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 0.7462 - acc: 0.7963\n",
      "Epoch 194/500\n",
      "162/162 [==============================] - 0s 92us/step - loss: 0.6271 - acc: 0.8025\n",
      "Epoch 195/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 0.9522 - acc: 0.7778\n",
      "Epoch 196/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 1.0455 - acc: 0.7284\n",
      "Epoch 197/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 2.4430 - acc: 0.7222\n",
      "Epoch 198/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 0.3860 - acc: 0.8704\n",
      "Epoch 199/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 1.1130 - acc: 0.8272\n",
      "Epoch 200/500\n",
      "162/162 [==============================] - 0s 109us/step - loss: 0.4771 - acc: 0.8210\n",
      "Epoch 201/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 1.4039 - acc: 0.6790\n",
      "Epoch 202/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 0.8165 - acc: 0.8951\n",
      "Epoch 203/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 0.3348 - acc: 0.8704\n",
      "Epoch 204/500\n",
      "162/162 [==============================] - 0s 95us/step - loss: 0.2371 - acc: 0.9012\n",
      "Epoch 205/500\n",
      "162/162 [==============================] - 0s 88us/step - loss: 1.9937 - acc: 0.7593\n",
      "Epoch 206/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 1.5594 - acc: 0.6235\n",
      "Epoch 207/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 0.7622 - acc: 0.7778\n",
      "Epoch 208/500\n",
      "162/162 [==============================] - 0s 93us/step - loss: 1.4670 - acc: 0.7778\n",
      "Epoch 209/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.2999 - acc: 0.8827\n",
      "Epoch 210/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.8302 - acc: 0.8457\n",
      "Epoch 211/500\n",
      "162/162 [==============================] - 0s 95us/step - loss: 0.4572 - acc: 0.9012\n",
      "Epoch 212/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 0.5842 - acc: 0.8086\n",
      "Epoch 213/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 1.0294 - acc: 0.7840\n",
      "Epoch 214/500\n",
      "162/162 [==============================] - 0s 92us/step - loss: 1.2083 - acc: 0.6667\n",
      "Epoch 215/500\n",
      "162/162 [==============================] - 0s 84us/step - loss: 0.6099 - acc: 0.7901\n",
      "Epoch 216/500\n",
      "162/162 [==============================] - 0s 91us/step - loss: 0.4562 - acc: 0.8457\n",
      "Epoch 217/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 0.6463 - acc: 0.7963\n",
      "Epoch 218/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 1.5028 - acc: 0.8025\n",
      "Epoch 219/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 0.2068 - acc: 0.9012\n",
      "Epoch 220/500\n",
      "162/162 [==============================] - 0s 86us/step - loss: 1.1911 - acc: 0.7593\n",
      "Epoch 221/500\n",
      "162/162 [==============================] - 0s 93us/step - loss: 0.6672 - acc: 0.7840\n",
      "Epoch 222/500\n",
      "162/162 [==============================] - 0s 91us/step - loss: 3.1750 - acc: 0.6790\n",
      "Epoch 223/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.4328 - acc: 0.8148\n",
      "Epoch 224/500\n",
      "162/162 [==============================] - 0s 110us/step - loss: 0.4670 - acc: 0.8086\n",
      "Epoch 225/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 1.1364 - acc: 0.7654\n",
      "Epoch 226/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 0.9325 - acc: 0.8086\n",
      "Epoch 227/500\n",
      "162/162 [==============================] - 0s 88us/step - loss: 1.0405 - acc: 0.7099\n",
      "Epoch 228/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 0.2891 - acc: 0.8704\n",
      "Epoch 229/500\n",
      "162/162 [==============================] - 0s 89us/step - loss: 0.8777 - acc: 0.7901\n",
      "Epoch 230/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 0.6828 - acc: 0.7778\n",
      "Epoch 231/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 1.3663 - acc: 0.7222\n",
      "Epoch 232/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 0.1981 - acc: 0.9259\n",
      "Epoch 233/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.4380 - acc: 0.8519\n",
      "Epoch 234/500\n",
      "162/162 [==============================] - 0s 89us/step - loss: 0.5358 - acc: 0.8951\n",
      "Epoch 235/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 0.8878 - acc: 0.7901\n",
      "Epoch 236/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.3711 - acc: 0.8889\n",
      "Epoch 237/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 1.2843 - acc: 0.8210\n",
      "Epoch 238/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 0.4706 - acc: 0.8580\n",
      "Epoch 239/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 1.0881 - acc: 0.7222\n",
      "Epoch 240/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 0.2889 - acc: 0.8889\n",
      "Epoch 241/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 0.5670 - acc: 0.8086\n",
      "Epoch 242/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.1644 - acc: 0.9321\n",
      "Epoch 243/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 0.5236 - acc: 0.8580\n",
      "Epoch 244/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 1.3343 - acc: 0.7037\n",
      "Epoch 245/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.6935 - acc: 0.8086\n",
      "Epoch 246/500\n",
      "162/162 [==============================] - 0s 95us/step - loss: 1.1926 - acc: 0.7469\n",
      "Epoch 247/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 0.7480 - acc: 0.7469\n",
      "Epoch 248/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.8177 - acc: 0.7840\n",
      "Epoch 249/500\n",
      "162/162 [==============================] - 0s 95us/step - loss: 0.7758 - acc: 0.8086\n",
      "Epoch 250/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.8515 - acc: 0.7963\n",
      "Epoch 251/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.8335 - acc: 0.8272\n",
      "Epoch 252/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.3100 - acc: 0.8889\n",
      "Epoch 253/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 0.5971 - acc: 0.8827\n",
      "Epoch 254/500\n",
      "162/162 [==============================] - 0s 92us/step - loss: 0.5002 - acc: 0.8580\n",
      "Epoch 255/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 2.0448 - acc: 0.6975\n",
      "Epoch 256/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.1858 - acc: 0.9321\n",
      "Epoch 257/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 0.3169 - acc: 0.8889\n",
      "Epoch 258/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 0.6869 - acc: 0.7840\n",
      "Epoch 259/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 1.0495 - acc: 0.7346\n",
      "Epoch 260/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 0.8341 - acc: 0.8272\n",
      "Epoch 261/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 0.2436 - acc: 0.9259\n",
      "Epoch 262/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.3651 - acc: 0.8889\n",
      "Epoch 263/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.7550 - acc: 0.8210\n",
      "Epoch 264/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 1.1597 - acc: 0.7901\n",
      "Epoch 265/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.7696 - acc: 0.7469\n",
      "Epoch 266/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 0.7726 - acc: 0.8457\n",
      "Epoch 267/500\n",
      "162/162 [==============================] - 0s 107us/step - loss: 1.2526 - acc: 0.7160\n",
      "Epoch 268/500\n",
      "162/162 [==============================] - 0s 95us/step - loss: 0.5791 - acc: 0.8395\n",
      "Epoch 269/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.1477 - acc: 0.9506\n",
      "Epoch 270/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 0.9735 - acc: 0.7963\n",
      "Epoch 271/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 0.9520 - acc: 0.7716\n",
      "Epoch 272/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 1.1267 - acc: 0.7469\n",
      "Epoch 273/500\n",
      "162/162 [==============================] - 0s 109us/step - loss: 0.1568 - acc: 0.9321\n",
      "Epoch 274/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 1.1832 - acc: 0.7901\n",
      "Epoch 275/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.9380 - acc: 0.7778\n",
      "Epoch 276/500\n",
      "162/162 [==============================] - 0s 108us/step - loss: 0.5046 - acc: 0.8580\n",
      "Epoch 277/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 0.7606 - acc: 0.8889\n",
      "Epoch 278/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 0.5344 - acc: 0.8148\n",
      "Epoch 279/500\n",
      "162/162 [==============================] - 0s 107us/step - loss: 0.1649 - acc: 0.9444\n",
      "Epoch 280/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 0.2651 - acc: 0.9321\n",
      "Epoch 281/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 1.5092 - acc: 0.6852\n",
      "Epoch 282/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.3088 - acc: 0.8580\n",
      "Epoch 283/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 1.1274 - acc: 0.7160\n",
      "Epoch 284/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 0.2465 - acc: 0.9012\n",
      "Epoch 285/500\n",
      "162/162 [==============================] - 0s 93us/step - loss: 2.1574 - acc: 0.7654\n",
      "Epoch 286/500\n",
      "162/162 [==============================] - 0s 107us/step - loss: 0.2001 - acc: 0.9259\n",
      "Epoch 287/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.8066 - acc: 0.7654\n",
      "Epoch 288/500\n",
      "162/162 [==============================] - 0s 111us/step - loss: 0.1866 - acc: 0.9074\n",
      "Epoch 289/500\n",
      "162/162 [==============================] - 0s 109us/step - loss: 1.1453 - acc: 0.7593\n",
      "Epoch 290/500\n",
      "162/162 [==============================] - 0s 110us/step - loss: 1.8048 - acc: 0.7840\n",
      "Epoch 291/500\n",
      "162/162 [==============================] - 0s 111us/step - loss: 0.2679 - acc: 0.8889\n",
      "Epoch 292/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.4422 - acc: 0.8519\n",
      "Epoch 293/500\n",
      "162/162 [==============================] - 0s 86us/step - loss: 1.3055 - acc: 0.7407\n",
      "Epoch 294/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 0.3227 - acc: 0.8889\n",
      "Epoch 295/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.4799 - acc: 0.8457\n",
      "Epoch 296/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 2.0790 - acc: 0.6296\n",
      "Epoch 297/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 0.3333 - acc: 0.8827\n",
      "Epoch 298/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 1.4891 - acc: 0.7593\n",
      "Epoch 299/500\n",
      "162/162 [==============================] - 0s 108us/step - loss: 0.2397 - acc: 0.9012\n",
      "Epoch 300/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 0.3711 - acc: 0.8519\n",
      "Epoch 301/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 0.7344 - acc: 0.8210\n",
      "Epoch 302/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 0.6784 - acc: 0.7901\n",
      "Epoch 303/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 1.5036 - acc: 0.7531\n",
      "Epoch 304/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 0.1493 - acc: 0.9506\n",
      "Epoch 305/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 0.1487 - acc: 0.9444\n",
      "Epoch 306/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 0.5341 - acc: 0.8333\n",
      "Epoch 307/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 1.1191 - acc: 0.8457\n",
      "Epoch 308/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 0.2814 - acc: 0.9074\n",
      "Epoch 309/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 0.2165 - acc: 0.9074\n",
      "Epoch 310/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 2.1529 - acc: 0.7593\n",
      "Epoch 311/500\n",
      "162/162 [==============================] - 0s 90us/step - loss: 0.1806 - acc: 0.9012\n",
      "Epoch 312/500\n",
      "162/162 [==============================] - 0s 90us/step - loss: 0.6198 - acc: 0.7963\n",
      "Epoch 313/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 0.7703 - acc: 0.8272\n",
      "Epoch 314/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.7936 - acc: 0.8580\n",
      "Epoch 315/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 1.0947 - acc: 0.7778\n",
      "Epoch 316/500\n",
      "162/162 [==============================] - 0s 95us/step - loss: 0.3502 - acc: 0.9136\n",
      "Epoch 317/500\n",
      "162/162 [==============================] - 0s 107us/step - loss: 0.6432 - acc: 0.7716\n",
      "Epoch 318/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.8602 - acc: 0.8457\n",
      "Epoch 319/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 0.7847 - acc: 0.7840\n",
      "Epoch 320/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 0.3574 - acc: 0.8642\n",
      "Epoch 321/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 0.9726 - acc: 0.7407\n",
      "Epoch 322/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 0.6711 - acc: 0.8025\n",
      "Epoch 323/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.1622 - acc: 0.9383\n",
      "Epoch 324/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.3168 - acc: 0.8704\n",
      "Epoch 325/500\n",
      "162/162 [==============================] - 0s 113us/step - loss: 0.4067 - acc: 0.9198\n",
      "Epoch 326/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 1.5648 - acc: 0.7160\n",
      "Epoch 327/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.3378 - acc: 0.8827\n",
      "Epoch 328/500\n",
      "162/162 [==============================] - 0s 95us/step - loss: 1.5788 - acc: 0.7346\n",
      "Epoch 329/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.1548 - acc: 0.9383\n",
      "Epoch 330/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 97us/step - loss: 0.7748 - acc: 0.7840\n",
      "Epoch 331/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 1.3790 - acc: 0.7222\n",
      "Epoch 332/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.2356 - acc: 0.9074\n",
      "Epoch 333/500\n",
      "162/162 [==============================] - 0s 111us/step - loss: 0.8663 - acc: 0.7654\n",
      "Epoch 334/500\n",
      "162/162 [==============================] - 0s 115us/step - loss: 0.5179 - acc: 0.8580\n",
      "Epoch 335/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 0.6529 - acc: 0.8519\n",
      "Epoch 336/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.2033 - acc: 0.8827\n",
      "Epoch 337/500\n",
      "162/162 [==============================] - 0s 111us/step - loss: 1.2402 - acc: 0.7901\n",
      "Epoch 338/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.5682 - acc: 0.8457\n",
      "Epoch 339/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 0.9081 - acc: 0.7531\n",
      "Epoch 340/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 0.8988 - acc: 0.7593\n",
      "Epoch 341/500\n",
      "162/162 [==============================] - 0s 109us/step - loss: 0.1655 - acc: 0.9321\n",
      "Epoch 342/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 0.5456 - acc: 0.8086\n",
      "Epoch 343/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.3272 - acc: 0.8704\n",
      "Epoch 344/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 1.1713 - acc: 0.7778\n",
      "Epoch 345/500\n",
      "162/162 [==============================] - 0s 109us/step - loss: 0.4753 - acc: 0.8519\n",
      "Epoch 346/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 0.8599 - acc: 0.8148\n",
      "Epoch 347/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.4377 - acc: 0.8395\n",
      "Epoch 348/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.4010 - acc: 0.8889\n",
      "Epoch 349/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 1.1232 - acc: 0.7531\n",
      "Epoch 350/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 0.4440 - acc: 0.8580\n",
      "Epoch 351/500\n",
      "162/162 [==============================] - 0s 107us/step - loss: 1.0362 - acc: 0.7901\n",
      "Epoch 352/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 0.1482 - acc: 0.9444\n",
      "Epoch 353/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 0.7858 - acc: 0.8457\n",
      "Epoch 354/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.1351 - acc: 0.9568\n",
      "Epoch 355/500\n",
      "162/162 [==============================] - 0s 113us/step - loss: 1.5209 - acc: 0.7222\n",
      "Epoch 356/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 0.1770 - acc: 0.9321\n",
      "Epoch 357/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 2.4260 - acc: 0.6914\n",
      "Epoch 358/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.2040 - acc: 0.9136\n",
      "Epoch 359/500\n",
      "162/162 [==============================] - 0s 107us/step - loss: 0.6681 - acc: 0.8086\n",
      "Epoch 360/500\n",
      "162/162 [==============================] - 0s 113us/step - loss: 0.5917 - acc: 0.8272\n",
      "Epoch 361/500\n",
      "162/162 [==============================] - 0s 95us/step - loss: 0.2539 - acc: 0.9136\n",
      "Epoch 362/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 1.8593 - acc: 0.6975\n",
      "Epoch 363/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.1963 - acc: 0.9074\n",
      "Epoch 364/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.4501 - acc: 0.8272\n",
      "Epoch 365/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 1.0386 - acc: 0.8333\n",
      "Epoch 366/500\n",
      "162/162 [==============================] - 0s 109us/step - loss: 1.1365 - acc: 0.7531\n",
      "Epoch 367/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.1838 - acc: 0.9198\n",
      "Epoch 368/500\n",
      "162/162 [==============================] - 0s 93us/step - loss: 1.0503 - acc: 0.8025\n",
      "Epoch 369/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.9488 - acc: 0.7963\n",
      "Epoch 370/500\n",
      "162/162 [==============================] - 0s 92us/step - loss: 1.5198 - acc: 0.7716\n",
      "Epoch 371/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 1.2560 - acc: 0.7284\n",
      "Epoch 372/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 0.8829 - acc: 0.7901\n",
      "Epoch 373/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.1598 - acc: 0.9444\n",
      "Epoch 374/500\n",
      "162/162 [==============================] - 0s 108us/step - loss: 0.4727 - acc: 0.8457\n",
      "Epoch 375/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 0.3012 - acc: 0.9198\n",
      "Epoch 376/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 2.6023 - acc: 0.7160\n",
      "Epoch 377/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.7233 - acc: 0.7901\n",
      "Epoch 378/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.7892 - acc: 0.7840\n",
      "Epoch 379/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 0.3546 - acc: 0.8580\n",
      "Epoch 380/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 1.2066 - acc: 0.8025\n",
      "Epoch 381/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 0.5401 - acc: 0.8457\n",
      "Epoch 382/500\n",
      "162/162 [==============================] - 0s 108us/step - loss: 0.1697 - acc: 0.9383\n",
      "Epoch 383/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 4.9082 - acc: 0.5432\n",
      "Epoch 384/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 0.5983 - acc: 0.8148\n",
      "Epoch 385/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 0.1995 - acc: 0.9321\n",
      "Epoch 386/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 1.1076 - acc: 0.8086\n",
      "Epoch 387/500\n",
      "162/162 [==============================] - 0s 108us/step - loss: 0.6048 - acc: 0.8395\n",
      "Epoch 388/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.6114 - acc: 0.8457\n",
      "Epoch 389/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.2724 - acc: 0.9074\n",
      "Epoch 390/500\n",
      "162/162 [==============================] - 0s 108us/step - loss: 1.5252 - acc: 0.8272\n",
      "Epoch 391/500\n",
      "162/162 [==============================] - 0s 107us/step - loss: 0.5316 - acc: 0.8333\n",
      "Epoch 392/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 1.2188 - acc: 0.8333\n",
      "Epoch 393/500\n",
      "162/162 [==============================] - 0s 109us/step - loss: 0.1603 - acc: 0.9383\n",
      "Epoch 394/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 0.5640 - acc: 0.8704\n",
      "Epoch 395/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 0.9142 - acc: 0.8457\n",
      "Epoch 396/500\n",
      "162/162 [==============================] - 0s 93us/step - loss: 0.5441 - acc: 0.8025\n",
      "Epoch 397/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 1.0385 - acc: 0.8272\n",
      "Epoch 398/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.1777 - acc: 0.9136\n",
      "Epoch 399/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 0.4965 - acc: 0.8827\n",
      "Epoch 400/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 1.0351 - acc: 0.7901\n",
      "Epoch 401/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 0.1523 - acc: 0.9321\n",
      "Epoch 402/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 0.3089 - acc: 0.9136\n",
      "Epoch 403/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.5536 - acc: 0.8889\n",
      "Epoch 404/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.5067 - acc: 0.8642\n",
      "Epoch 405/500\n",
      "162/162 [==============================] - 0s 93us/step - loss: 0.5087 - acc: 0.8457\n",
      "Epoch 406/500\n",
      "162/162 [==============================] - 0s 95us/step - loss: 0.4738 - acc: 0.8210\n",
      "Epoch 407/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.6166 - acc: 0.8210\n",
      "Epoch 408/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 1.3951 - acc: 0.8395\n",
      "Epoch 409/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 0.1573 - acc: 0.9259\n",
      "Epoch 410/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 1.1161 - acc: 0.8025\n",
      "Epoch 411/500\n",
      "162/162 [==============================] - 0s 111us/step - loss: 0.7436 - acc: 0.8457\n",
      "Epoch 412/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 0.3049 - acc: 0.8765\n",
      "Epoch 413/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 0.6458 - acc: 0.8580\n",
      "Epoch 414/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.8608 - acc: 0.8272\n",
      "Epoch 415/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 1.0319 - acc: 0.8025\n",
      "Epoch 416/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 0.1504 - acc: 0.9506\n",
      "Epoch 417/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 1.1410 - acc: 0.7531\n",
      "Epoch 418/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.5335 - acc: 0.8333\n",
      "Epoch 419/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 0.7430 - acc: 0.8457\n",
      "Epoch 420/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 0.1295 - acc: 0.9506\n",
      "Epoch 421/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 1.1147 - acc: 0.7778\n",
      "Epoch 422/500\n",
      "162/162 [==============================] - 0s 93us/step - loss: 0.2713 - acc: 0.9136\n",
      "Epoch 423/500\n",
      "162/162 [==============================] - 0s 89us/step - loss: 0.2479 - acc: 0.9012\n",
      "Epoch 424/500\n",
      "162/162 [==============================] - 0s 109us/step - loss: 1.5415 - acc: 0.7407\n",
      "Epoch 425/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 0.1619 - acc: 0.9383\n",
      "Epoch 426/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 0.3136 - acc: 0.8580\n",
      "Epoch 427/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 0.4720 - acc: 0.9136\n",
      "Epoch 428/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.4421 - acc: 0.8827\n",
      "Epoch 429/500\n",
      "162/162 [==============================] - 0s 92us/step - loss: 0.1893 - acc: 0.9321\n",
      "Epoch 430/500\n",
      "162/162 [==============================] - 0s 95us/step - loss: 1.2070 - acc: 0.7778\n",
      "Epoch 431/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.1697 - acc: 0.9321\n",
      "Epoch 432/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 0.1991 - acc: 0.9444\n",
      "Epoch 433/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.1314 - acc: 0.9383\n",
      "Epoch 434/500\n",
      "162/162 [==============================] - 0s 107us/step - loss: 1.3196 - acc: 0.7840\n",
      "Epoch 435/500\n",
      "162/162 [==============================] - 0s 83us/step - loss: 0.5217 - acc: 0.8395\n",
      "Epoch 436/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.8309 - acc: 0.8827\n",
      "Epoch 437/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.9301 - acc: 0.7840\n",
      "Epoch 438/500\n",
      "162/162 [==============================] - 0s 86us/step - loss: 1.8765 - acc: 0.7469\n",
      "Epoch 439/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.4262 - acc: 0.8642\n",
      "Epoch 440/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 1.3673 - acc: 0.8210\n",
      "Epoch 441/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 0.1703 - acc: 0.9444\n",
      "Epoch 442/500\n",
      "162/162 [==============================] - 0s 84us/step - loss: 1.2250 - acc: 0.7531\n",
      "Epoch 443/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 0.3174 - acc: 0.8457\n",
      "Epoch 444/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.7648 - acc: 0.7901\n",
      "Epoch 445/500\n",
      "162/162 [==============================] - 0s 112us/step - loss: 0.9655 - acc: 0.7593\n",
      "Epoch 446/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 1.6045 - acc: 0.7901\n",
      "Epoch 447/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 0.1537 - acc: 0.9259\n",
      "Epoch 448/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 0.1385 - acc: 0.9506\n",
      "Epoch 449/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 0.1457 - acc: 0.9383\n",
      "Epoch 450/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.9331 - acc: 0.8086\n",
      "Epoch 451/500\n",
      "162/162 [==============================] - 0s 111us/step - loss: 0.9639 - acc: 0.7963\n",
      "Epoch 452/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 0.1453 - acc: 0.9383\n",
      "Epoch 453/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 1.1597 - acc: 0.8272\n",
      "Epoch 454/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 0.6329 - acc: 0.8272\n",
      "Epoch 455/500\n",
      "162/162 [==============================] - 0s 109us/step - loss: 0.4627 - acc: 0.8889\n",
      "Epoch 456/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.1680 - acc: 0.9321\n",
      "Epoch 457/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 1.2033 - acc: 0.7531\n",
      "Epoch 458/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.1948 - acc: 0.9198\n",
      "Epoch 459/500\n",
      "162/162 [==============================] - 0s 110us/step - loss: 0.1947 - acc: 0.9259\n",
      "Epoch 460/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 1.3320 - acc: 0.7840\n",
      "Epoch 461/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.9619 - acc: 0.7778\n",
      "Epoch 462/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 0.5607 - acc: 0.9012\n",
      "Epoch 463/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 0.1380 - acc: 0.9568\n",
      "Epoch 464/500\n",
      "162/162 [==============================] - 0s 89us/step - loss: 0.1632 - acc: 0.9321\n",
      "Epoch 465/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 1.1058 - acc: 0.7654\n",
      "Epoch 466/500\n",
      "162/162 [==============================] - 0s 97us/step - loss: 1.0365 - acc: 0.7593\n",
      "Epoch 467/500\n",
      "162/162 [==============================] - 0s 95us/step - loss: 0.2096 - acc: 0.9136\n",
      "Epoch 468/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 0.5073 - acc: 0.8642\n",
      "Epoch 469/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.7575 - acc: 0.8210\n",
      "Epoch 470/500\n",
      "162/162 [==============================] - 0s 100us/step - loss: 1.2104 - acc: 0.7778\n",
      "Epoch 471/500\n",
      "162/162 [==============================] - 0s 105us/step - loss: 0.7200 - acc: 0.7963\n",
      "Epoch 472/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 0.4995 - acc: 0.8272\n",
      "Epoch 473/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 0.2546 - acc: 0.9136\n",
      "Epoch 474/500\n",
      "162/162 [==============================] - 0s 104us/step - loss: 0.5103 - acc: 0.8827\n",
      "Epoch 475/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.5571 - acc: 0.8642\n",
      "Epoch 476/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.6023 - acc: 0.8148\n",
      "Epoch 477/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.4613 - acc: 0.9259\n",
      "Epoch 478/500\n",
      "162/162 [==============================] - 0s 114us/step - loss: 0.6411 - acc: 0.8395\n",
      "Epoch 479/500\n",
      "162/162 [==============================] - 0s 93us/step - loss: 0.6606 - acc: 0.8086\n",
      "Epoch 480/500\n",
      "162/162 [==============================] - 0s 108us/step - loss: 0.8995 - acc: 0.8519\n",
      "Epoch 481/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.4568 - acc: 0.8457\n",
      "Epoch 482/500\n",
      "162/162 [==============================] - 0s 101us/step - loss: 0.1608 - acc: 0.9444\n",
      "Epoch 483/500\n",
      "162/162 [==============================] - 0s 92us/step - loss: 1.4104 - acc: 0.7346\n",
      "Epoch 484/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.1275 - acc: 0.9383\n",
      "Epoch 485/500\n",
      "162/162 [==============================] - 0s 107us/step - loss: 0.4907 - acc: 0.9012\n",
      "Epoch 486/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 0.6555 - acc: 0.8519\n",
      "Epoch 487/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.8527 - acc: 0.8704\n",
      "Epoch 488/500\n",
      "162/162 [==============================] - 0s 98us/step - loss: 0.9262 - acc: 0.8333\n",
      "Epoch 489/500\n",
      "162/162 [==============================] - 0s 96us/step - loss: 1.0336 - acc: 0.7346\n",
      "Epoch 490/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.5241 - acc: 0.8951\n",
      "Epoch 491/500\n",
      "162/162 [==============================] - 0s 99us/step - loss: 0.1491 - acc: 0.9444\n",
      "Epoch 492/500\n",
      "162/162 [==============================] - 0s 113us/step - loss: 0.1553 - acc: 0.9506\n",
      "Epoch 493/500\n",
      "162/162 [==============================] - 0s 108us/step - loss: 1.3053 - acc: 0.7099\n",
      "Epoch 494/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 108us/step - loss: 0.1417 - acc: 0.9444\n",
      "Epoch 495/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 0.4870 - acc: 0.8642\n",
      "Epoch 496/500\n",
      "162/162 [==============================] - 0s 106us/step - loss: 0.3178 - acc: 0.9136\n",
      "Epoch 497/500\n",
      "162/162 [==============================] - 0s 94us/step - loss: 0.8466 - acc: 0.8272\n",
      "Epoch 498/500\n",
      "162/162 [==============================] - 0s 102us/step - loss: 0.3399 - acc: 0.8765\n",
      "Epoch 499/500\n",
      "162/162 [==============================] - 0s 103us/step - loss: 0.3172 - acc: 0.8889\n",
      "Epoch 500/500\n",
      "162/162 [==============================] - 0s 110us/step - loss: 0.9455 - acc: 0.8704\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'knn balanced': {'acc': 0.5345744680851063,\n",
       "  'dmc': -1035.0,\n",
       "  'f2': 0.08253094910591471},\n",
       " 'knn unbalanced': {'acc': 0.9361702127659575, 'dmc': -580.0, 'f2': 0.0},\n",
       " 'lda balanced': {'acc': 0.8138297872340425,\n",
       "  'dmc': -235.0,\n",
       "  'f2': 0.2911392405063291},\n",
       " 'lda unbalanced': {'acc': 0.9414893617021277,\n",
       "  'dmc': -545.0,\n",
       "  'f2': 0.1851851851851852},\n",
       " 'logistic regression balanced': {'acc': 0.8829787234042553,\n",
       "  'dmc': -105.0,\n",
       "  'f2': 0.3951890034364261},\n",
       " 'logistic regression unbalanced': {'acc': 0.9547872340425532,\n",
       "  'dmc': -270.0,\n",
       "  'f2': 0.632183908045977},\n",
       " 'mlp balanced': {'dmc': -320.0, 'f2': 0.257985257985258},\n",
       " 'mlp unbalanced': {'dmc': -575.0, 'f2': 0.0},\n",
       " 'rfc balanced': {'acc': 0.07446808510638298,\n",
       "  'dmc': -1650.0,\n",
       "  'f2': 0.0733822548365577},\n",
       " 'rfc unbalanced': {'acc': 0.9388297872340425, 'dmc': -575.0, 'f2': 0.0},\n",
       " 'svc balanced': {'acc': 0.07446808510638298,\n",
       "  'dmc': -1650.0,\n",
       "  'f2': 0.0733822548365577},\n",
       " 'svc unbalanced': {'acc': 0.9388297872340425, 'dmc': -575.0, 'f2': 0.0},\n",
       " 'xgb balanced': {'acc': 0.925531914893617,\n",
       "  'dmc': -50.0,\n",
       "  'f2': 0.502283105022831},\n",
       " 'xgb unbalanced': {'acc': 0.973404255319149,\n",
       "  'dmc': -85.0,\n",
       "  'f2': 0.7943925233644861}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1503/1503 [==============================] - 0s 89us/step - loss: 0.5027 - acc: 0.9115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/Projects/dmc2019/venv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "mlp_unbalanced = model.fit(df_train_unbalanced_x,df_train_unbalanced_y)\n",
    "res['mlp unbalanced'] = calculate_metrics(df_val_x, df_val_y, model, \"keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'knn balanced': {'acc': 0.5345744680851063,\n",
       "  'dmc': -1035.0,\n",
       "  'f2': 0.08253094910591471},\n",
       " 'knn unbalanced': {'acc': 0.9361702127659575, 'dmc': -580.0, 'f2': 0.0},\n",
       " 'lda balanced': {'acc': 0.8138297872340425,\n",
       "  'dmc': -235.0,\n",
       "  'f2': 0.2911392405063291},\n",
       " 'lda unbalanced': {'acc': 0.9414893617021277,\n",
       "  'dmc': -545.0,\n",
       "  'f2': 0.1851851851851852},\n",
       " 'logistic regression balanced': {'acc': 0.8829787234042553,\n",
       "  'dmc': -105.0,\n",
       "  'f2': 0.3951890034364261},\n",
       " 'logistic regression unbalanced': {'acc': 0.9547872340425532,\n",
       "  'dmc': -270.0,\n",
       "  'f2': 0.632183908045977},\n",
       " 'mlp balanced': {'dmc': -705.0, 'f2': 0.14685314685314685},\n",
       " 'mlp unbalanced': {'dmc': -575.0, 'f2': 0.0},\n",
       " 'rfc balanced': {'acc': 0.07446808510638298,\n",
       "  'dmc': -1650.0,\n",
       "  'f2': 0.0733822548365577},\n",
       " 'rfc unbalanced': {'acc': 0.9388297872340425, 'dmc': -575.0, 'f2': 0.0},\n",
       " 'svc balanced': {'acc': 0.07446808510638298,\n",
       "  'dmc': -1650.0,\n",
       "  'f2': 0.0733822548365577},\n",
       " 'svc unbalanced': {'acc': 0.9388297872340425, 'dmc': -575.0, 'f2': 0.0},\n",
       " 'xgb balanced': {'acc': 0.925531914893617,\n",
       "  'dmc': -50.0,\n",
       "  'f2': 0.502283105022831},\n",
       " 'xgb unbalanced': {'acc': 0.973404255319149,\n",
       "  'dmc': -85.0,\n",
       "  'f2': 0.7943925233644861}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
