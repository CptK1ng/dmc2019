{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with the Test split?\n",
    "This is what we want to do:\n",
    "\n",
    "1. predict batches with ...\n",
    ".TODO: create list of content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.) set run-config and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_SUBMISSION = True # will perform a test on a validation split if set to False\n",
    "\n",
    "TEST_BATCH_SIZE = 500 # Number of Test entries to add to the training set for the next iteration\n",
    "ITER_PRINT_EVERY = 1 # Which Iterations to print (every nth)\n",
    "\n",
    "TRAIN_WITHOUT_HIGHTRUST = False # Set to True to train classifiers only on rows with trustLevel <= 2\n",
    "\n",
    "# Enable Quick test mode to test functionality on a smaller part of original data or with less iterations etc. than in final version\n",
    "QUICK_TEST_MODE = False\n",
    "#TODO!!! Bug: Plot function plot_results_ssl(res) has error sometimes, e.g. for TEST_BATCH_SIZE=250 and QUICK_TEST_MODE_TEST_SIZE = 1000 or 251. I think it might be when QUICK_TEST_MODE_TEST_SIZE is a multiple of TEST_BATCH_SIZE or if nr. iterations is pretty small\n",
    "QUICK_TEST_MODE_TEST_SIZE = 4000 #set this to the number of test samples you want to use\n",
    "#TODO lower other hyperparams as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors, KDTree\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.) define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scores(y_test, y_pred):\n",
    "    y_test_tmp = y_test.copy()\n",
    "    accuracy = metrics.accuracy_score(y_test_tmp, y_pred)\n",
    "    confusion_matrix = (metrics.confusion_matrix(y_test_tmp, y_pred)).tolist()\n",
    "    dmc_score = np.sum(confusion_matrix * np.array([[0, -25], [-5, 5]]))\n",
    "    return accuracy, dmc_score, confusion_matrix\n",
    "\n",
    "def get_classifier(name):\n",
    "    \"\"\"\n",
    "    Old: \n",
    "    XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "                colsample_bytree=1, disable_default_eval_metric=1,eval_metric='aucpr',\n",
    "                gamma=1.8285912697052542, reg_lambda=0.4149772770711012,\n",
    "                max_bin=254, max_delta_step=7.2556696256684035,\n",
    "                max_depth=3, min_child_weight=1.0317712458399741, missing=None,\n",
    "                n_estimators=445, n_jobs=-1, objective='binary:logistic', random_state=0, reg_alpha=0,\n",
    "                scale_pos_weight=1,silent=True,\n",
    "                subsample=1, tree_method='gpu_hist', verbosity=2, seed=42)\n",
    "    NEW: \n",
    "    XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, disable_default_eval_metric=1,\n",
    "       eval_metric='aucpr', gamma=0.8785511762914533,\n",
    "       learnin_rate=0.6349847443673119, learning_rate=0.1,\n",
    "       max_delta_step=8.564303568772093, max_depth=3,\n",
    "       min_child_weight=1.3399467345621474, missing=None, n_estimators=448,\n",
    "       n_gpus=1, n_jobs=1, nthread=None, objective='binary:logistic',\n",
    "       random_state=0, reg_alpha=0, reg_lambda=0.16733567917931627,\n",
    "       scale_pos_weight=1, seed=None, silent=True, subsample=1,\n",
    "       tree_method='gpu_hist', verbosity=2)\n",
    "    \"\"\"\n",
    "    return {\n",
    "            'xgb': XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "                       colsample_bytree=1, disable_default_eval_metric=1,\n",
    "                       eval_metric='aucpr', gamma=0.8785511762914533,\n",
    "                       learnin_rate=0.6349847443673119, learning_rate=0.05,\n",
    "                       max_delta_step=8.564303568772093, max_depth=3,\n",
    "                       min_child_weight=1.3399467345621474, n_estimators=448,\n",
    "                       n_gpus=1, n_jobs=1,  objective='binary:logistic',\n",
    "                       random_state=42, reg_alpha=0, reg_lambda=0.16733567917931627,\n",
    "                       scale_pos_weight=1, silent=True, subsample=1,\n",
    "                       tree_method='gpu_hist', verbosity=2)\n",
    "                if not TRAIN_WITHOUT_HIGHTRUST else \n",
    "                    XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "                       colsample_bytree=1, disable_default_eval_metric=1,\n",
    "                       eval_metric='aucpr', gamma=0.5361394269637965,\n",
    "                       learning_rate=0.05,\n",
    "                       max_delta_step=2.809035999095031, max_depth=2,\n",
    "                       min_child_weight=0.9672585190973582, missing=None, n_estimators=403,\n",
    "                       n_gpus=1, n_jobs=1, nthread=None, objective='binary:logistic',\n",
    "                       random_state=0, reg_alpha=0, reg_lambda=0.16439595189061076,\n",
    "                       scale_pos_weight=1, seed=None, silent=True, subsample=1,\n",
    "                       tree_method='gpu_hist', verbosity=2),\n",
    "            'svc': SVC(C=11.439334564226868, cache_size=8000, class_weight=None, coef0=0.0,\n",
    "                        decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
    "                        kernel='linear', max_iter=-1, probability=True, random_state=42,\n",
    "                        shrinking=False, tol=0.08370638742373739, verbose=0)\n",
    "                if not TRAIN_WITHOUT_HIGHTRUST else \n",
    "                    SVC(C=5.675273359994213, cache_size=8000, class_weight=None, coef0=0.0,\n",
    "                        decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
    "                        kernel='linear', max_iter=-1, probability=True, random_state=42,\n",
    "                        shrinking=False, tol=0.07958926694361011, verbose=0)\n",
    "\n",
    "\n",
    "        \n",
    "    }[name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.) Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Xy_original_df = pd.read_csv(\"../data/train.csv\", sep=\"|\") if FINAL_SUBMISSION else pd.read_csv(\"../data/train_new.csv\", sep=\"|\")\n",
    "test_X_original_df  = pd.read_csv(\"../data/test.csv\", sep=\"|\", nrows=None if not QUICK_TEST_MODE else QUICK_TEST_MODE_TEST_SIZE) # For faster testing we can use less data from the test set\n",
    "\n",
    "#Only for test routines\n",
    "val_Xy_original_df = pd.read_csv(\"../data/val_new.csv\", sep=\"|\")\n",
    "train_complete_Xy_original_df = pd.read_csv(\"../data/train.csv\", sep=\"|\")\n",
    "test_final_X_df = pd.read_csv(\"../data/test.csv\", sep=\"|\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.) Prepare Input X and Label Y Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convention for variables names: datasetname_columntype_transformstatus_dataframeornot\n",
    "train_y_original_df = train_Xy_original_df[[\"fraud\"]].copy()\n",
    "train_X_original_df = train_Xy_original_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "\n",
    "# Only for test routie#nes\n",
    "val_y_originial_df = val_Xy_original_df[[\"fraud\"]].copy()\n",
    "val_X_originial_df = val_Xy_original_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "train_complete_y_originial_df = train_complete_Xy_original_df[[\"fraud\"]].copy()\n",
    "train_complete_X_originial_df = train_complete_Xy_original_df.copy().drop(\"fraud\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.) Data Transformation and data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>totalScannedLineItems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.291111</td>\n",
       "      <td>-0.848505</td>\n",
       "      <td>1.332970</td>\n",
       "      <td>-0.433864</td>\n",
       "      <td>0.947966</td>\n",
       "      <td>0.878671</td>\n",
       "      <td>-0.101899</td>\n",
       "      <td>-0.019018</td>\n",
       "      <td>-0.124066</td>\n",
       "      <td>-0.979100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.294480</td>\n",
       "      <td>0.167044</td>\n",
       "      <td>0.311624</td>\n",
       "      <td>0.436284</td>\n",
       "      <td>0.315796</td>\n",
       "      <td>-0.877634</td>\n",
       "      <td>-0.079026</td>\n",
       "      <td>-0.095174</td>\n",
       "      <td>-0.360512</td>\n",
       "      <td>1.328388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  \\\n",
       "0    0.291111               -0.848505    1.332970      -0.433864   \n",
       "1   -0.294480                0.167044    0.311624       0.436284   \n",
       "\n",
       "   scansWithoutRegistration  quantityModifications  scannedLineItemsPerSecond  \\\n",
       "0                  0.947966               0.878671                  -0.101899   \n",
       "1                  0.315796              -0.877634                  -0.079026   \n",
       "\n",
       "   valuePerSecond  lineItemVoidsPerPosition  totalScannedLineItems  \n",
       "0       -0.019018                 -0.124066              -0.979100  \n",
       "1       -0.095174                 -0.360512               1.328388  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "transformer = DataTransformer(scaler)\n",
    "\n",
    "if not TRAIN_WITHOUT_HIGHTRUST:\n",
    "    train_lowtrust_X_original_df = transformer.remove_high_trust(train_X_original_df)\n",
    "\n",
    "# Adding new Features to train and test set\n",
    "train_X_unscaled_df = transformer.add_features(train_X_original_df if not TRAIN_WITHOUT_HIGHTRUST else train_lowtrust_X_original_df)\n",
    "test_X_unscaled_df = transformer.add_features(test_X_original_df)\n",
    "\n",
    "val_X_unscaled_df = transformer.add_features(val_X_originial_df)\n",
    "train_complete_X_unscaled_df = transformer.add_features(train_complete_X_originial_df) \n",
    "\n",
    "transformer.fit_scaler(transformer.add_features(train_complete_X_unscaled_df.append(test_X_unscaled_df, sort=False)))\n",
    "train_X_scaled_df = transformer.apply_scaler(train_X_unscaled_df)\n",
    "\n",
    "test_X_scaled_df  = transformer.apply_scaler(test_X_unscaled_df)\n",
    "val_X_scaled_df = transformer.apply_scaler(val_X_unscaled_df)\n",
    "train_complete_X_scaled_df = transformer.apply_scaler(train_complete_X_unscaled_df)\n",
    "\n",
    "\n",
    "# labels\n",
    "# TODO: this seems redundant^^\n",
    "\n",
    "train_y_df = train_y_original_df.copy()\n",
    "val_y_df = val_y_originial_df.copy()\n",
    "train_complete_y_df = train_complete_y_originial_df.copy()\n",
    "\n",
    "test_final_X_df = transformer.add_features(test_final_X_df)\n",
    "\n",
    "\n",
    "test_X_scaled_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name     | New Features | Scaled |\n",
    "|----------|--------------|--------|\n",
    "| orig     |      [ ]     |   [ ]  |\n",
    "| unscaled |      [X]     |   [ ]  |\n",
    "| scaled   |      [X]     |   [X]  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_shallow_nn(xgboost_fitted, linear_svc_fitted, data_to_predict, transformer, model):\n",
    "    prediction = []\n",
    "    data_to_predict_scaled = transformer.apply_scaler(data_to_predict.copy().reset_index(drop=True))\n",
    "    data_to_predict_unscaled = data_to_predict.copy().reset_index(drop=True)\n",
    "    \n",
    "    threshhold = 25/35    \n",
    "    for i, row in data_to_predict_unscaled.iterrows():\n",
    "        \n",
    "        if row.trustLevel >= 3:\n",
    "            prediction.append(0)\n",
    "\n",
    "        else:\n",
    "            xgb_pred = xgboost_fitted.predict([data_to_predict_unscaled.iloc[i].values])[0]\n",
    "            xgb_prob = max(np.ravel(xgboost_fitted.predict_proba([data_to_predict_unscaled.iloc[i].values])))\n",
    "            #print(\"XGB Prob: \", xgb_prob)\n",
    "            lsvc_pred = linear_svc_fitted.predict([data_to_predict_scaled.iloc[i]])[0]\n",
    "            lsvc_prob = max(np.ravel(linear_svc_fitted.predict_proba([data_to_predict_scaled.iloc[i].values])))\n",
    "            #print(\"LinSVC Prob: \", lsvc_prob)\n",
    "            \n",
    "            res = np.ravel(model.predict(np.array([lsvc_pred,lsvc_prob,xgb_pred,xgb_prob]).reshape((1, -1))))[0]\n",
    "            prediction.append(1 if res > 0.5 else 0)\n",
    "    return pd.DataFrame({\"fraud\": prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_cv(xgboost_fitted, linear_svc_fitted, data_to_predict, transformer, model):\n",
    "    prediction = []\n",
    "    data_to_predict_scaled = transformer.apply_scaler(data_to_predict.copy().reset_index(drop=True))\n",
    "    data_to_predict_unscaled = data_to_predict.copy().reset_index(drop=True)\n",
    "    \n",
    "    threshhold = 25/35    \n",
    "    for i, row in data_to_predict_unscaled.iterrows():\n",
    "        \n",
    "        if row.trustLevel >= 3:\n",
    "            prediction.append(0)\n",
    "\n",
    "        else:\n",
    "            xgb_pred = xgboost_fitted.predict([data_to_predict_unscaled.iloc[i].values])[0]\n",
    "            xgb_prob = max(np.ravel(xgboost_fitted.predict_proba([data_to_predict_unscaled.iloc[i].values])))\n",
    "            #print(\"XGB Prob: \", xgb_prob)\n",
    "            lsvc_pred = linear_svc_fitted.predict([data_to_predict_scaled.iloc[i]])[0]\n",
    "            lsvc_prob = max(np.ravel(linear_svc_fitted.predict_proba([data_to_predict_scaled.iloc[i].values])))\n",
    "            #print(\"LinSVC Prob: \", lsvc_prob)\n",
    "            \n",
    "            res = np.ravel(model.predict(np.array([lsvc_pred,lsvc_prob,xgb_pred,xgb_prob]).reshape((1, -1))))[0]\n",
    "            prediction.append(1 if res > 0.5 else 0)\n",
    "    return pd.DataFrame(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_supervised_learning_procedure(test_X_unscaled,train_X_unscaled, train_y, test_data_dict, transformer, model):\n",
    "    test_data_dict = test_data_dict\n",
    "    pred_train_dataframes = []\n",
    "    pred_val_dataframes = []\n",
    "    results = []\n",
    "    \n",
    "    # initialize pseudo labeled train (pltrain) dataset with the transformed training data\n",
    "    pltrain_X_unscaled_df = train_X_unscaled.copy()\n",
    "    train_X_scaled_df = transformer.apply_scaler(train_X_unscaled)\n",
    "\n",
    "    pltrain_X_unscaled_df = test_data_dict['train_X_unscaled_df'].copy()\n",
    "    pltrain_X_scaled_df = transformer.apply_scaler(pltrain_X_unscaled_df)\n",
    "    pltrain_y_df = test_data_dict['train_y_df']\n",
    "    \n",
    "    \n",
    "    pltrain_y_df = train_y.copy()\n",
    "    train_X_scaled_len = len(train_X_unscaled)\n",
    "    train_X_scaled_len = len(pltrain_X_unscaled_df)\n",
    "    print(\"{} available train data before pseudo labeling\".format(train_X_scaled_len))\n",
    "\n",
    "    train_X_scaled_df = transformer.apply_scaler(train_X_unscaled)\n",
    "\n",
    "    linear_svc_initial = get_classifier('svc')\n",
    "    linear_svc_initial.fit(train_X_scaled_df.values, pltrain_y_df.values)\n",
    "    xgboost_initial = get_classifier('xgb')\n",
    "    xgboost_initial.fit(train_X_unscaled.values, pltrain_y_df.values)\n",
    "    \n",
    "   \n",
    "    print(\"with a batchsize of \",TEST_BATCH_SIZE, \"we will need\", len(test_X_unscaled)/TEST_BATCH_SIZE, \"iterations:\")\n",
    "    for i in range(TEST_BATCH_SIZE, len(test_X_unscaled), TEST_BATCH_SIZE):\n",
    "        if int(i / TEST_BATCH_SIZE) % ITER_PRINT_EVERY == 0:\n",
    "            print(\"iteration\", int(i / TEST_BATCH_SIZE), \"\\t/\", int(np.ceil(len(test_X_unscaled) / TEST_BATCH_SIZE)),\n",
    "                  \"with batch from\", i - TEST_BATCH_SIZE, \"\\t to\", i, \", training with\", len(pltrain_y_df), \"samples\")\n",
    "        # get batch from test set\n",
    "        testbatch_X_df = test_X_unscaled.iloc[i - TEST_BATCH_SIZE:i].copy().reset_index(drop=True)\n",
    "                \n",
    "\n",
    "        \"\"\"\n",
    "        ----------------------------------------------------NEW PART ------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        \n",
    "        pltrain_X_unscaled_df, pltrain_y_df = get_extended_pltrain_for_batch_shallow_nn(testbatch_X_df, pltrain_X_unscaled_df,\n",
    "                                                                             pltrain_y_df, transformer, model)\n",
    "        pltrain_X_scaled_df = transformer.apply_scaler(pltrain_X_unscaled_df)\n",
    "        test_data_dict['pltrain_X_unscaled_df'] = pltrain_X_unscaled_df.copy()\n",
    "        test_data_dict['pltrain_X_scaled_df'] = pltrain_X_scaled_df.copy()\n",
    "        test_data_dict['pltrain_y_df'] = pltrain_y_df.copy()\n",
    "        \n",
    "        \n",
    "        linear_svc = get_classifier('svc')\n",
    "        linear_svc.fit(pltrain_X_scaled_df.values, pltrain_y_df.values)\n",
    "        \n",
    "        \n",
    "        xgboost = get_classifier('xgb')\n",
    "        xgboost.fit(pltrain_X_unscaled_df.values, pltrain_y_df.values)\n",
    "        \n",
    "        \n",
    "        # Only Test from PLTrain without the data from train\n",
    "        test_data_dict['test_X_unscaled_df'] = pltrain_X_unscaled_df.iloc[train_X_scaled_len:]\n",
    "        test_data_dict['test_X_scaled_df'] = pltrain_X_scaled_df.iloc[train_X_scaled_len:]\n",
    "        test_data_dict['test_y_df'] = pltrain_y_df.iloc[train_X_scaled_len:]\n",
    "        \n",
    "        \n",
    "        res, pred_train_df, pred_val_df = test_routine_shallow_nn(test_data_dict, transformer, model)\n",
    "       \n",
    "        results.append(res)\n",
    "        pred_train_dataframes.append(pred_train_df)\n",
    "        pred_val_dataframes.append(pred_val_df)\n",
    "        \n",
    "        # TODO: also output prediction for validation set when trained on full final dataset\n",
    "        if int(i / TEST_BATCH_SIZE) % ITER_PRINT_EVERY == 0:\n",
    "            print(\"Shallow NN:   XGBoost: PLTrain auf Val: {} --- PLTest auf Train: {} || LinearSVC:  PLTrain auf Val: {} --- PLTest auf Train: {} || Own Classifier:  PLTrain auf Val: {} --- PLTest auf Train: {}\".format(\n",
    "                res['xgboost']['val']['dmc_score'],res['xgboost']['train']['dmc_score'],res['lin_svc']['val']['dmc_score'],res['lin_svc']['train']['dmc_score'],res['own_classifier']['val']['dmc_score'],res['own_classifier']['train']['dmc_score']))\n",
    "        \n",
    "        break\n",
    "    xgb_final = get_classifier(\"xgb\")\n",
    "    lsvc_final = get_classifier(\"svc\")\n",
    "    \n",
    "    xgb_final.fit(pltrain_X_unscaled_df.values, pltrain_y_df.values)\n",
    "    lsvc_final.fit(pltrain_X_scaled_df.values, pltrain_y_df.values)\n",
    "    final_prediction = classify_with_shallow_nn(xgb_final, lsvc_final, test_final_X_df, transformer,model)\n",
    "    \n",
    "    \n",
    "    return results, final_prediction, pred_val_dataframes, pred_val_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.) iterative model training using pseudo-labeling\n",
    "predict batches of the test set, add them to the previous training set and use this new training set to predict the next batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extended_pltrain_for_batch_shallow_nn(testbatch_X_unscaled_df, pltrain_X_unscaled_df, pltrain_y_df,transformer, model):\n",
    "    # train a classificator on the pseudo labeled train (pltrain) dataset\n",
    "    pltrain_X_scaled_df = transformer.apply_scaler(pltrain_X_unscaled_df)\n",
    "    linear_svc = get_classifier('svc')\n",
    "    linear_svc.fit(pltrain_X_scaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "    xgboost = get_classifier('xgb')\n",
    "    xgboost.fit(pltrain_X_unscaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "    testbatch_y = classify_with_shallow_nn(xgboost, linear_svc, testbatch_X_unscaled_df, transformer, model)\n",
    "    \n",
    "    # add batch to pseudo labeled train (pltrain) dataset. needs to ignore index as ids in test also start with 0\n",
    "    pltrainnew_X_unscaled_df = pltrain_X_unscaled_df.append(testbatch_X_unscaled_df, ignore_index=True)\n",
    "    pltrainnew_y_df = pltrain_y_df.append(testbatch_y,\n",
    "                                          ignore_index=True)  # pltrainnew_Xy_unscaled_df[['fraud']], ignore_index=True)\n",
    "    return pltrainnew_X_unscaled_df, pltrainnew_y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extended_pltrain_for_batch_shallow_nn_cv(testbatch_X_unscaled_df, pltrain_X_unscaled_df, pltrain_y_df,transformer, model):\n",
    "    # train a classificator on the pseudo labeled train (pltrain) dataset\n",
    "    pltrain_X_scaled_df = transformer.apply_scaler(pltrain_X_unscaled_df)\n",
    "    linear_svc = get_classifier('svc')\n",
    "    linear_svc.fit(pltrain_X_scaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "    xgboost = get_classifier('xgb')\n",
    "    xgboost.fit(pltrain_X_unscaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "    testbatch_y = classify_cv(xgboost, linear_svc, testbatch_X_unscaled_df, transformer, model)\n",
    "    \n",
    "    # add batch to pseudo labeled train (pltrain) dataset. needs to ignore index as ids in test also start with 0\n",
    "    pltrainnew_X_unscaled_df = pltrain_X_unscaled_df.append(testbatch_X_unscaled_df, ignore_index=True)\n",
    "    pltrainnew_y_df = pltrain_y_df.append(testbatch_y,\n",
    "                                          ignore_index=True)  # pltrainnew_Xy_unscaled_df[['fraud']], ignore_index=True)\n",
    "    return pltrainnew_X_unscaled_df, pltrainnew_y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Xy_for_crossval = train_complete_X_unscaled_df.copy()\n",
    "train_Xy_for_crossval['fraud'] = train_complete_y_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "keras_model = None\n",
    "with open('../data/model_architecture_all.json', 'r') as f:\n",
    "    keras_model = model_from_json(f.read())\n",
    "    \n",
    "keras_model.load_weights(\"../data/nn_weights_all.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1879 available train data before pseudo labeling\n",
      "with a batchsize of  500 we will need 996.242 iterations:\n",
      "iteration 1 \t/ 997 with batch from 0 \t to 500 , training with 1879 samples\n",
      "Shallow NN:   XGBoost: PLTrain auf Val: 115 --- PLTest auf Train: -65 || LinearSVC:  PLTrain auf Val: 80 --- PLTest auf Train: 195 || Own Classifier:  PLTrain auf Val: 80 --- PLTest auf Train: 195\n"
     ]
    }
   ],
   "source": [
    "data_dict = dict()\n",
    "data_dict['train_complete_X_unscaled_df'] = train_complete_X_unscaled_df.copy()\n",
    "data_dict['train_complete_X_scaled_df'] = transformer.apply_scaler(train_complete_X_unscaled_df)\n",
    "data_dict['train_complete_y_df'] = train_complete_y_df.copy()\n",
    "\n",
    "data_dict['val_X_unscaled_df'] = val_X_unscaled_df.copy()\n",
    "data_dict['val_X_scaled_df'] = transformer.apply_scaler(val_X_unscaled_df)\n",
    "data_dict['val_y_df'] = val_y_df.copy()\n",
    "data_dict['train_X_unscaled_df'] = train_X_unscaled_df\n",
    "data_dict['train_X_scaled_df'] = train_X_scaled_df\n",
    "data_dict['train_y_df'] = train_y_df\n",
    "\n",
    "\n",
    "# results, final_prediction, knn_dataframes, pred_dfs, pred_val_dfs, results_wo_knn, final_prediction_wo_knn,  pred_train_wo_knn_dfs, pred_val_wo_knn_dfs\n",
    "results, final_prediction,  pred_train_dfs, pred_val_dfs= semi_supervised_learning_procedure(test_X_unscaled_df, train_X_unscaled_df, train_y_df, data_dict, transformer, keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>498121.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.04831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.21442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              fraud\n",
       "count  498121.00000\n",
       "mean        0.04831\n",
       "std         0.21442\n",
       "min         0.00000\n",
       "25%         0.00000\n",
       "50%         0.00000\n",
       "75%         0.00000\n",
       "max         1.00000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prediction.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_final_X_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e5e635df719e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#plot_results_ssl(results)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_data_Xy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_final_X_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_data_Xy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fraud'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_data_Xy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_final_X_df' is not defined"
     ]
    }
   ],
   "source": [
    "#plot_results_ssl(results)\n",
    "test_data_Xy = test_final_X_df\n",
    "test_data_Xy['fraud'] = final_prediction\n",
    "test_data_Xy.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_Xy.to_csv(\"../data/test_pl.csv\", sep=\"|\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pred_val_dfs[0]\n",
    "val[val.fraud != val.own_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val[val.fraud != val.lsvc_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val[val.fraud != val.xgb_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val2 = pred_val_dfs[10]\n",
    "val2[val2.fraud != val2.own_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>totalScannedLineItems</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1054</td>\n",
       "      <td>54.70</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.027514</td>\n",
       "      <td>0.051898</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>108</td>\n",
       "      <td>27.36</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  \\\n",
       "0           5                    1054       54.70              7   \n",
       "1           3                     108       27.36              5   \n",
       "\n",
       "   scansWithoutRegistration  quantityModifications  scannedLineItemsPerSecond  \\\n",
       "0                         0                      3                   0.027514   \n",
       "1                         2                      4                   0.129630   \n",
       "\n",
       "   valuePerSecond  lineItemVoidsPerPosition  totalScannedLineItems  fraud  \n",
       "0        0.051898                  0.241379                   29.0      0  \n",
       "1        0.253333                  0.357143                   14.0      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_complete_Xy = train_complete_X_unscaled_df.copy()\n",
    "train_complete_Xy['fraud'] = train_complete_Xy_original_df.fraud\n",
    "train_complete_Xy.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Own Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DMC Mean: 64.0 --- DMC Sum: 320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64.0, 320)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "own_classifier_cross_validation(train_complete_Xy, test_X_unscaled_df, 1, 5, transformer,keras_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best n testsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking  500 testsamples\n",
      "DMC Mean: 58.0 --- DMC Sum: 290\n",
      "DMC Mean: 66.0 --- DMC Sum: 330\n",
      "DMC Mean: 64.0 --- DMC Sum: 320\n",
      "DMC Mean: 63.0 --- DMC Sum: 315\n",
      "DMC Mean: 63.0 --- DMC Sum: 315\n",
      "DMC Mean: 57.0 --- DMC Sum: 285\n",
      "DMC Mean: 60.0 --- DMC Sum: 300\n",
      "DMC Mean: 65.0 --- DMC Sum: 325\n",
      "DMC Mean: 62.0 --- DMC Sum: 310\n",
      "DMC Mean: 54.0 --- DMC Sum: 270\n",
      "Mean of DMC Mean is 61.2 and DMC Sum is 306.0 for 500 testsamples\n"
     ]
    }
   ],
   "source": [
    "for i in [500]:\n",
    "    print(\"Taking \",i, \"testsamples\")\n",
    "    dmc_scores = 0\n",
    "    dmc_sum = 0\n",
    "    for j in range(10):\n",
    "        train_Xy_for_crossval = train_complete_X_unscaled_df.copy()\n",
    "        train_Xy_for_crossval['fraud'] = train_complete_y_df.copy()\n",
    "        test_X_for_crossval = test_X_unscaled_df.copy()\n",
    "        res = own_classifier_cross_validation(train_Xy_for_crossval.copy().sample(frac=1),test_X_for_crossval,i,5,transformer, keras_model)\n",
    "        dmc_scores += res[0]\n",
    "        dmc_sum += res[1]\n",
    "    print(\"Mean of DMC Mean is {} and DMC Sum is {} for {} testsamples\".format(dmc_scores/10,dmc_sum/10, i, ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Traindataset for neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 . Split\n",
      "2 . Split\n",
      "3 . Split\n",
      "4 . Split\n",
      "5 . Split\n"
     ]
    }
   ],
   "source": [
    "pred_val_set = create_nn_train_data(train_Xy_for_crossval, 5, transformer)\n",
    "pred_val_set.to_csv(\"../data/nn_train_data_all.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Define train1\n",
    "\n",
    "\"\"\"\n",
    "prf_own = metrics.precision_recall_fscore_support(train1.fraud, train1.own_predict, beta=0.5172, average='binary')\n",
    "prf_xgb = metrics.precision_recall_fscore_support(train1.fraud, train1.xgb_predict, beta=0.5172, average='binary')\n",
    "prf_lsvc = metrics.precision_recall_fscore_support(train1.fraud, train1.lsvc_predict, beta=0.5172, average='binary')\n",
    "dmc_score_own = np.sum(metrics.confusion_matrix(train1.fraud, train1.own_predict)* np.array([[0, -25], [-5, 5]]))\n",
    "dmc_score_xgb = np.sum(metrics.confusion_matrix(train1.fraud, train1.xgb_predict)* np.array([[0, -25], [-5, 5]]))\n",
    "dmc_score_lsvc = np.sum(metrics.confusion_matrix(train1.fraud, train1.lsvc_predict)* np.array([[0, -25], [-5, 5]]))\n",
    "\n",
    "\n",
    "print(\"OWN CLASSIFIER: Precision: {} Recall: {} Fbeta: {} DMC: {}\".format(prf[0], prf[1], prf[2],dmc_score_own))\n",
    "print(\"XGBOOST: Precision: {} Recall: {} Fbeta: {} DMC: {}\".format(prf_xgb[0], prf_xgb[1], prf_xgb[2],dmc_score_xgb))\n",
    "print(\"LINEAR SVCPrecision: {} Recall: {} Fbeta: {} DMC: {}\".format(prf_lsvc[0], prf_lsvc[1], prf_lsvc[2],dmc_score_lsvc))\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.) pseudo-label the test set and create new classifier based on this\n",
    "first we predict the original test data labels using the new extended pltrain from above cell and second we use this test data labels to train a new classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.) evaluate our new classifier with the original training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.) combine the pseudo labeled test set with the original train data to train our final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_clf = ...\n",
    "#TODO; it would be cool if our classify loop can get a sklearn-like api for fit and predict, but we run out of time.\n",
    "#      we can instead extract final classification from the existing dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.) predict labels for the test set using our final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_testpred_y_original = final_clf.predict(test_X_scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.) generate output file neeeded for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(final_testpred_y_original, columns=[\"fraud\"]).to_csv(\"HS_Karlsruhe_1.csv\", index=False)\n",
    "#pd.read_csv(\"HS_Karlsruhe_1.csv\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.) evaluate our new classifier with the validation set\n",
    "Now at the very end we can also test our final model on a validation split never used before. just for comparison. \n",
    "\n",
    "**For the final submission, the following code should will not be run and the full train (incl. this val split) set will be used above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not FINAL_SUBMISSION:\n",
    "#    final_valpred_y_original = final_clf.predict(val_X_scaled_df)\n",
    "#    print(calc_scores(val_y_original_df.fraud.values, final_valpred_y_original))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
