{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with the Test split?\n",
    "This is what we want to do:\n",
    "\n",
    "1. predict batches with ...\n",
    ".TODO: create list of content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.) set run-config and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_SUBMISSION = False # will perform a test on a validation split if set to False\n",
    "\n",
    "TEST_BATCH_SIZE = 200 # Number of Test entries to add to the training set for the next iteration\n",
    "ITER_PRINT_EVERY = 2 # Which Iterations to print (every nth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebooks/utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors, KDTree\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.) define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scores(y_test, y_pred):\n",
    "    y_test_tmp = y_test.copy()\n",
    "    accuracy = metrics.accuracy_score(y_test_tmp, y_pred)\n",
    "    confusion_matrix = (metrics.confusion_matrix(y_test_tmp, y_pred)).tolist()\n",
    "    dmc_score = np.sum(confusion_matrix * np.array([[0, -25], [-5, 5]]))\n",
    "    return accuracy, dmc_score, confusion_matrix\n",
    "\n",
    "def get_classifier(name):\n",
    "    return {\n",
    "            'xgb': XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "                colsample_bytree=1, disable_default_eval_metric=1,eval_metric='aucpr',\n",
    "                gamma=1.8285912697052542, reg_lambda=0.4149772770711012,\n",
    "                max_bin=254, max_delta_step=7.2556696256684035,\n",
    "                max_depth=3, min_child_weight=1.0317712458399741, missing=None,\n",
    "                n_estimators=445, n_jobs=-1, objective='binary:logistic', random_state=0, reg_alpha=0,\n",
    "                scale_pos_weight=1,silent=True,\n",
    "                subsample=1, tree_method='gpu_hist', verbosity=2, seed=42),\n",
    "            'svc': SVC(C=56.98164719395536, cache_size=8000, class_weight=None, coef0=0.0,  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
    "                kernel='linear', max_iter=-1, probability=True, random_state=None,\n",
    "                shrinking=True, tol=0.002848504943774676, verbose=0)}[name]\n",
    "\n",
    "def get_best_classifier_for_sample(idx, validation_set):\n",
    "    ground_truth = validation_set.iloc[idx].fraud\n",
    "    \n",
    "    lsvc_predict = validation_set.iloc[idx].lsvc_predict\n",
    "    xgb_predict = validation_set.iloc[idx].xgb_predict\n",
    "    lsvc_proba = validation_set.iloc[idx].lsvc_proba\n",
    "    xgb_prob = validation_set.iloc[idx].xgb_proba\n",
    "    \n",
    "    \n",
    "    # Both classifier predicted the calue correctly\n",
    "    if (lsvc_predict == ground_truth) and (xgb_predict == ground_truth):\n",
    "        if (lsvc_proba + 0.025) > xgb_prob:\n",
    "            return \"lsvc\"\n",
    "        else:\n",
    "            return \"xgboost\"\n",
    "    # lsvc predicted correctly\n",
    "    elif (validation_set.iloc[idx].lsvc_predict == ground_truth) and (xgb_predict != ground_truth):\n",
    "        return \"lsvc\"\n",
    "    \n",
    "    # xgboost predicted correcltly\n",
    "    elif (lsvc_predict != ground_truth) and (xgb_predict == ground_truth):\n",
    "        return \"xgboost\"\n",
    "    \n",
    "    # If No classifier predicted the knn correct, None is returned\n",
    "    else: \n",
    "        return ground_truth.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Search Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNLookup():\n",
    "\n",
    "    def __init__(self, knn_data):\n",
    "\n",
    "        #self.knn = NearestNeighbors(n_neighbors=1)\n",
    "        #self.knn.fit(knn_data.values.tolist())\n",
    "\n",
    "        self.tree = KDTree(knn_data)\n",
    "\n",
    "    def refit(self, knn_data):\n",
    "        self.tree = KDTree(knn_data)\n",
    "\n",
    "    def find_nearest_neighbor(self, row_scaled, dataset_scaled):\n",
    "        diffs = [np.sum((row_scaled - ds_row) ** 2) for idx, ds_row in dataset_scaled.iterrows()]\n",
    "        idx = np.argmin(diffs)\n",
    "        return idx, diffs[idx]\n",
    "\n",
    "    def find_nearest_neighbor2(self, row_scaled, dataset_scaled):\n",
    "        dist, ind = self.tree.query([row_scaled.values], k=1)\n",
    "        return np.ravel(ind)[0], np.ravel(dist)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.) Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainandknn_Xy_original_df = pd.read_csv(\"../data/train.csv\", sep=\"|\") if FINAL_SUBMISSION else pd.read_csv(\"../data/train_new.csv\", sep=\"|\")\n",
    "train_Xy_original_df, knn_Xy_original_df = train_test_split(trainandknn_Xy_original_df,train_size=0.75, random_state=42) # if FINAL_SUBMISSION else 0.8) #small\n",
    "test_X_original_df  = pd.read_csv(\"../data/test.csv\", sep=\"|\").iloc[0:4000] #TODO: For faster testing we use less data from the test set\n",
    "test_final_X_df = pd.read_csv(\"../data/test.csv\", sep=\"|\")\n",
    "\n",
    "train_Xy_wo_knn_df = pd.read_csv(\"../data/train_new.csv\", sep=\"|\")\n",
    "\n",
    "#Only for test routines\n",
    "val_Xy_original_df = pd.read_csv(\"../data/val_new.csv\", sep=\"|\")\n",
    "train_complete_Xy_original_df = pd.read_csv(\"../data/train.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.) Prepare Input X and Label Y Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convention for variables names: datasetname_columntype_transformstatus_dataframeornot\n",
    "train_y_original_df = train_Xy_original_df[[\"fraud\"]].copy()\n",
    "train_X_original_df = train_Xy_original_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "knn_y_original_df = knn_Xy_original_df[[\"fraud\"]].copy()\n",
    "knn_X_original_df = knn_Xy_original_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "# Only for test routie#nes\n",
    "val_y_originial_df = val_Xy_original_df[[\"fraud\"]].copy()\n",
    "val_X_originial_df = val_Xy_original_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "train_y_wo_knn_df = train_Xy_wo_knn_df[[\"fraud\"]].copy()\n",
    "train_X_wo_knn_df = train_Xy_wo_knn_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "train_complete_y_originial_df = train_complete_Xy_original_df[[\"fraud\"]].copy()\n",
    "train_complete_X_originial_df = train_complete_Xy_original_df.copy().drop(\"fraud\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.) DataTransformer Class and data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer:\n",
    "    \"\"\"\n",
    "    for scaling, data transformations (new features, one-hot encoding, categorical, ...)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scaler):\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def fit_scaler(self, df):\n",
    "        df_tmp = df.copy()\n",
    "        self.scaler.fit(df_tmp.astype(np.float64))\n",
    "        return self\n",
    "        \n",
    "    def apply_scaler(self, df):\n",
    "        df_temp = df.copy()\n",
    "        return pd.DataFrame(self.scaler.transform(df_temp),df_temp.index, df_temp.columns)\n",
    "    \n",
    "    def inverse_scale(self, df):\n",
    "        df_tmp = df.copy()\n",
    "        return pd.DataFrame(self.scaler.inverse_transform(df_tmp), df_tmp.index, df_tmp.columns)\n",
    "    \n",
    "    def add_features(self,df):\n",
    "        #TODO: Choose relevant features\n",
    "        df_tmp = df.copy()\n",
    "        df_tmp['totalScannedLineItems'] = df_tmp['scannedLineItemsPerSecond'] * df_tmp['totalScanTimeInSeconds']\n",
    "        #df['avgTimePerScan'] = 1/ df['scannedLineItemsPerSecond']\n",
    "        #df['avgValuePerScan'] = df['avgTimePerScan'] * df['valuePerSecond']\n",
    "        #df['withoutRegisPerPosition'] = df['scansWithoutRegistration'] / df['totalScannedLineItems'] #equivalent to lineItemVoidsPerPosition?\n",
    "        #df['quantiModPerPosition'] = df['quantityModifications'] / df['totalScannedLineItems']\n",
    "        #df['lineItemVoidsPerTotal'] = df['lineItemVoids'] / df['grandTotal']\n",
    "        #df['withoutRegisPerTotal'] = df['scansWithoutRegistration'] / df['grandTotal']\n",
    "        #df['quantiModPerTotal'] = df['quantityModifications'] / df['grandTotal']\n",
    "        #df['lineItemVoidsPerTime'] = df['lineItemVoids'] / df['totalScanTimeInSeconds']\n",
    "        #df['withoutRegisPerTime'] = df['scansWithoutRegistration'] / df['totalScanTimeInSeconds']\n",
    "        #df['quantiModPerTime'] = df['quantityModifications'] / df['totalScanTimeInSeconds']\n",
    "        #df['valuePerScannedLineItem'] = df['valuePerSecond'] / df['scannedLineItemsPerSecond']\n",
    "        return df_tmp\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        All in one: Apply all transform methods\n",
    "            1.) addFeatures\n",
    "            2.) apply_scaler\n",
    "        \"\"\"\n",
    "        df_tmp = df.copy()\n",
    "        return self.apply_scaler(self.add_features(df_tmp))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>totalScannedLineItems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.317174</td>\n",
       "      <td>-0.857599</td>\n",
       "      <td>1.329772</td>\n",
       "      <td>-0.418354</td>\n",
       "      <td>0.959638</td>\n",
       "      <td>0.869131</td>\n",
       "      <td>-0.085073</td>\n",
       "      <td>-0.021819</td>\n",
       "      <td>-0.110333</td>\n",
       "      <td>-0.98995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.276669</td>\n",
       "      <td>0.157498</td>\n",
       "      <td>0.306417</td>\n",
       "      <td>0.451036</td>\n",
       "      <td>0.324238</td>\n",
       "      <td>-0.884543</td>\n",
       "      <td>-0.068487</td>\n",
       "      <td>-0.094210</td>\n",
       "      <td>-0.352573</td>\n",
       "      <td>1.33811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  \\\n",
       "0    0.317174               -0.857599    1.329772      -0.418354   \n",
       "1   -0.276669                0.157498    0.306417       0.451036   \n",
       "\n",
       "   scansWithoutRegistration  quantityModifications  scannedLineItemsPerSecond  \\\n",
       "0                  0.959638               0.869131                  -0.085073   \n",
       "1                  0.324238              -0.884543                  -0.068487   \n",
       "\n",
       "   valuePerSecond  lineItemVoidsPerPosition  totalScannedLineItems  \n",
       "0       -0.021819                 -0.110333               -0.98995  \n",
       "1       -0.094210                 -0.352573                1.33811  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "transformer = DataTransformer(scaler)\n",
    "\n",
    "# Adding new Features to train and test set\n",
    "train_X_unscaled_df = transformer.add_features(train_X_original_df)\n",
    "test_X_unscaled_df = transformer.add_features(test_X_original_df)\n",
    "knn_X_unscaled_df = transformer.add_features(knn_X_original_df)\n",
    "\n",
    "val_X_unscaled_df = transformer.add_features(val_X_originial_df)\n",
    "train_complete_X_unscaled_df = transformer.add_features(train_complete_X_originial_df) \n",
    "\n",
    "transformer.fit_scaler(transformer.add_features(train_complete_X_unscaled_df.append(test_X_unscaled_df, sort=False)))\n",
    "train_X_scaled_df = transformer.apply_scaler(train_X_unscaled_df)\n",
    "knn_X_scaled_df   = transformer.apply_scaler(knn_X_unscaled_df)\n",
    "\n",
    "\n",
    "test_X_scaled_df  = transformer.apply_scaler(test_X_unscaled_df)\n",
    "val_X_scaled_df = transformer.apply_scaler(val_X_unscaled_df)\n",
    "train_complete_X_scaled_df = transformer.apply_scaler(train_complete_X_unscaled_df)\n",
    "\n",
    "train_X_wo_knn_unscaled_df = transformer.add_features(train_X_wo_knn_df.copy())\n",
    "train_X_wo_knn_scaled_df = transformer.apply_scaler(train_X_wo_knn_unscaled_df)\n",
    "\n",
    "# labels\n",
    "train_y_df = train_y_original_df.copy()\n",
    "val_y_df = val_y_originial_df.copy()\n",
    "knn_y_df = knn_y_original_df.copy()\n",
    "train_complete_y_df = train_complete_y_originial_df.copy()\n",
    "\n",
    "test_final_X_df = transformer.add_features(test_final_X_df)\n",
    "\n",
    "\n",
    "test_X_scaled_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name     | New Features | Scaled |\n",
    "|----------|--------------|--------|\n",
    "| orig     |      [ ]     |   [ ]  |\n",
    "| unscaled |      [X]     |   [ ]  |\n",
    "| scaled   |      [X]     |   [X]  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 1/2.) train normally with all available classifiers for classifying knn split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>totalScannedLineItems</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>923</td>\n",
       "      <td>93.36</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.018418</td>\n",
       "      <td>0.101148</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1473</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005431</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>575</td>\n",
       "      <td>8.55</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050435</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1519</td>\n",
       "      <td>90.38</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>84.08</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>2.899310</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  \\\n",
       "0           4                     923       93.36              2   \n",
       "1           6                    1473        0.01             10   \n",
       "2           6                     575        8.55              5   \n",
       "3           4                    1519       90.38              8   \n",
       "4           4                      29       84.08              1   \n",
       "\n",
       "   scansWithoutRegistration  quantityModifications  scannedLineItemsPerSecond  \\\n",
       "0                         2                      3                   0.018418   \n",
       "1                         8                      0                   0.005431   \n",
       "2                         1                      0                   0.050435   \n",
       "3                         5                      2                   0.007900   \n",
       "4                         0                      4                   0.724138   \n",
       "\n",
       "   valuePerSecond  lineItemVoidsPerPosition  totalScannedLineItems  fraud  \n",
       "0        0.101148                  0.117647                   17.0      0  \n",
       "1        0.000007                  1.250000                    8.0      0  \n",
       "2        0.014870                  0.172414                   29.0      0  \n",
       "3        0.059500                  0.666667                   12.0      0  \n",
       "4        2.899310                  0.047619                   21.0      0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnwithprobs_Xy_df = knn_X_unscaled_df.copy()\n",
    "knnwithprobs_Xy_df['fraud'] = knn_y_df\n",
    "knnwithprobs_Xy_df = knnwithprobs_Xy_df.reset_index(drop=True)\n",
    "knnwithprobs_Xy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_routine(data_dict, data_transformer, knn_lookup):\n",
    "    pltrain_X_unscaled_df = data_dict['pltrain_X_unscaled_df']\n",
    "    pltrain_X_scaled_df = transformer.apply_scaler(pltrain_X_unscaled_df)\n",
    "    pltrain_y_df = data_dict['pltrain_y_df']\n",
    "\n",
    "    lsvc_pltrain = get_classifier(\"svc\")\n",
    "    xgb_pltrain = get_classifier(\"xgb\")\n",
    "\n",
    "    lsvc_pltrain.fit(pltrain_X_scaled_df.values, pltrain_y_df.values)\n",
    "    xgb_pltrain.fit(pltrain_X_unscaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "    # Predict on ValidationSet\n",
    "    lsvc_pred_val = lsvc_pltrain.predict(data_dict['val_X_scaled_df'].values)\n",
    "    xgb_pred_val = xgb_pltrain.predict(data_dict['val_X_unscaled_df'].values)\n",
    "    lsvc_prob_val = lsvc_pltrain.predict_proba(data_dict['val_X_scaled_df'].values)\n",
    "    xgb_prob_val = xgb_pltrain.predict_proba(data_dict['val_X_unscaled_df'].values)\n",
    "\n",
    "    own_classifier_pred_val = classify(xgb_pltrain, lsvc_pltrain,  data_dict['val_X_unscaled_df'],\n",
    "                                       data_dict['knnwithprob_Xy_unscaled_df'], data_transformer, knn_lookup)\n",
    "    \n",
    "    val_pred_Xy = data_dict['val_X_unscaled_df'].copy()\n",
    "    val_pred_Xy['fraud'] = data_dict['val_y_df'].copy()\n",
    "    val_pred_Xy['xgb_predict'] = xgb_pred_val\n",
    "    val_pred_Xy['lsvc_predict'] = lsvc_pred_val\n",
    "    val_pred_Xy['xgb_proba'] = [round(max(x), 3) for x in xgb_prob_val]\n",
    "    val_pred_Xy['lsvc_proba'] = [round(max(x), 3) for x in lsvc_prob_val]\n",
    "    val_pred_Xy['own_predict'] = own_classifier_pred_val\n",
    "\n",
    "    \n",
    "\n",
    "    lsvc_val_acc, lsvc_val_dmc, lsvc_val_conf_mat = calc_scores(data_dict['val_y_df'].values, lsvc_pred_val)\n",
    "    xgb_val_acc, xgb_val_dmc, xgb_val_conf_mat = calc_scores(data_dict['val_y_df'].values, xgb_pred_val)\n",
    "    own_classifier_val_acc, own_classifier_val_dmc, own_classifier_val_conf_mat = calc_scores(\n",
    "        data_dict['val_y_df'], own_classifier_pred_val)\n",
    "\n",
    "    lsvc_pltrain = get_classifier(\"svc\")\n",
    "    xgb_pltrain = get_classifier(\"xgb\")\n",
    "\n",
    "    lsvc_pltrain.fit(data_dict['test_X_scaled_df'].values, data_dict['test_y_df'].values)\n",
    "    xgb_pltrain.fit(data_dict['test_X_unscaled_df'].values, data_dict['test_y_df'].values)\n",
    "\n",
    "    # Predict on original full size (~1900 samples) just trained on test_data\n",
    "    lsvc_pred_train = lsvc_pltrain.predict(data_dict['train_complete_X_scaled_df'].values)\n",
    "    xgb_pred_train = xgb_pltrain.predict(data_dict['train_complete_X_unscaled_df'].values)\n",
    "    lsvc_prob_train = lsvc_pltrain.predict_proba(data_dict['train_complete_X_scaled_df'].values)\n",
    "    xgb_prob_train = xgb_pltrain.predict_proba(data_dict['train_complete_X_unscaled_df'].values)\n",
    "    \n",
    "    own_classifier_pred_train = classify(xgb_pltrain,lsvc_pltrain, data_dict['train_complete_X_unscaled_df'],\n",
    "                                         data_dict['knnwithprob_Xy_unscaled_df'], data_transformer, knn_lookup)\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_pred_complete_Xy = data_dict['train_complete_X_unscaled_df'].copy()\n",
    "    train_pred_complete_Xy['fraud'] = data_dict['train_complete_y_df'].copy()\n",
    "    train_pred_complete_Xy['xgb_predict'] = xgb_pred_train\n",
    "    train_pred_complete_Xy['lsvc_predict'] = lsvc_pred_train\n",
    "    train_pred_complete_Xy['xgb_prob'] = [round(max(x), 3) for x in xgb_prob_train]\n",
    "    train_pred_complete_Xy['lsvc_prob'] = [round(max(x), 3) for x in lsvc_prob_train]\n",
    "    train_pred_complete_Xy['own_predict'] = own_classifier_pred_train\n",
    "    \n",
    "    \n",
    "    \n",
    "    lsvc_train_acc, lsvc_train_dmc, lsvc_train_conf_mat = calc_scores(data_dict['train_complete_y_df'], lsvc_pred_train)\n",
    "    xgb_train_acc, xgb_train_dmc, xgb_train_conf_mat = calc_scores(data_dict['train_complete_y_df'], xgb_pred_train)\n",
    "    own_classifier_train_acc, own_classifier_train_dmc, own_classifier_train_conf_mat = calc_scores(\n",
    "        data_dict['train_complete_y_df'], own_classifier_pred_train)\n",
    "\n",
    "    results = {\"lin_svc\": {\n",
    "        \"val\": {\n",
    "            \"dmc_score\": lsvc_val_dmc,\n",
    "            \"conf_matrix\": lsvc_val_conf_mat\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"dmc_score\": lsvc_train_dmc,\n",
    "            \"conf_matrix\": lsvc_train_conf_mat\n",
    "        }\n",
    "    },\n",
    "        \"xgboost\": {\n",
    "            \"val\": {\n",
    "                \"dmc_score\": xgb_val_dmc,\n",
    "                \"conf_matrix\": xgb_val_conf_mat\n",
    "            },\n",
    "            \"train\": {\n",
    "                \"dmc_score\": xgb_train_dmc,\n",
    "                \"conf_matrix\": xgb_train_conf_mat\n",
    "            }\n",
    "        },\n",
    "        \"own_classifier\": {\n",
    "            \"val\": {\n",
    "                \"dmc_score\": own_classifier_val_dmc,\n",
    "                \"conf_matrix\": own_classifier_val_conf_mat\n",
    "            },\n",
    "            \"train\": {\n",
    "                \"dmc_score\": own_classifier_train_dmc,\n",
    "                \"conf_matrix\": own_classifier_train_conf_mat\n",
    "            }\n",
    "        },\n",
    "\n",
    "    }\n",
    "    return results, train_pred_complete_Xy.copy(), val_pred_Xy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_routine_without_knn(data_dict, data_transformer):\n",
    "    pltrain_X_unscaled_df = data_dict['pltrain_wo_knn_X_unscaled_df']\n",
    "    pltrain_X_scaled_df = transformer.apply_scaler(pltrain_X_unscaled_df)\n",
    "    pltrain_y_df = data_dict['pltrain_wo_knn_y_df']\n",
    "    \n",
    "    \n",
    "    pred_val_time = time.time()\n",
    "    lsvc_pltrain = get_classifier(\"svc\")\n",
    "    xgb_pltrain = get_classifier(\"xgb\")\n",
    "\n",
    "    lsvc_pltrain.fit(pltrain_X_scaled_df.values, pltrain_y_df.values)\n",
    "    xgb_pltrain.fit(pltrain_X_unscaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "    # Predict on ValidationSet\n",
    "    lsvc_pred_val = lsvc_pltrain.predict(data_dict['val_X_scaled_df'].values)\n",
    "    xgb_pred_val = xgb_pltrain.predict(data_dict['val_X_unscaled_df'].values)\n",
    "    lsvc_prob_val = lsvc_pltrain.predict_proba(data_dict['val_X_scaled_df'].values)\n",
    "    xgb_prob_val = xgb_pltrain.predict_proba(data_dict['val_X_unscaled_df'].values)\n",
    "\n",
    "    own_classifier_pred_val = classify_without_knn(xgb_pltrain, lsvc_pltrain, data_dict['val_X_unscaled_df'], data_transformer)\n",
    "    \n",
    "    pred_val_end = time.time()\n",
    "    print(\"Prediction on val \", pred_val_end- pred_val_time)\n",
    "    \n",
    "    start = time.time()\n",
    "    val_pred_Xy = data_dict['val_X_unscaled_df'].copy()\n",
    "    val_pred_Xy['fraud'] = data_dict['val_y_df'].copy()\n",
    "    val_pred_Xy['xgb_predict'] = xgb_pred_val\n",
    "    val_pred_Xy['lsvc_predict'] = lsvc_pred_val\n",
    "    val_pred_Xy['xgb_proba'] = [round(max(x), 3) for x in xgb_prob_val]\n",
    "    val_pred_Xy['lsvc_proba'] = [round(max(x), 3) for x in lsvc_prob_val]\n",
    "    val_pred_Xy['own_predict'] = own_classifier_pred_val\n",
    "\n",
    "    \n",
    "\n",
    "    lsvc_val_acc, lsvc_val_dmc, lsvc_val_conf_mat = calc_scores(data_dict['val_y_df'].values, lsvc_pred_val)\n",
    "    xgb_val_acc, xgb_val_dmc, xgb_val_conf_mat = calc_scores(data_dict['val_y_df'].values, xgb_pred_val)\n",
    "    own_classifier_val_acc, own_classifier_val_dmc, own_classifier_val_conf_mat = calc_scores(\n",
    "        data_dict['val_y_df'], own_classifier_pred_val)\n",
    "    print(\"Calculating score val: \", time.time()- start)\n",
    "    \n",
    "    start_train = time.time()\n",
    "    \n",
    "    lsvc_pltrain = get_classifier(\"svc\")\n",
    "    xgb_pltrain = get_classifier(\"xgb\")\n",
    "\n",
    "    lsvc_pltrain.fit(data_dict['test_wo_knn_X_scaled_df'].values, data_dict['test_wo_knn_y_df'].values)\n",
    "    xgb_pltrain.fit(data_dict['test_wo_knn_X_unscaled_df'].values, data_dict['test_wo_knn_y_df'].values)\n",
    "\n",
    "    # Predict on original full size (~1900 samples) just trained on test_data\n",
    "    lsvc_pred_train = lsvc_pltrain.predict(data_dict['train_complete_X_scaled_df'].values)\n",
    "    xgb_pred_train = xgb_pltrain.predict(data_dict['train_complete_X_unscaled_df'].values)\n",
    "    lsvc_prob_train = lsvc_pltrain.predict_proba(data_dict['train_complete_X_scaled_df'].values)\n",
    "    xgb_prob_train = xgb_pltrain.predict_proba(data_dict['train_complete_X_unscaled_df'].values)\n",
    "       \n",
    "    own_classifier_pred_train = classify_without_knn(xgb_pltrain, lsvc_pltrain, data_dict['train_complete_X_unscaled_df'], data_transformer)\n",
    "    \n",
    "    print(\"Prediction on train \", time.time()- start_train)\n",
    "\n",
    "    start_calc_time = time.time()\n",
    "    train_pred_complete_Xy = data_dict['train_complete_X_unscaled_df'].copy()\n",
    "    train_pred_complete_Xy['fraud'] = data_dict['train_complete_y_df'].copy()\n",
    "    train_pred_complete_Xy['xgb_predict'] = xgb_pred_train\n",
    "    train_pred_complete_Xy['lsvc_predict'] = lsvc_pred_train\n",
    "    train_pred_complete_Xy['xgb_prob'] = [round(max(x), 3) for x in xgb_prob_train]\n",
    "    train_pred_complete_Xy['lsvc_prob'] = [round(max(x), 3) for x in lsvc_prob_train]\n",
    "    train_pred_complete_Xy['own_predict'] = own_classifier_pred_train\n",
    "    \n",
    "    \n",
    "    \n",
    "    lsvc_train_acc, lsvc_train_dmc, lsvc_train_conf_mat = calc_scores(data_dict['train_complete_y_df'], lsvc_pred_train)\n",
    "    xgb_train_acc, xgb_train_dmc, xgb_train_conf_mat = calc_scores(data_dict['train_complete_y_df'], xgb_pred_train)\n",
    "    own_classifier_train_acc, own_classifier_train_dmc, own_classifier_train_conf_mat = calc_scores(\n",
    "        data_dict['train_complete_y_df'], own_classifier_pred_train)\n",
    "    \n",
    "    print(\"Calculating score train: \", time.time()- start_calc_time)\n",
    "\n",
    "    results = {\"lin_svc\": {\n",
    "        \"val\": {\n",
    "            \"dmc_score\": lsvc_val_dmc,\n",
    "            \"conf_matrix\": lsvc_val_conf_mat\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"dmc_score\": lsvc_train_dmc,\n",
    "            \"conf_matrix\": lsvc_train_conf_mat\n",
    "        }\n",
    "    },\n",
    "        \"xgboost\": {\n",
    "            \"val\": {\n",
    "                \"dmc_score\": xgb_val_dmc,\n",
    "                \"conf_matrix\": xgb_val_conf_mat\n",
    "            },\n",
    "            \"train\": {\n",
    "                \"dmc_score\": xgb_train_dmc,\n",
    "                \"conf_matrix\": xgb_train_conf_mat\n",
    "            }\n",
    "        },\n",
    "        \"own_classifier\": {\n",
    "            \"val\": {\n",
    "                \"dmc_score\": own_classifier_val_dmc,\n",
    "                \"conf_matrix\": own_classifier_val_conf_mat\n",
    "            },\n",
    "            \"train\": {\n",
    "                \"dmc_score\": own_classifier_train_dmc,\n",
    "                \"conf_matrix\": own_classifier_train_conf_mat\n",
    "            }\n",
    "        },\n",
    "\n",
    "    }\n",
    "    return results, train_pred_complete_Xy.copy(), val_pred_Xy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(xgboost_fitted, linear_svc_fitted, data_to_predict, data_knn_with_probs, transformer, knn_lookup):\n",
    "    prediction = []\n",
    "    data_knn_X_scaled = transformer.apply_scaler(\n",
    "        data_knn_with_probs.copy().drop(columns=[\"fraud\", \"xgb_predict\", \"xgb_proba\", \"lsvc_predict\", \"lsvc_proba\"]))\n",
    "    data_to_predict_scaled = transformer.apply_scaler(data_to_predict.copy())\n",
    "    data_to_predict_unscaled = data_to_predict.copy()\n",
    "\n",
    "    knn_time = []\n",
    "    pred_time = []\n",
    "    tolerance = 0.5\n",
    "    \n",
    "    #Check which scaler was used for preprocessing\n",
    "    if str(type(transformer.scaler)) == \"<class 'sklearn.preprocessing.data.StandardScaler'>\":\n",
    "        tolerance = 1.4\n",
    "    \n",
    "    #print(len(data_to_predict_unscaled))\n",
    "    for i, row in data_to_predict_unscaled.iterrows():\n",
    "        #print(i)\n",
    "        if row.trustLevel >= 3:\n",
    "            prediction.append(0)\n",
    "\n",
    "        else:\n",
    "\n",
    "            start_knn = time.time()\n",
    "            idx_knn, distance_knn = knn_lookup.find_nearest_neighbor2(data_to_predict_scaled.iloc[i], data_knn_X_scaled)\n",
    "            knn_time.append(time.time() - start_knn)\n",
    "            # If distance to knn is to big, classify them directly\n",
    "            if distance_knn > tolerance:\n",
    "\n",
    "                pred_start = time.time()\n",
    "                xgb_pred = xgboost_fitted.predict([data_to_predict_unscaled.iloc[i].values])[0]\n",
    "                xgb_prob = max(np.ravel(xgboost_fitted.predict_proba([data_to_predict_unscaled.iloc[i].values])))\n",
    "\n",
    "                lsvc_pred = linear_svc_fitted.predict([data_to_predict_scaled.iloc[i]])[0]\n",
    "                lsvc_prob = max(np.ravel(linear_svc_fitted.predict_proba([data_to_predict_scaled.iloc[i].values])))\n",
    "\n",
    "                pred_time.append(time.time() - pred_start)\n",
    "                # If both classified them equal, take one of both\n",
    "                if xgb_pred == lsvc_pred:\n",
    "                    prediction.append(xgb_pred)\n",
    "\n",
    "                # if classification is not equal, take the one with higher probability\n",
    "                elif xgb_prob > lsvc_prob:\n",
    "                    prediction.append(xgb_pred)\n",
    "                else:\n",
    "                    prediction.append(lsvc_pred)\n",
    "\n",
    "            # If distance is smaller than 0.15, use knn    \n",
    "            else:\n",
    "                best_classifier = get_best_classifier_for_sample(idx_knn, data_knn_with_probs)\n",
    "                if isinstance(best_classifier, int):\n",
    "                    prediction.append(best_classifier)\n",
    "                    \n",
    "                elif best_classifier == \"xgboost\":\n",
    "                    pred_start = time.time()\n",
    "                    prediction.append(xgboost_fitted.predict([data_to_predict_unscaled.iloc[i].values])[0])\n",
    "                    pred_end = time.time()\n",
    "\n",
    "\n",
    "                elif best_classifier == \"lsvc\":\n",
    "                    pred_start = time.time()\n",
    "                    prediction.append(linear_svc_fitted.predict([data_to_predict_scaled.iloc[i].values])[0])\n",
    "                    pred_end = time.time()\n",
    "\n",
    "                pred_time.append(pred_end - pred_start)\n",
    "    return pd.DataFrame({\"fraud\": prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_without_knn(xgboost_fitted, linear_svc_fitted, data_to_predict, transformer):\n",
    "    prediction = []\n",
    "    data_to_predict_scaled = transformer.apply_scaler(data_to_predict.copy())\n",
    "    data_to_predict_unscaled = data_to_predict.copy()\n",
    "    pred_time, comp_time =[], []\n",
    "    threshhold = 25/35\n",
    "    \n",
    "    #print(len(data_to_predict_unscaled))\n",
    "    for i, row in data_to_predict_unscaled.iterrows():\n",
    "        #print(i)\n",
    "        \n",
    "        if row.trustLevel >= 3:\n",
    "            prediction.append(0)\n",
    "\n",
    "        else:\n",
    "            start_pred = time.time()\n",
    "            xgb_pred = xgboost_fitted.predict([data_to_predict_unscaled.iloc[i].values])[0]\n",
    "            xgb_prob = max(np.ravel(xgboost_fitted.predict_proba([data_to_predict_unscaled.iloc[i].values])))\n",
    "\n",
    "            lsvc_pred = linear_svc_fitted.predict([data_to_predict_scaled.iloc[i]])[0]\n",
    "            lsvc_prob = max(np.ravel(linear_svc_fitted.predict_proba([data_to_predict_scaled.iloc[i].values])))\n",
    "            \n",
    "            end_pred = time.time()\n",
    "            \n",
    "            prob_mean = np.mean(xgb_prob + lsvc_prob)\n",
    "            \n",
    "            if (lsvc_pred == 1) and (lsvc_prob > threshhold):\n",
    "                prediction.append(lsvc_pred)\n",
    "                continue\n",
    "            \n",
    "            elif (xgb_pred == 1) and (prob_mean > threshhold):\n",
    "                prediction.append(xgb_pred)\n",
    "                continue\n",
    "            \n",
    "            elif xgb_prob < (lsvc_prob + 0.025):\n",
    "                prediction.append(lsvc_pred)\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                prediction.append(xgb_pred)\n",
    "            \n",
    "            end_comparision = time.time()\n",
    "            \n",
    "            pred_time.append(end_pred- start_pred)\n",
    "            comp_time.append(end_comparision-end_pred)\n",
    "            \n",
    "    print(\"Prediction Time. {} --- Comparison Time: {}\".format(np.mean(pred_time), np.mean(comp_time)))\n",
    "    return pd.DataFrame({\"fraud\": prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_supervised_learning_procedure(test_X_unscaled,train_X_unscaled, train_y, test_data_dict, transformer):\n",
    "    test_data_dict = test_data_dict\n",
    "    pred_dfs = []\n",
    "    pred_val_dfs = []\n",
    "    \n",
    "    pred_train_wo_knn_dfs = []\n",
    "    pred_val_wo_knn_dfs = []\n",
    "    results_wo_knn = []\n",
    "    \n",
    "    # initialize pseudo labeled train (pltrain) dataset with the transformed training data\n",
    "    pltrain_X_unscaled_df = train_X_unscaled.copy()\n",
    "    train_X_scaled_df = transformer.apply_scaler(train_X_unscaled)\n",
    "\n",
    "    pltrain_wo_knn_X_unscaled_df = test_data_dict['train_X_wo_knn_unscaled_df'].copy()\n",
    "    pltrain_wo_knn_X_scaled_df = transformer.apply_scaler(pltrain_wo_knn_X_unscaled_df)\n",
    "    pltrain_wo_knn_y_df = test_data_dict['train_y_wo_knn_df']\n",
    "    \n",
    "    \n",
    "    pltrain_y_df = train_y.copy()\n",
    "    train_X_scaled_len = len(train_X_unscaled)\n",
    "    train_wo_knn_X_scaled_len = len(pltrain_wo_knn_X_unscaled_df)\n",
    "    print(\"{} available train data before pseudo labeling\".format(train_X_scaled_len))\n",
    "\n",
    "    train_X_scaled_df = transformer.apply_scaler(train_X_unscaled)\n",
    "\n",
    "    linear_svc_initial = get_classifier('svc')\n",
    "    linear_svc_initial.fit(train_X_scaled_df.values, pltrain_y_df.values)\n",
    "    xgboost_initial = get_classifier('xgb')\n",
    "    xgboost_initial.fit(train_X_unscaled.values, pltrain_y_df.values)\n",
    "    \n",
    "    knnwithprob_Xy_unscaled_df = test_data_dict['knnwithprob_Xy_unscaled_df']\n",
    "    knnwithprobs_X_unscaled_df = knnwithprob_Xy_unscaled_df.copy().drop(columns=['fraud'])\n",
    "    knnwithprobs_X_scaled_df = transformer.apply_scaler(knnwithprobs_X_unscaled_df.copy())\n",
    "    knn_lookup = KNNLookup(knnwithprobs_X_unscaled_df)\n",
    "    \n",
    "    knnwithprob_Xy_unscaled_df['xgb_predict'] = xgboost_initial.predict(knnwithprobs_X_unscaled_df.values)\n",
    "    knnwithprob_Xy_unscaled_df['xgb_proba'] = [round(max(x), 3) for x in\n",
    "                                       xgboost_initial.predict_proba(knnwithprobs_X_unscaled_df.values)]\n",
    "    knnwithprob_Xy_unscaled_df['lsvc_predict'] = linear_svc_initial.predict(knnwithprobs_X_scaled_df.values)\n",
    "    knnwithprob_Xy_unscaled_df['lsvc_proba'] = [round(max(x), 3) for x in\n",
    "                                        linear_svc_initial.predict_proba(knnwithprobs_X_scaled_df.values)]\n",
    "    knn_dataframes = [knnwithprob_Xy_unscaled_df.copy()]\n",
    "    results = []\n",
    "    for i in range(TEST_BATCH_SIZE, len(test_X_unscaled), TEST_BATCH_SIZE):\n",
    "        if int(i / TEST_BATCH_SIZE) % ITER_PRINT_EVERY == 0:\n",
    "            print(\"iteration\", int(i / TEST_BATCH_SIZE), \"\\t/\", int(np.ceil(len(test_X_unscaled) / TEST_BATCH_SIZE)),\n",
    "                  \"with batch from\", i - TEST_BATCH_SIZE, \"\\t to\", i, \", training with\", len(pltrain_y_df), \"samples\")\n",
    "        # get batch from test set\n",
    "        testbatch_X_df = test_X_unscaled.iloc[i - TEST_BATCH_SIZE:i].copy().reset_index(drop=True)\n",
    "                \n",
    "        \"\"\"\n",
    "        ----------------------------------------------------OLD PART ------------------------------------------------------------\n",
    "        \"\"\"     \n",
    "        \"\"\"\n",
    "        # extend pseudo labeled train (pltrain) dataset by predicting the batch\n",
    "        pltrain_X_unscaled_df, pltrain_y_df = get_extended_pltrain_for_batch(testbatch_X_df, pltrain_X_unscaled_df,\n",
    "                                                                             pltrain_y_df, knnwithprob_Xy_unscaled_df,\n",
    "                                                                             transformer, knn_lookup)\n",
    "        pltrain_X_scaled_df = transformer.apply_scaler(pltrain_X_unscaled_df)\n",
    "        test_data_dict['pltrain_X_unscaled_df'] = pltrain_X_unscaled_df.copy()\n",
    "        test_data_dict['pltrain_X_scaled_df'] = pltrain_X_scaled_df.copy()\n",
    "        test_data_dict['pltrain_y_df'] = pltrain_y_df.copy()\n",
    "        \n",
    "        \n",
    "        linear_svc = get_classifier('svc')\n",
    "        linear_svc.fit(pltrain_X_scaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "        xgboost = get_classifier('xgb')\n",
    "        xgboost.fit(pltrain_X_unscaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "        knnwithprob_Xy_unscaled_df_tmp = knnwithprob_Xy_unscaled_df.copy().drop(\n",
    "            columns=['fraud', 'xgb_predict', 'xgb_proba', 'lsvc_predict', 'lsvc_proba'])\n",
    "        \n",
    "        #own preds on knn for DEBUG\n",
    "        own_preds = classify(xgboost, linear_svc,knnwithprob_Xy_unscaled_df_tmp.copy(), knnwithprob_Xy_unscaled_df, transformer, knn_lookup)\n",
    "        \n",
    "        knnwithprob_Xy_unscaled_df['xgb_predict'] = xgboost.predict(knnwithprob_Xy_unscaled_df_tmp.values)\n",
    "        knnwithprob_Xy_unscaled_df['xgb_proba'] = [round(max(x), 3) for x in xgboost.predict_proba(knnwithprob_Xy_unscaled_df_tmp.values)]\n",
    "        knnwithprob_Xy_unscaled_df['lsvc_predict'] = linear_svc.predict(transformer.apply_scaler(knnwithprob_Xy_unscaled_df_tmp).values)\n",
    "        knnwithprob_Xy_unscaled_df['lsvc_proba'] = [round(max(x), 3) for x in\n",
    "                                            linear_svc.predict_proba(transformer.apply_scaler(knnwithprob_Xy_unscaled_df_tmp).values)]\n",
    "        \n",
    "        knnwithprob_Xy_unscaled_df_copy = knnwithprob_Xy_unscaled_df.copy()\n",
    "        knnwithprob_Xy_unscaled_df_copy['own_predict'] = own_preds\n",
    "        knn_dataframes.append(knnwithprob_Xy_unscaled_df_copy)\n",
    "        test_data_dict['knnwithprob_Xy_unscaled_df'] = knnwithprob_Xy_unscaled_df\n",
    "        \n",
    "        # Only Test from PLTrain without the data from train\n",
    "        test_data_dict['test_X_unscaled_df'] = pltrain_X_unscaled_df.iloc[train_X_scaled_len:]\n",
    "        test_data_dict['test_X_scaled_df'] = pltrain_X_scaled_df.iloc[train_X_scaled_len:]\n",
    "        test_data_dict['test_y_df'] = pltrain_y_df.iloc[train_X_scaled_len:]\n",
    "        \n",
    "        res, pred_df, pred_val_df = test_routine(test_data_dict, transformer, knn_lookup)\n",
    "        pred_dfs.append(pred_df)\n",
    "        pred_val_dfs.append(pred_val_df)\n",
    "        print(\"XGBoost: PLTrain auf Val: {} --- PLTest auf Train: {} || LinearSVC:  PLTrain auf Val: {} --- PLTest auf Train: {} || Own Classifier:  PLTrain auf Val: {} --- PLTest auf Train: {}\".format(\n",
    "            res['xgboost']['val']['dmc_score'],res['xgboost']['train']['dmc_score'],res['lin_svc']['val']['dmc_score'],res['lin_svc']['train']['dmc_score'],res['own_classifier']['val']['dmc_score'],res['own_classifier']['train']['dmc_score']))\n",
    "        \n",
    "        results.append(res)\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        ----------------------------------------------------NEW PART ------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        print(\"Next Batch starts\")\n",
    "        pltrain_wo_knn_X_unscaled_df, pltrain_wo_knn_y_df = get_extended_pltrain_for_batch_without_knn(testbatch_X_df, pltrain_wo_knn_X_unscaled_df,\n",
    "                                                                             pltrain_wo_knn_y_df, transformer)\n",
    "        pltrain_wo_knn_X_scaled_df = transformer.apply_scaler(pltrain_wo_knn_X_unscaled_df)\n",
    "        test_data_dict['pltrain_wo_knn_X_unscaled_df'] = pltrain_wo_knn_X_unscaled_df.copy()\n",
    "        test_data_dict['pltrain_wo_knn_X_scaled_df'] = pltrain_wo_knn_X_scaled_df.copy()\n",
    "        test_data_dict['pltrain_wo_knn_y_df'] = pltrain_wo_knn_y_df.copy()\n",
    "        \n",
    "        \n",
    "        linear_svc_wo_knn = get_classifier('svc')\n",
    "        linear_svc_wo_knn.fit(pltrain_wo_knn_X_unscaled_df.values, pltrain_wo_knn_y_df.values)\n",
    "\n",
    "        xgboost_wo_knn = get_classifier('xgb')\n",
    "        xgboost_wo_knn.fit(pltrain_wo_knn_X_unscaled_df.values, pltrain_wo_knn_y_df.values)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Only Test from PLTrain without the data from train\n",
    "        test_data_dict['test_wo_knn_X_unscaled_df'] = pltrain_wo_knn_X_unscaled_df.iloc[train_wo_knn_X_scaled_len:]\n",
    "        test_data_dict['test_wo_knn_X_scaled_df'] = pltrain_wo_knn_X_scaled_df.iloc[train_wo_knn_X_scaled_len:]\n",
    "        test_data_dict['test_wo_knn_y_df'] = pltrain_wo_knn_y_df.iloc[train_wo_knn_X_scaled_len:]\n",
    "        \n",
    "        res_wo_knn, pred_train_df_wo_knn, pred_val_df_wo_knn = test_routine_without_knn(test_data_dict, transformer)\n",
    "        \n",
    "        results_wo_knn.append(res_wo_knn)\n",
    "        pred_train_wo_knn_dfs.append(pred_train_df_wo_knn)\n",
    "        pred_val_wo_knn_dfs.append(pred_val_df_wo_knn)\n",
    "        \n",
    "        print(\"XGBoost: PLTrain auf Val: {} --- PLTest auf Train: {} || LinearSVC:  PLTrain auf Val: {} --- PLTest auf Train: {} || Own Classifier:  PLTrain auf Val: {} --- PLTest auf Train: {}\".format(\n",
    "            res_wo_knn['xgboost']['val']['dmc_score'],res_wo_knn['xgboost']['train']['dmc_score'],res_wo_knn['lin_svc']['val']['dmc_score'],res_wo_knn['lin_svc']['train']['dmc_score'],res_wo_knn['own_classifier']['val']['dmc_score'],res_wo_knn['own_classifier']['train']['dmc_score']))\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    # use last few rows that cant fill up a complete batch as a smaller batch\n",
    "    print(\"iteration\", int(i / TEST_BATCH_SIZE) + 1, \"\\twith batch from\", i, \"\\t to\", len(test_X_unscaled),\n",
    "          \", training with\", len(pltrain_X_unscaled_df), \"samples\")\n",
    "    testbatch_X_transformed_df = test_X_unscaled.iloc[i:len(test_X_unscaled)]\n",
    "    \"\"\"\n",
    "    xgb_final = get_classifier(\"xgb\")\n",
    "    lsvc_final = get_classifier(\"svc\")\n",
    "    \n",
    "    xgb_final.fit(pltrain_X_unscaled_df.values, pltrain_y_df.values)\n",
    "    lsvc_final.fit(pltrain_X_scaled_df.values, pltrain_y_df.values)\n",
    "    final_prediction = classify(xgb_final, lsvc_final, test_final_X_df.iloc[:10000], test_data_dict['knnwithprob_Xy_unscaled_df'], transformer, knn_lookup)\n",
    "    \n",
    "    xgb_final_wo_knn = get_classifier(\"xgb\")\n",
    "    lsvc_final_wo_knn = get_classifier(\"svc\")\n",
    "    \n",
    "    xgb_final_wo_knn.fit(pltrain_X_unscaled_df.values, pltrain_y_df.values)\n",
    "    lsvc_final_wo_knn.fit(pltrain_X_scaled_df.values, pltrain_y_df.values)\n",
    "    final_prediction_wo_knn = classify(xgb_final, lsvc_final, test_final_X_df.iloc[:10000], test_data_dict['knnwithprob_Xy_unscaled_df'], transformer, knn_lookup)\n",
    "    \n",
    "    \n",
    "    return results, final_prediction, knn_dataframes, pred_dfs, pred_val_dfs, results_wo_knn, final_prediction_wo_knn,  pred_train_wo_knn_dfs, pred_val_wo_knn_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.) iterative model training using pseudo-labeling\n",
    "predict batches of the test set, add them to the previous training set and use this new training set to predict the next batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extended_pltrain_for_batch(testbatch_X_unscaled_df, pltrain_X_unscaled_df, pltrain_y_df,\n",
    "                                   knnwithprobs_Xy_unscaled, transformer, knn_lookup):\n",
    "    # train a classificator on the pseudo labeled train (pltrain) dataset\n",
    "    pltrain_X_scaled_df = transformer.apply_scaler(pltrain_X_unscaled_df)\n",
    "    linear_svc = get_classifier('svc')\n",
    "    linear_svc.fit(pltrain_X_scaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "    xgboost = get_classifier('xgb')\n",
    "    xgboost.fit(pltrain_X_unscaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "    testbatch_y = classify(xgboost, linear_svc, testbatch_X_unscaled_df, knnwithprobs_Xy_unscaled, transformer,\n",
    "                           knn_lookup)\n",
    "\n",
    "    # add batch to pseudo labeled train (pltrain) dataset. needs to ignore index as ids in test also start with 0\n",
    "    pltrainnew_X_unscaled_df = pltrain_X_unscaled_df.append(testbatch_X_unscaled_df, ignore_index=True)\n",
    "    pltrainnew_y_df = pltrain_y_df.append(testbatch_y,\n",
    "                                          ignore_index=True)  # pltrainnew_Xy_unscaled_df[['fraud']], ignore_index=True)\n",
    "\n",
    "    return pltrainnew_X_unscaled_df, pltrainnew_y_df\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extended_pltrain_for_batch_without_knn(testbatch_X_unscaled_df, pltrain_X_unscaled_df, pltrain_y_df,transformer):\n",
    "    start = time.time()\n",
    "    # train a classificator on the pseudo labeled train (pltrain) dataset\n",
    "    pltrain_X_scaled_df = transformer.apply_scaler(pltrain_X_unscaled_df)\n",
    "    linear_svc = get_classifier('svc')\n",
    "    linear_svc.fit(pltrain_X_scaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "    xgboost = get_classifier('xgb')\n",
    "    xgboost.fit(pltrain_X_unscaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "    testbatch_y = classify_without_knn(xgboost, linear_svc, testbatch_X_unscaled_df, transformer)\n",
    "    \n",
    "    # add batch to pseudo labeled train (pltrain) dataset. needs to ignore index as ids in test also start with 0\n",
    "    pltrainnew_X_unscaled_df = pltrain_X_unscaled_df.append(testbatch_X_unscaled_df, ignore_index=True)\n",
    "    pltrainnew_y_df = pltrain_y_df.append(testbatch_y,\n",
    "                                          ignore_index=True)  # pltrainnew_Xy_unscaled_df[['fraud']], ignore_index=True)\n",
    "    print(\"Calculating batch takes \", time.time()-start)\n",
    "    return pltrainnew_X_unscaled_df, pltrainnew_y_df\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def semi_supervised_learning_procedure(test_X_unscaled, train_y, test_data_dict, transformer):\n",
    "\n",
    "\n",
    "data_dict = dict()\n",
    "data_dict['train_complete_X_unscaled_df'] = train_complete_X_unscaled_df.copy()\n",
    "data_dict['train_complete_X_scaled_df'] = transformer.apply_scaler(train_complete_X_unscaled_df)\n",
    "data_dict['train_complete_y_df'] = train_complete_y_df.copy()\n",
    "\n",
    "data_dict['knnwithprob_Xy_unscaled_df'] = transformer.add_features(knnwithprobs_Xy_df.copy())\n",
    "data_dict['val_X_unscaled_df'] = val_X_unscaled_df.copy()\n",
    "data_dict['val_X_scaled_df'] = transformer.apply_scaler(val_X_unscaled_df)\n",
    "data_dict['val_y_df'] = val_y_df.copy()\n",
    "data_dict['train_X_wo_knn_unscaled_df'] = train_X_wo_knn_unscaled_df\n",
    "data_dict['train_X_wo_knn_scaled_df'] = train_X_wo_knn_scaled_df\n",
    "data_dict['train_y_wo_knn_df'] = train_y_wo_knn_df\n",
    "\n",
    "# results, final_prediction, knn_dataframes, pred_dfs, pred_val_dfs, results_wo_knn, final_prediction_wo_knn,  pred_train_wo_knn_dfs, pred_val_wo_knn_dfs\n",
    "res, fin_pred, knnp_reds, train_preds, val_preds, results_wo_knn, final_prediction_wo_knn,  pred_train_wo_knn_dfs, pred_val_wo_knn_dfs= semi_supervised_learning_procedure(test_X_unscaled_df, train_X_unscaled_df, train_y_df, data_dict, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_complete_X_unscaled_df.copy().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_pred.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results_ssl(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results_ssl(results_wo_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pred_val_wo_knn_dfs[0]\n",
    "val[val.fraud != val.own_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val[val.fraud != val.lsvc_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val[val.fraud != val.xgb_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val3 = val_preds[2]\n",
    "display(val3[(val3.xgb_predict != val3.fraud)])\n",
    "print(\"lsvc\")\n",
    "display(val3[(val3.lsvc_predict != val3.fraud)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val3 = val_preds[2].copy()\n",
    "lst = []\n",
    "for idx, row in val3.iterrows():\n",
    "    if row.own_predict == 1:\n",
    "        if ((row.xgb_proba + row.lsvc_proba)/2) >= (25/35) :\n",
    "            lst.append(1)          \n",
    "        else : lst.append(0)\n",
    "\n",
    "    else : lst.append(0)\n",
    "\n",
    "val3['custom_predict'] = lst\n",
    "val3[(val3.fraud != val3.own_predict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_scaler = MinMaxScaler(feature_range=(0.5,1))\n",
    "xgb_scaler.fit([val3.xgb_proba])\n",
    "val3['xgb_scaled'] = np.ravel(xgb_scaler.transform([val3.xgb_proba]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val3[(val3.lsvc_predict == 1) & (val3.lsvc_proba >= (25/35))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val3[(val3.xgb_predict == 1) & (val3.xgb_proba >= (25/35))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3 = train_preds[2]\n",
    "knn3 = knnp_reds[2]\n",
    "knn3[(knn3.xgb_predict != knn3.fraud) | (knn3.fraud != knn3.lsvc_predict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3 = train_preds[2]\n",
    "knn3 = knnp_reds[2]\n",
    "\n",
    "lst = []\n",
    "for idx, row in knn3.iterrows():\n",
    "    if row.own_predict == 1:\n",
    "        if ((row.xgb_proba + row.lsvc_proba)/2) >= (25/35) :\n",
    "            lst.append(1)          \n",
    "        else : lst.append(0)\n",
    "\n",
    "    else : lst.append(0)\n",
    "\n",
    "knn3['custom_predict'] = lst\n",
    "\n",
    "knn3[(knn3.xgb_predict != knn3.fraud) | (knn3.fraud != knn3.lsvc_predict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn3[(knn3.xgb_predict != knn3.fraud) | (knn3.fraud != knn3.lsvc_predict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn3[(knn3.xgb_predict != knn3.fraud) | (knn3.fraud != knn3.lsvc_predict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4 = train_preds[3]\n",
    "knn4 = knnp_reds[3]\n",
    "knn4[(knn4.xgb_predict != knn4.fraud) | (knn4.fraud != knn4.lsvc_predict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1[(train1.xgb_predict != train1.fraud) | (train1.fraud != train1.lsvc_predict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prf_own = metrics.precision_recall_fscore_support(train1.fraud, train1.own_predict, beta=0.5172, average='binary')\n",
    "prf_xgb = metrics.precision_recall_fscore_support(train1.fraud, train1.xgb_predict, beta=0.5172, average='binary')\n",
    "prf_lsvc = metrics.precision_recall_fscore_support(train1.fraud, train1.lsvc_predict, beta=0.5172, average='binary')\n",
    "dmc_score_own = np.sum(metrics.confusion_matrix(train1.fraud, train1.own_predict)* np.array([[0, -25], [-5, 5]]))\n",
    "dmc_score_xgb = np.sum(metrics.confusion_matrix(train1.fraud, train1.xgb_predict)* np.array([[0, -25], [-5, 5]]))\n",
    "dmc_score_lsvc = np.sum(metrics.confusion_matrix(train1.fraud, train1.lsvc_predict)* np.array([[0, -25], [-5, 5]]))\n",
    "\n",
    "\n",
    "print(\"OWN CLASSIFIER: Precision: {} Recall: {} Fbeta: {} DMC: {}\".format(prf[0], prf[1], prf[2],dmc_score_own))\n",
    "print(\"XGBOOST: Precision: {} Recall: {} Fbeta: {} DMC: {}\".format(prf_xgb[0], prf_xgb[1], prf_xgb[2],dmc_score_xgb))\n",
    "print(\"LINEAR SVCPrecision: {} Recall: {} Fbeta: {} DMC: {}\".format(prf_lsvc[0], prf_lsvc[1], prf_lsvc[2],dmc_score_lsvc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.) pseudo-label the test set and create new classifier based on this\n",
    "first we predict the original test data labels using the new extended pltrain from above cell and second we use this test data labels to train a new classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.) evaluate our new classifier with the original training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.) combine the pseudo labeled test set with the original train data to train our final classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.) predict labels for the test set using our final classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.) generate output file neeeded for submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.) evaluate our new classifier with the validation set\n",
    "Now at the very end we can also test our final model on a validation split never used before. just for comparison. \n",
    "\n",
    "**For the final submission, the following code should will not be run and the full train (incl. this val split) set will be used above**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
