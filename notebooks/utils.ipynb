{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef calc_scores(y_test, y_pred):\\n    accuracy = metrics.accuracy_score(y_test, y_pred)\\n    confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\\n    f2_score = (0 if all(y_pred == 0) else metrics.fbeta_score(y_test, y_pred, beta=2))\\n    dmc_score = np.sum(confusion_matrix * np.array([[0, -25], [-5, 5]]))\\n\\n    return accuracy, f2_score, dmc_score, confusion_matrix\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def calc_scores(y_test, y_pred):\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "    f2_score = (0 if all(y_pred == 0) else metrics.fbeta_score(y_test, y_pred, beta=2))\n",
    "    dmc_score = np.sum(confusion_matrix * np.array([[0, -25], [-5, 5]]))\n",
    "\n",
    "    return accuracy, f2_score, dmc_score, confusion_matrix\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def own_scorer(estimator, X_val, ground_truth):\n",
    "    prediction = estimator.predict(X_val)\n",
    "    confusion_matrix = metrics.confusion_matrix(ground_truth, prediction)\n",
    "    dmc_score = np.sum(confusion_matrix * np.array([[0, -25], [-5, 5]]))\n",
    "    return dmc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def own_scorer_normalized(estimator, X_val, ground_truth):\n",
    "    prediction = estimator.predict(X_val)\n",
    "    confusion_matrix = metrics.confusion_matrix(ground_truth, prediction)\n",
    "    dmc_score = np.sum(confusion_matrix * np.array([[0, -25], [-5, 5]]))\n",
    "    return dmc_score/len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def own_f2_score(estimator, X_val, ground_truth):\n",
    "    prediction = estimator.predict(X_val)\n",
    "    return 0 if all(prediction == 0) else metrics.fbeta_score(ground_truth, prediction, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_best_scores(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                results['mean_test_score'][candidate],\n",
    "                results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def own_cross_validation(classifier, data, cv=5):\n",
    "    k = KFold(n_splits=cv)\n",
    "    splits = k.get_n_splits(data)\n",
    "    print(splits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your Classifier by multiple k fold cross validates and shuffles. Currently only sklearn familiar classifiers are allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classification(classifier, df_train, df_val, predict_proba=True, ret_dataframe=True, ):\n",
    "    result = dict()\n",
    "    dmc_scores = list()\n",
    "    dmc_scores_norm = list()\n",
    "    res_dataframe = df_val.copy()\n",
    "    y_train = df_train.fraud\n",
    "    x_train = df_train.drop(columns=['fraud'])\n",
    "    y_val = df_val.fraud\n",
    "    x_val = df_val.drop(columns=['fraud'])\n",
    "    \n",
    "    classifier.fit(x_train, y_train)\n",
    "    \n",
    "    dmc_score = own_scorer(classifier, x_val, y_val)\n",
    "    dmc_norm = own_scorer_normalized(classifier, x_val, y_val)\n",
    "    res_dataframe['prediction'] = classifier.predict(x_val)\n",
    "    \n",
    "    if predict_proba:\n",
    "        res_dataframe['probablity'] = [round(max(x),3) for x in classifier.predict_proba(x_val)]\n",
    "    # Cross Validation\n",
    "    scorings = {\"DMC\" : own_scorer, \"DMC_Norm\" : own_scorer_normalized}\n",
    "\n",
    "    \n",
    "    x_train_complete = x_train.append(x_val)\n",
    "    y_train_complete = y_train.append(y_val)\n",
    "    cross_validation = cross_validate(classifier, x_train_complete,y_train_complete,scoring=scorings, cv=5)\n",
    "    cv_dmc_mean = sum(cross_validation['test_DMC'])/len(cross_validation['test_DMC'])\n",
    "    cv_dmc_norm_mean = sum(cross_validation['test_DMC_Norm'])/len(cross_validation['test_DMC_Norm'])\n",
    "    \n",
    "    result = {\"dmc_score\" : dmc_score, \"dmc_score_norm\" : dmc_norm, \"cv_dmc_score\": cv_dmc_mean,\"cv_dmc_score_norm\":cv_dmc_norm_mean}\n",
    "    if ret_dataframe:\n",
    "        result['dataframe'] = res_dataframe\n",
    "    print(\"Results Fix Split: \\nDMC Score: {}  ---  Normalized DMC Score: {}, \\n\\nResults Cross Validation: \\nDMC Score: {}  ---  Normalized DMC Score: {} \".format(dmc_score, dmc_norm, cv_dmc_mean,cv_dmc_norm_mean))\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding additional features by calling this on the specific Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_features(dataframe):\n",
    "    dataframe['totalScannedItems'] = dataframe['scannedLineItemsPerSecond'] * dataframe['totalScanTimeInSeconds']\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale dataframe with Sklean Scaler (Please scale only model input \"train_x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataframe(df, scaler):\n",
    "    df_tmp = pd.DataFrame()\n",
    "    tmp_data = scaler.transform(df[df.columns])\n",
    "    df_tmp[df.columns] = pd.DataFrame(tmp_data)\n",
    "    return df_tmp.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find nearest neigbour to test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbor(row_scaled, dataset_scaled):  \n",
    "    diffs = [np.sum((row_scaled[0] - ds_row)**2) for ds_row in dataset_scaled]\n",
    "    idx = np.argmin(diffs)[0]\n",
    "    return idx, diffs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_classifier_for_sample(idx, validation_set):\n",
    "    ground_truth = validation_set.iloc[idx].fraud\n",
    "    \n",
    "    # Both classifier predicted the calue correctly\n",
    "    if (validation_set.iloc[idx].lsvc_predict == ground_truth) and (validation_set.iloc[idx].xgb_predict == ground_truth):\n",
    "        if validation_set.iloc[idx].lsvc_proba > validation_set.iloc[idx].xgb_proba:\n",
    "            return \"lsvc\"\n",
    "        else:\n",
    "            return \"xgboost\"\n",
    "    # lsvc predicted correctly\n",
    "    elif (validation_set.iloc[idx].lsvc_predict == ground_truth) and (validation_set.iloc[idx].xgb_predict != ground_truth):\n",
    "        return \"lsvc\"\n",
    "    \n",
    "    # xgboost predicted correcltly\n",
    "    elif (validation_set.iloc[idx].lsvc_predict != ground_truth) and (validation_set.iloc[idx].xgb_predict == ground_truth):\n",
    "        return \"xgboost\"\n",
    "    \n",
    "    # If No classifier predicted the knn correct, None is returned\n",
    "    else: \n",
    "        return None\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer:\n",
    "    \"\"\"\n",
    "    for scaling, data transformations (new features, one-hot encoding, categorical, ...)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scaler):\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def fit_scaler(self, df):\n",
    "        df_tmp = df.copy()\n",
    "        self.scaler.fit(df_tmp.astype(np.float64))\n",
    "        return self\n",
    "        \n",
    "    def apply_scaler(self, df):\n",
    "        df_temp = df.copy()\n",
    "        return pd.DataFrame(self.scaler.transform(df_temp),df_temp.index, df_temp.columns)\n",
    "    \n",
    "    def inverse_scale(self, df):\n",
    "        df_tmp = df.copy()\n",
    "        return pd.DataFrame(self.scaler.inverse_transform(df_tmp), df_tmp.index, df_tmp.columns)\n",
    "    \n",
    "    def add_features(self,df):\n",
    "        #TODO: Choose relevant features\n",
    "        df_tmp = df.copy()\n",
    "        df_tmp['totalScannedLineItems'] = df_tmp['scannedLineItemsPerSecond'] * df_tmp['totalScanTimeInSeconds']\n",
    "        #df['avgTimePerScan'] = 1/ df['scannedLineItemsPerSecond']\n",
    "        #df['avgValuePerScan'] = df['avgTimePerScan'] * df['valuePerSecond']\n",
    "        #df['withoutRegisPerPosition'] = df['scansWithoutRegistration'] / df['totalScannedLineItems'] #equivalent to lineItemVoidsPerPosition?\n",
    "        #df['quantiModPerPosition'] = df['quantityModifications'] / df['totalScannedLineItems']\n",
    "        #df['lineItemVoidsPerTotal'] = df['lineItemVoids'] / df['grandTotal']\n",
    "        #df['withoutRegisPerTotal'] = df['scansWithoutRegistration'] / df['grandTotal']\n",
    "        #df['quantiModPerTotal'] = df['quantityModifications'] / df['grandTotal']\n",
    "        #df['lineItemVoidsPerTime'] = df['lineItemVoids'] / df['totalScanTimeInSeconds']\n",
    "        #df['withoutRegisPerTime'] = df['scansWithoutRegistration'] / df['totalScanTimeInSeconds']\n",
    "        #df['quantiModPerTime'] = df['quantityModifications'] / df['totalScanTimeInSeconds']\n",
    "        #df['valuePerScannedLineItem'] = df['valuePerSecond'] / df['scannedLineItemsPerSecond']\n",
    "        return df_tmp\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        All in one: Apply all transform methods\n",
    "            1.) addFeatures\n",
    "            2.) apply_scaler\n",
    "        \"\"\"\n",
    "        df_tmp = df.copy()\n",
    "        return self.apply_scaler(self.add_features(df_tmp))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNLookup():\n",
    "\n",
    "    def __init__(self, knn_data):\n",
    "\n",
    "        #self.knn = NearestNeighbors(n_neighbors=1)\n",
    "        #self.knn.fit(knn_data.values.tolist())\n",
    "\n",
    "        self.tree = KDTree(knn_data)\n",
    "\n",
    "    def refit(self, knn_data):\n",
    "        self.tree = KDTree(knn_data)\n",
    "\n",
    "    def find_nearest_neighbor(self, row_scaled, dataset_scaled):\n",
    "        diffs = [np.sum((row_scaled - ds_row) ** 2) for idx, ds_row in dataset_scaled.iterrows()]\n",
    "        idx = np.argmin(diffs)\n",
    "        return idx, diffs[idx]\n",
    "\n",
    "    def find_nearest_neighbor2(self, row_scaled, dataset_scaled):\n",
    "        dist, ind = self.tree.query([row_scaled.values], k=1)\n",
    "        return np.ravel(ind)[0], np.ravel(dist)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "def plot_results_ssl(result_dict):\n",
    "    # recall = TP / (TP + FN)\n",
    "    # precision = TP / (TP + FP)\n",
    "\n",
    "    dmc_scores_val = {}\n",
    "    #dmc_scores_val = dict.fromkeys(['lin_svg', 'xgboost', 'own_classifier'])\n",
    "    dmc_scores_train = {}\n",
    "    precision_val = {}\n",
    "    precision_train = {}\n",
    "    recall_val = {}\n",
    "    recall_train = {}\n",
    "\n",
    "    for iteration_number, elem in enumerate(res):\n",
    "        iteration_dict = elem\n",
    "        #print(\"# Iteration: \", iteration_number, \"\\n# Value: \", iteration_dict)\n",
    "        for classifier, results in iteration_dict.items():\n",
    "            #print(\"\\t## Classifier: \", classifier, \"\\n\\t## Results: \", results)\n",
    "            for set_name, result in results.items():\n",
    "                conf_matrix = result['conf_matrix']\n",
    "                tp = conf_matrix[0][0]\n",
    "                fp = conf_matrix[0][1]\n",
    "                fn = conf_matrix[1][0]\n",
    "                tn = conf_matrix[1][1]\n",
    "\n",
    "                recall = tp / (tp + fn)\n",
    "                precision = tp / (tp + fp)\n",
    "\n",
    "                if set_name == \"val\":\n",
    "                    if classifier in dmc_scores_val:\n",
    "                        dmc_scores_val[classifier].append(result['dmc_score'])\n",
    "                    else:\n",
    "                        dmc_scores_val[classifier] = [result['dmc_score']]\n",
    "                    if classifier in recall_val:\n",
    "                        recall_val[classifier].append(recall)\n",
    "                    else:\n",
    "                        recall_val[classifier] = [recall]\n",
    "                    if classifier in precision_val:\n",
    "                        precision_val[classifier].append(precision)\n",
    "                    else:\n",
    "                        precision_val[classifier] = [precision]\n",
    "                else:\n",
    "                    if classifier in dmc_scores_train:\n",
    "                        dmc_scores_train[classifier].append(result['dmc_score'])\n",
    "                    else:\n",
    "                        dmc_scores_train[classifier] = [result['dmc_score']]\n",
    "                    if classifier in recall_train:\n",
    "                        recall_train[classifier].append(recall)\n",
    "                    else:\n",
    "                        recall_train[classifier] = [recall]\n",
    "                    if classifier in precision_train:\n",
    "                        precision_train[classifier].append(precision)\n",
    "                    else:\n",
    "                        precision_train[classifier] = [precision]\n",
    "                #print(\"\\t\\t### Set: \", set_name, \"\\n\\t\\t### Result: \", result)\n",
    "                #print(\"TP: \", tp, \"\\tFP: \", fp, \"\\tFN: \", fn, \"\\tTN: \", tn)\n",
    "\n",
    "\n",
    "    #print(dmc_scores_val)\n",
    "    #print(dmc_scores_train)\n",
    "    #print(precision_val)\n",
    "    #print(precision_train)\n",
    "    #print(recall_val)\n",
    "    #print(recall_train)\n",
    "    \n",
    "    plt.figure(num=1, figsize=(7,7))\n",
    "    \n",
    "    # ----------------- DMC Score on train set ---------------------------------\n",
    "    \n",
    "    ax1 = plt.subplot(311)\n",
    "    ax1.set_ylim(-80, 520)\n",
    "    \n",
    "    x_values = np.arange(1, len(dmc_scores_train['lin_svc']) + 1, 1)\n",
    "    x_new = np.linspace(x_values.min(), x_values.max(), 300)\n",
    "    \n",
    "    ax1.set_xticks(x_values)\n",
    "   \n",
    "    spl = make_interp_spline(x_values, dmc_scores_train['lin_svc'], k=3) #BSpline object\n",
    "    smooth_lin_svc = spl(x_new)\n",
    "    ax1.plot(x_new, smooth_lin_svc, label='lin_svc', color='r') # Entwicklung der Lin SVC\n",
    "    \n",
    "    spl = make_interp_spline(x_values, dmc_scores_train['xgboost'], k=3) \n",
    "    smooth_xgboost = spl(x_new)\n",
    "    ax1.plot(x_new, smooth_xgboost, label='xgboost', color='b') # Entwicklung des XGBoostings\n",
    "    \n",
    "    spl = make_interp_spline(x_values, dmc_scores_train['own_classifier'], k=3) \n",
    "    smooth_own_classifier = spl(x_new)\n",
    "    ax1.plot(x_new, smooth_own_classifier, label='own_classifier', color='g') # Entwicklung des eigenen Klassifizierers\n",
    "    \n",
    "    ax1.set_title('DMC Score on the train set')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.set_xlabel('Iteration')\n",
    "    ax1.legend()\n",
    "\n",
    "    # ----------------- Precision on train set ---------------------------------\n",
    "    \n",
    "    ax2 = plt.subplot(312)\n",
    "    \n",
    "    x_values = np.arange(1, len(precision_train['lin_svc']) + 1, 1)\n",
    "    x_new = np.linspace(x_values.min(), x_values.max(), 300)\n",
    "    \n",
    "    ax2.set_xticks(x_values)\n",
    "    \n",
    "    spl = make_interp_spline(x_values, precision_train['lin_svc'], k=3) \n",
    "    smooth_lin_svc = spl(x_new)\n",
    "    ax2.plot(x_new, smooth_lin_svc, label='lin_svc', color='r')# Entwicklung der Lin SVC\n",
    "    \n",
    "    spl = make_interp_spline(x_values, precision_train['xgboost'], k=3) \n",
    "    smooth_xgboost = spl(x_new)\n",
    "    ax2.plot(x_new, smooth_xgboost, label='xgboost', color='b')# Entwicklung des XGBoostings\n",
    "    \n",
    "    spl = make_interp_spline(x_values, precision_train['own_classifier'], k=3) \n",
    "    smooth_own_classifier = spl(x_new)\n",
    "    ax2.plot(x_new, smooth_own_classifier, label='own_classifier', color='g')# Entwicklung des eigenen Klassifizierers\n",
    "    ax2.set_title('Precision on the train set')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_xlabel('Iteration')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # ----------------- Recall on train set ---------------------------------\n",
    "\n",
    "    ax3 = plt.subplot(313)\n",
    "    \n",
    "    x_values = np.arange(1, len(recall_train['lin_svc']) + 1, 1)\n",
    "    x_new = np.linspace(x_values.min(), x_values.max(), 300)\n",
    "    \n",
    "    ax3.set_xticks(x_values)\n",
    "    \n",
    "    spl = make_interp_spline(x_values, recall_train['lin_svc'], k=3) \n",
    "    smooth_lin_svc = spl(x_new)\n",
    "    ax3.plot(x_new, smooth_lin_svc, label='lin_svc', color='r')# Entwicklung der Lin SVC\n",
    "    \n",
    "    spl = make_interp_spline(x_values, recall_train['xgboost'], k=3) \n",
    "    smooth_xgboost = spl(x_new)\n",
    "    ax3.plot(x_new, smooth_xgboost, label='xgboost', color='b')# Entwicklung des XGBoostings\n",
    "    \n",
    "    spl = make_interp_spline(x_values, recall_train['own_classifier'], k=3) \n",
    "    smooth_own_classifier = spl(x_new)\n",
    "    ax3.plot(x_new, smooth_own_classifier, label='own_classifier', color='g')# Entwicklung des eigenen Klassifizierers\n",
    "    ax3.set_title('Recall on the train set')\n",
    "    ax3.set_ylabel('Recall')\n",
    "    ax3.set_xlabel('Iteration')\n",
    "    ax3.legend()\n",
    "    plt.subplots_adjust(left=0.0, right=1.0, bottom=1.8, top=3.5)\n",
    "\n",
    "    plt.figure(2, figsize=(7,7))\n",
    "\n",
    "    # ----------------- DMC Score on val set ---------------------------------\n",
    "\n",
    "    ax4 = plt.subplot(311)\n",
    "    ax4.set_ylim(-80, 115)\n",
    "    \n",
    "    x_values = np.arange(1, len(dmc_scores_val['lin_svc']) + 1, 1)\n",
    "    x_new = np.linspace(x_values.min(), x_values.max(), 300)\n",
    "    \n",
    "    ax4.set_xticks(x_values)\n",
    "    \n",
    "    spl = make_interp_spline(x_values, dmc_scores_val['lin_svc'], k=3) \n",
    "    smooth_lin_svc = spl(x_new)\n",
    "    ax4.plot(x_new, smooth_lin_svc, label='lin_svc', color='r')# Entwicklung der Lin SVC\n",
    "    \n",
    "    spl = make_interp_spline(x_values, dmc_scores_val['xgboost'], k=3) \n",
    "    smooth_xgboost = spl(x_new)\n",
    "    ax4.plot(x_new, smooth_xgboost, label='xgboost', color='b')# Entwicklung des XGBoostings\n",
    "    \n",
    "    spl = make_interp_spline(x_values, dmc_scores_val['own_classifier'], k=3) \n",
    "    smooth_own_classifier = spl(x_new)\n",
    "    ax4.plot(x_new, smooth_own_classifier, label='own_classifier', color='g')# Entwicklung des eigenen Klassifizierers\n",
    "    ax4.set_title('DMC Score on the val set')\n",
    "    ax4.set_ylabel('Score')\n",
    "    ax4.set_xlabel('Iteration')\n",
    "    ax4.legend()\n",
    "\n",
    "    # ----------------- Precision on val set ---------------------------------\n",
    "    \n",
    "    ax5 = plt.subplot(312)\n",
    "    \n",
    "    x_values = np.arange(1, len(precision_val['lin_svc']) + 1, 1)\n",
    "    x_new = np.linspace(x_values.min(), x_values.max(), 300)\n",
    "    \n",
    "    ax5.set_xticks(x_values)\n",
    "    \n",
    "    spl = make_interp_spline(x_values, precision_val['lin_svc'], k=3) \n",
    "    smooth_lin_svc = spl(x_new)    \n",
    "    ax5.plot(x_new, smooth_lin_svc, label='lin_svc', color='r')# Entwicklung der Lin SVC\n",
    "    \n",
    "    spl = make_interp_spline(x_values, precision_val['xgboost'], k=3) \n",
    "    smooth_xgboost = spl(x_new)\n",
    "    ax5.plot(x_new, smooth_xgboost, label='xgboost', color='b')# Entwicklung des XGBoostings\n",
    "    \n",
    "    spl = make_interp_spline(x_values, precision_train['own_classifier'], k=3) \n",
    "    smooth_own_classifier = spl(x_new)\n",
    "    ax5.plot(x_new, smooth_own_classifier, label='own_classifier', color='g')# Entwicklung des eigenen Klassifizierers\n",
    "    ax5.set_title('Precision on the val set')\n",
    "    ax5.set_ylabel('Precision')\n",
    "    ax5.set_xlabel('Iteration')\n",
    "    ax5.legend()\n",
    "    \n",
    "    # ----------------- Recall on train set ---------------------------------\n",
    "\n",
    "    ax6 = plt.subplot(313)\n",
    "    \n",
    "    x_values = np.arange(1, len(recall_val['lin_svc']) + 1, 1)\n",
    "    x_new = np.linspace(x_values.min(), x_values.max(), 300)\n",
    "    \n",
    "    ax6.set_xticks(x_values)\n",
    "    \n",
    "    spl = make_interp_spline(x_values, recall_val['lin_svc'], k=3) \n",
    "    smooth_lin_svc = spl(x_new)\n",
    "    ax6.plot(x_new, smooth_lin_svc, label='lin_svc', color='r')# Entwicklung der Lin SVC\n",
    "    \n",
    "    spl = make_interp_spline(x_values, recall_val['xgboost'], k=3) \n",
    "    smooth_xgboost = spl(x_new)\n",
    "    ax6.plot(x_new, smooth_xgboost, label='xgboost', color='b')# Entwicklung des XGBoostings\n",
    "    \n",
    "    spl = make_interp_spline(x_values, recall_val['own_classifier'], k=3) \n",
    "    smooth_own_classifier = spl(x_new)\n",
    "    ax6.plot(x_new, smooth_own_classifier, label='own_classifier', color='g')# Entwicklung des eigenen Klassifizierers\n",
    "    ax6.set_title('Recall on the val set')\n",
    "    ax6.set_ylabel('Recall')\n",
    "    ax6.set_xlabel('Iteration')\n",
    "    ax6.legend()\n",
    "\n",
    "    plt.subplots_adjust(left=0.0, right=1.0, bottom=1.8, top=3.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation for own classifier with ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def own_classifier_cross_validation(train_data_Xy_unscaled,test_dataset_X_unscaled, n_pltest_samples, cv, transformer):\n",
    "    k = KFold(n_splits=cv)\n",
    "    data_X_unscaled = train_data_Xy_unscaled.copy().drop(columns=['fraud'])\n",
    "    data_y_unscaled = train_data_Xy_unscaled.fraud\n",
    "    test_dataset = test_dataset_X_unscaled.copy()\n",
    "    accs, dmcs, conf_mats = [],[],[]\n",
    "    for train_index, test_index in k.split(data_X_unscaled):\n",
    "        \n",
    "        train_data_Xy_unscaled = data_X_unscaled.copy().iloc[train_index]\n",
    "        train_data_X_unscaled = data_X_unscaled.copy().iloc[train_index]\n",
    "        train_data_X_scaled = transformer.apply_scaler(train_data_X_unscaled)\n",
    "        train_data_y = data_y_unscaled.copy().iloc[train_index]\n",
    "               \n",
    "        test_dataset_X_unscaled = test_dataset.copy().sample(frac=1)        \n",
    "        testbatch_X_df_unscaled = test_dataset_X_unscaled.iloc[:n_pltest_samples].copy().reset_index(drop=True)\n",
    "        testbatch_X_df_scaled = transformer.apply_scaler(testbatch_X_df_unscaled)\n",
    "        \n",
    "        pltrain_X_unscaled = train_data_X_unscaled.copy()\n",
    "        pltrain_y = train_data_y.copy()\n",
    "        \n",
    "        pltrain_X_unscaled, pltrain_y = get_extended_pltrain_for_batch_without_knn(testbatch_X_df_unscaled, train_data_X_unscaled, train_data_y, transformer)\n",
    "        \n",
    "        linear_svc = get_classifier(\"svc\")\n",
    "        pltrain_X_scaled = transformer.apply_scaler(pltrain_X_unscaled)\n",
    "        linear_svc.fit(pltrain_X_scaled.values, pltrain_y.values)\n",
    "        \n",
    "        xgboost = get_classifier(\"xgb\")\n",
    "        xgboost.fit(pltrain_X_unscaled.values, pltrain_y.values)\n",
    "              \n",
    "        \n",
    "        validation_X_unscaled = data_X_unscaled.copy().iloc[test_index]\n",
    "        validation_y = data_y_unscaled.copy().iloc[test_index]\n",
    "        \n",
    "        val_preds = classify_without_knn(xgboost, linear_svc, validation_X_unscaled, transformer)\n",
    "        acc, dmc, conf_mat = calc_scores(validation_y, val_preds)\n",
    "        accs.append(acc)\n",
    "        dmcs.append(dmc)\n",
    "        conf_mats.append(conf_mat)\n",
    "        \n",
    "    print(\"DMC Mean: {} --- DMC Sum: {}\".format(np.mean(dmcs), np.sum(dmcs)))\n",
    "    return np.mean(dmcs), np.sum(dmcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn_train_data(train_data_Xy_unscaled, cv, transformer):\n",
    "    k = KFold(n_splits=cv)\n",
    "    data_X_unscaled = train_data_Xy_unscaled.copy().drop(columns=['fraud'])\n",
    "    data_y_unscaled = train_data_Xy_unscaled.fraud\n",
    "    #test_dataset = test_dataset_X_unscaled.copy()\n",
    "    \n",
    "    predicted_val_df = pd.DataFrame()\n",
    "    counter = 1\n",
    "    for train_index, test_index in k.split(data_X_unscaled):\n",
    "        print(counter, \". Split\")\n",
    "        train_data_Xy_unscaled = data_X_unscaled.copy().iloc[train_index]\n",
    "        \n",
    "        train_data_X_unscaled = data_X_unscaled.copy().iloc[train_index]\n",
    "        train_data_X_scaled = transformer.apply_scaler(train_data_X_unscaled)\n",
    "        train_data_y = data_y_unscaled.copy().iloc[train_index]\n",
    "               \n",
    "        validation_X_unscaled = data_X_unscaled.copy().iloc[test_index]\n",
    "        validation_X_scaled = transformer.apply_scaler(validation_X_unscaled)\n",
    "        validation_y = data_y_unscaled.copy().iloc[test_index]\n",
    "        \n",
    "        val_batch_df = validation_X_unscaled.copy()\n",
    "                        \n",
    "        linear_svc = get_classifier(\"svc\")\n",
    "        linear_svc.fit(train_data_X_scaled.values, train_data_y.values)\n",
    "        \n",
    "        xgboost = get_classifier(\"xgb\")\n",
    "        xgboost.fit(train_data_X_unscaled.values, train_data_y.values)\n",
    "        \n",
    "        lsvc_predict = linear_svc.predict(validation_X_scaled.values)\n",
    "        lsvc_proba = linear_svc.predict_proba(validation_X_scaled.values)\n",
    "        \n",
    "        xgb_predict = xgboost.predict(validation_X_unscaled.values)\n",
    "        xgb_proba = xgboost.predict_proba(validation_X_unscaled.values)\n",
    "        \n",
    "        val_batch_df['lsvc_predict'] = lsvc_predict     \n",
    "        val_batch_df['lsvc_proba'] = [round(max(x),3) for x in lsvc_proba]\n",
    "        val_batch_df['xgb_predict'] = xgb_predict     \n",
    "        val_batch_df['xgb_proba'] = [round(max(x),3) for x in xgb_proba]\n",
    "        val_batch_df['fraud'] = validation_y\n",
    "        predicted_val_df = predicted_val_df.append(val_batch_df)\n",
    "        counter +=1\n",
    "        \n",
    "\n",
    "    return predicted_val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras_metrics import binary_average_recall, binary_recall, binary_precision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def keras_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, input_shape=(4,),activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def load_trained_model():\n",
    "    model = keras_model()\n",
    "    model.load_weights(weights_path)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
