{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scores(y_test, y_pred):\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "    f2_score = (0 if all(y_pred == 0) else metrics.fbeta_score(y_test, y_pred, beta=2))\n",
    "    dmc_score = np.sum(confusion_matrix * np.array([[0, -25], [-5, 5]]))\n",
    "\n",
    "    return accuracy, f2_score, dmc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def own_scorer(estimator, X_val, ground_truth):\n",
    "    prediction = estimator.predict(X_val)\n",
    "    confusion_matrix = metrics.confusion_matrix(ground_truth, prediction)\n",
    "    dmc_score = np.sum(confusion_matrix * np.array([[0, -25], [-5, 5]]))\n",
    "    return dmc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def own_scorer_normalized(estimator, X_val, ground_truth):\n",
    "    prediction = estimator.predict(X_val)\n",
    "    confusion_matrix = metrics.confusion_matrix(ground_truth, prediction)\n",
    "    dmc_score = np.sum(confusion_matrix * np.array([[0, -25], [-5, 5]]))\n",
    "    return dmc_score/len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def own_f2_score(estimator, X_val, ground_truth):\n",
    "    prediction = estimator.predict(X_val)\n",
    "    return 0 if all(prediction == 0) else metrics.fbeta_score(ground_truth, prediction, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_best_scores(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                results['mean_test_score'][candidate],\n",
    "                results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your Classifier by multiple k fold cross validates and shuffles. Currently only sklearn familiar classifiers are allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classification(classifier, df_train, df_val, predict_proba=True, ret_dataframe=True, ):\n",
    "    result = dict()\n",
    "    dmc_scores = list()\n",
    "    dmc_scores_norm = list()\n",
    "    res_dataframe = df_val.copy()\n",
    "    y_train = df_train.fraud\n",
    "    x_train = df_train.drop(columns=['fraud'])\n",
    "    y_val = df_val.fraud\n",
    "    x_val = df_val.drop(columns=['fraud'])\n",
    "    \n",
    "    dmc_score = own_scorer(classifier, x_val, y_val)\n",
    "    dmc_norm = own_scorer_normalized(classifier, x_val, y_val)\n",
    "    res_dataframe['prediction'] = classifier.predict(x_val)\n",
    "    \n",
    "    if predict_proba:\n",
    "        res_dataframe['probablity'] = [round(max(x),3) for x in classifier.predict_proba(x_val)]\n",
    "    # Cross Validation\n",
    "    scorings = {\"DMC\" : own_scorer, \"DMC_Norm\" : own_scorer_normalized}\n",
    "\n",
    "    \n",
    "    x_train_complete = x_train.append(x_val)\n",
    "    y_train_complete = y_train.append(y_val)\n",
    "    cross_validation = cross_validate(classifier, x_train_complete,y_train_complete,scoring=scorings, cv=5)\n",
    "    cv_dmc_mean = sum(cross_validation['test_DMC'])/len(cross_validation['test_DMC'])\n",
    "    cv_dmc_norm_mean = sum(cross_validation['test_DMC_Norm'])/len(cross_validation['test_DMC_Norm'])\n",
    "    \n",
    "    result = {\"dmc_score\" : dmc_score, \"dmc_score_norm\" : dmc_norm, \"cv_dmc_score\": cv_dmc_mean,\"cv_dmc_score_norm\":cv_dmc_norm_mean}\n",
    "    if ret_dataframe:\n",
    "        result['dataframe'] = res_dataframe\n",
    "    print(\"Results Fix Split: \\nDMC Score: {}  ---  Normalized DMC Score: {}, \\n\\nResults Cross Validation: \\nDMC Score: {}  ---  Normalized DMC Score: {} \".format(dmc_score, dmc_norm, cv_dmc_mean,cv_dmc_norm_mean))\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding additional features by calling this on the specific Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_features(dataframe):\n",
    "    dataframe['totalScannedItems'] = dataframe['scannedLineItemsPerSecond'] * dataframe['totalScanTimeInSeconds']\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale dataframe with Sklean Scaler (Please scale only model input \"train_x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataframe(df, scaler):\n",
    "    df_tmp = pd.DataFrame()\n",
    "    tmp_data = scaler.fit_transform(df[df.columns])\n",
    "    df_tmp[df.columns] = pd.DataFrame(tmp_data)\n",
    "    return df_tmp.copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
