{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with the Test split?\n",
    "This is what we want to do:\n",
    "\n",
    "1. predict batches with ...\n",
    ".TODO: create list of content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.) set run-config and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_SUBMISSION = True # will perform a test on a validation split if set to False\n",
    "\n",
    "TEST_BATCH_SIZE = 500 # Number of Test entries to add to the training set for the next iteration\n",
    "ITER_PRINT_EVERY = 1 # Which Iterations to print (every nth)\n",
    "\n",
    "TRAIN_WITHOUT_HIGHTRUST = True # Set to True to train classifiers only on rows with trustLevel <= 2\n",
    "\n",
    "# Enable Quick test mode to test functionality on a smaller part of original data or with less iterations etc. than in final version\n",
    "QUICK_TEST_MODE = False\n",
    "#TODO!!! Bug: Plot function plot_results_ssl(res) has error sometimes, e.g. for TEST_BATCH_SIZE=250 and QUICK_TEST_MODE_TEST_SIZE = 1000 or 251. I think it might be when QUICK_TEST_MODE_TEST_SIZE is a multiple of TEST_BATCH_SIZE or if nr. iterations is pretty small\n",
    "QUICK_TEST_MODE_TEST_SIZE = 4000 #set this to the number of test samples you want to use\n",
    "#TODO lower other hyperparams as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: double check which functions we actually need and only import them / copy them into this.\n",
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors, KDTree\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.) define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scores(y_test, y_pred):\n",
    "    y_test_tmp = y_test.copy()\n",
    "    accuracy = metrics.accuracy_score(y_test_tmp, y_pred)\n",
    "    confusion_matrix = (metrics.confusion_matrix(y_test_tmp, y_pred)).tolist()\n",
    "    dmc_score = np.sum(confusion_matrix * np.array([[0, -25], [-5, 5]]))\n",
    "    return accuracy, dmc_score, confusion_matrix\n",
    "\n",
    "def get_classifier(name):\n",
    "    \"\"\"\n",
    "    Old: \n",
    "    XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "                colsample_bytree=1, disable_default_eval_metric=1,eval_metric='aucpr',\n",
    "                gamma=1.8285912697052542, reg_lambda=0.4149772770711012,\n",
    "                max_bin=254, max_delta_step=7.2556696256684035,\n",
    "                max_depth=3, min_child_weight=1.0317712458399741, missing=None,\n",
    "                n_estimators=445, n_jobs=-1, objective='binary:logistic', random_state=0, reg_alpha=0,\n",
    "                scale_pos_weight=1,silent=True,\n",
    "                subsample=1, tree_method='gpu_hist', verbosity=2, seed=42)\n",
    "    NEW: \n",
    "    XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, disable_default_eval_metric=1,\n",
    "       eval_metric='aucpr', gamma=0.8785511762914533,\n",
    "       learnin_rate=0.6349847443673119, learning_rate=0.1,\n",
    "       max_delta_step=8.564303568772093, max_depth=3,\n",
    "       min_child_weight=1.3399467345621474, missing=None, n_estimators=448,\n",
    "       n_gpus=1, n_jobs=1, nthread=None, objective='binary:logistic',\n",
    "       random_state=0, reg_alpha=0, reg_lambda=0.16733567917931627,\n",
    "       scale_pos_weight=1, seed=None, silent=True, subsample=1,\n",
    "       tree_method='gpu_hist', verbosity=2)\n",
    "    \"\"\"\n",
    "    return {\n",
    "            'xgb': XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "                       colsample_bytree=1, disable_default_eval_metric=1,\n",
    "                       eval_metric='aucpr', gamma=0.8785511762914533,\n",
    "                       learnin_rate=0.6349847443673119, learning_rate=0.05,\n",
    "                       max_delta_step=8.564303568772093, max_depth=3,\n",
    "                       min_child_weight=1.3399467345621474, n_estimators=448,\n",
    "                       n_gpus=1, n_jobs=1,  objective='binary:logistic',\n",
    "                       random_state=42, reg_alpha=0, reg_lambda=0.16733567917931627,\n",
    "                       scale_pos_weight=1, silent=True, subsample=1,\n",
    "                       tree_method='gpu_hist', verbosity=2)\n",
    "                if not TRAIN_WITHOUT_HIGHTRUST else \n",
    "                    XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "                       colsample_bytree=1, disable_default_eval_metric=1,\n",
    "                       eval_metric='aucpr', gamma=0.5361394269637965,\n",
    "                       learning_rate=0.05,\n",
    "                       max_delta_step=2.809035999095031, max_depth=2,\n",
    "                       min_child_weight=0.9672585190973582, missing=None, n_estimators=403,\n",
    "                       n_gpus=1, n_jobs=1, nthread=None, objective='binary:logistic',\n",
    "                       random_state=0, reg_alpha=0, reg_lambda=0.16439595189061076,\n",
    "                       scale_pos_weight=1, seed=None, silent=True, subsample=1,\n",
    "                       tree_method='gpu_hist', verbosity=2),\n",
    "            'svc': SVC(C=11.439334564226868, cache_size=8000, class_weight=None, coef0=0.0,\n",
    "                        decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
    "                        kernel='linear', max_iter=-1, probability=True, random_state=42,\n",
    "                        shrinking=False, tol=0.08370638742373739, verbose=0)\n",
    "                if not TRAIN_WITHOUT_HIGHTRUST else \n",
    "                    #TODO: insert hyperparams for lowtrust-only-training here:\n",
    "                    SVC(C=5.675273359994213, cache_size=8000, class_weight=None, coef0=0.0,\n",
    "                        decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
    "                        kernel='linear', max_iter=-1, probability=True, random_state=42,\n",
    "                        shrinking=False, tol=0.07958926694361011, verbose=0)\n",
    "\n",
    "\n",
    "        \n",
    "    }[name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.) Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Xy_original_df = pd.read_csv(\"../data/train.csv\", sep=\"|\") if FINAL_SUBMISSION else pd.read_csv(\"../data/train_new.csv\", sep=\"|\")\n",
    "test_X_original_df  = pd.read_csv(\"../data/test.csv\", sep=\"|\", nrows=None if not QUICK_TEST_MODE else QUICK_TEST_MODE_TEST_SIZE) # For faster testing we can use less data from the test set\n",
    "\n",
    "#Only for test routines\n",
    "val_Xy_original_df = pd.read_csv(\"../data/val_new.csv\", sep=\"|\")\n",
    "train_complete_Xy_original_df = pd.read_csv(\"../data/train.csv\", sep=\"|\")\n",
    "test_final_X_df = pd.read_csv(\"../data/test.csv\", sep=\"|\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.) Prepare Input X and Label Y Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convention for variables names: datasetname_columntype_transformstatus_dataframeornot\n",
    "train_y_original_df = train_Xy_original_df[[\"fraud\"]].copy()\n",
    "train_X_original_df = train_Xy_original_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "\n",
    "# Only for test routie#nes\n",
    "val_y_originial_df = val_Xy_original_df[[\"fraud\"]].copy()\n",
    "val_X_originial_df = val_Xy_original_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "train_complete_y_originial_df = train_complete_Xy_original_df[[\"fraud\"]].copy()\n",
    "train_complete_X_originial_df = train_complete_Xy_original_df.copy().drop(\"fraud\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.) Data Transformation and data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>totalScannedLineItems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.291111</td>\n",
       "      <td>-0.848505</td>\n",
       "      <td>1.332970</td>\n",
       "      <td>-0.433864</td>\n",
       "      <td>0.947966</td>\n",
       "      <td>0.878671</td>\n",
       "      <td>-0.101899</td>\n",
       "      <td>-0.019018</td>\n",
       "      <td>-0.124066</td>\n",
       "      <td>-0.979100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.294480</td>\n",
       "      <td>0.167044</td>\n",
       "      <td>0.311624</td>\n",
       "      <td>0.436284</td>\n",
       "      <td>0.315796</td>\n",
       "      <td>-0.877634</td>\n",
       "      <td>-0.079026</td>\n",
       "      <td>-0.095174</td>\n",
       "      <td>-0.360512</td>\n",
       "      <td>1.328388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  \\\n",
       "0    0.291111               -0.848505    1.332970      -0.433864   \n",
       "1   -0.294480                0.167044    0.311624       0.436284   \n",
       "\n",
       "   scansWithoutRegistration  quantityModifications  scannedLineItemsPerSecond  \\\n",
       "0                  0.947966               0.878671                  -0.101899   \n",
       "1                  0.315796              -0.877634                  -0.079026   \n",
       "\n",
       "   valuePerSecond  lineItemVoidsPerPosition  totalScannedLineItems  \n",
       "0       -0.019018                 -0.124066              -0.979100  \n",
       "1       -0.095174                 -0.360512               1.328388  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "transformer = DataTransformer(scaler)\n",
    "\n",
    "if TRAIN_WITHOUT_HIGHTRUST:\n",
    "    train_all = train_complete_Xy_original_df.copy()    \n",
    "    train_all = transformer.remove_high_trust(train_all)\n",
    "    train_lowtrust_X_original_df = transformer.add_features(train_all.copy().drop(columns=['fraud']))\n",
    "    train_lowtrust_y = train_all.fraud\n",
    "\n",
    "# Adding new Features to train and test set\n",
    "train_X_unscaled_df = transformer.add_features(train_X_original_df if not TRAIN_WITHOUT_HIGHTRUST else train_lowtrust_X_original_df)\n",
    "test_X_unscaled_df = transformer.add_features(test_X_original_df)\n",
    "\n",
    "val_X_unscaled_df = transformer.add_features(val_X_originial_df)\n",
    "train_complete_X_unscaled_df = transformer.add_features(train_complete_X_originial_df) \n",
    "\n",
    "transformer.fit_scaler(transformer.add_features(train_complete_X_unscaled_df.append(test_X_unscaled_df, sort=False)))\n",
    "train_X_scaled_df = transformer.apply_scaler(train_X_unscaled_df)\n",
    "\n",
    "test_X_scaled_df  = transformer.apply_scaler(test_X_unscaled_df)\n",
    "val_X_scaled_df = transformer.apply_scaler(val_X_unscaled_df)\n",
    "train_complete_X_scaled_df = transformer.apply_scaler(train_complete_X_unscaled_df)\n",
    "\n",
    "\n",
    "# labels\n",
    "# TODO: this seems redundant^^\n",
    "\n",
    "train_y_df = train_y_original_df.copy() if not TRAIN_WITHOUT_HIGHTRUST else train_lowtrust_y\n",
    "val_y_df = val_y_originial_df.copy()\n",
    "train_complete_y_df = train_complete_y_originial_df.copy()\n",
    "\n",
    "test_final_X_df = transformer.add_features(test_final_X_df)\n",
    "\n",
    "\n",
    "test_X_scaled_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>totalScannedLineItems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>679.000000</td>\n",
       "      <td>679.000000</td>\n",
       "      <td>679.000000</td>\n",
       "      <td>679.000000</td>\n",
       "      <td>679.000000</td>\n",
       "      <td>679.000000</td>\n",
       "      <td>679.000000</td>\n",
       "      <td>679.000000</td>\n",
       "      <td>679.000000</td>\n",
       "      <td>679.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.511046</td>\n",
       "      <td>911.656848</td>\n",
       "      <td>48.523019</td>\n",
       "      <td>5.543446</td>\n",
       "      <td>4.826215</td>\n",
       "      <td>2.530191</td>\n",
       "      <td>0.047390</td>\n",
       "      <td>0.158916</td>\n",
       "      <td>0.730383</td>\n",
       "      <td>15.359352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500246</td>\n",
       "      <td>535.079567</td>\n",
       "      <td>29.287414</td>\n",
       "      <td>3.426099</td>\n",
       "      <td>3.186784</td>\n",
       "      <td>1.683308</td>\n",
       "      <td>0.130887</td>\n",
       "      <td>0.727461</td>\n",
       "      <td>1.243906</td>\n",
       "      <td>8.699703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>452.500000</td>\n",
       "      <td>23.920000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008653</td>\n",
       "      <td>0.025134</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>900.000000</td>\n",
       "      <td>45.740000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.016875</td>\n",
       "      <td>0.052280</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1373.000000</td>\n",
       "      <td>74.940000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.033464</td>\n",
       "      <td>0.113151</td>\n",
       "      <td>0.759615</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1830.000000</td>\n",
       "      <td>99.820000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.611111</td>\n",
       "      <td>17.035000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  \\\n",
       "count  679.000000              679.000000  679.000000     679.000000   \n",
       "mean     1.511046              911.656848   48.523019       5.543446   \n",
       "std      0.500246              535.079567   29.287414       3.426099   \n",
       "min      1.000000                2.000000    0.070000       0.000000   \n",
       "25%      1.000000              452.500000   23.920000       3.000000   \n",
       "50%      2.000000              900.000000   45.740000       5.000000   \n",
       "75%      2.000000             1373.000000   74.940000       9.000000   \n",
       "max      2.000000             1830.000000   99.820000      11.000000   \n",
       "\n",
       "       scansWithoutRegistration  quantityModifications  \\\n",
       "count                679.000000             679.000000   \n",
       "mean                   4.826215               2.530191   \n",
       "std                    3.186784               1.683308   \n",
       "min                    0.000000               0.000000   \n",
       "25%                    2.000000               1.000000   \n",
       "50%                    5.000000               3.000000   \n",
       "75%                    8.000000               4.000000   \n",
       "max                   10.000000               5.000000   \n",
       "\n",
       "       scannedLineItemsPerSecond  valuePerSecond  lineItemVoidsPerPosition  \\\n",
       "count                 679.000000      679.000000                679.000000   \n",
       "mean                    0.047390        0.158916                  0.730383   \n",
       "std                     0.130887        0.727461                  1.243906   \n",
       "min                     0.000551        0.000044                  0.000000   \n",
       "25%                     0.008653        0.025134                  0.166667   \n",
       "50%                     0.016875        0.052280                  0.375000   \n",
       "75%                     0.033464        0.113151                  0.759615   \n",
       "max                     1.611111       17.035000                 10.000000   \n",
       "\n",
       "       totalScannedLineItems  \n",
       "count             679.000000  \n",
       "mean               15.359352  \n",
       "std                 8.699703  \n",
       "min                 1.000000  \n",
       "25%                 8.000000  \n",
       "50%                15.000000  \n",
       "75%                23.000000  \n",
       "max                30.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_unscaled_df.reset_index(drop=True).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name     | New Features | Scaled |\n",
    "|----------|--------------|--------|\n",
    "| orig     |      [ ]     |   [ ]  |\n",
    "| unscaled |      [X]     |   [ ]  |\n",
    "| scaled   |      [X]     |   [X]  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_shallow_nn(xgboost_fitted, linear_svc_fitted, data_to_predict, transformer, model):\n",
    "    prediction = []\n",
    "    data_to_predict_scaled = transformer.apply_scaler(data_to_predict.copy().reset_index(drop=True))\n",
    "    data_to_predict_unscaled = data_to_predict.copy().reset_index(drop=True)\n",
    "    \n",
    "    threshhold = 25/35    \n",
    "    for i, row in data_to_predict_unscaled.iterrows():\n",
    "        \n",
    "        if row.trustLevel >= 3:\n",
    "            prediction.append(0)\n",
    "\n",
    "        else:\n",
    "            xgb_pred = xgboost_fitted.predict([data_to_predict_unscaled.iloc[i].values])[0]\n",
    "            xgb_prob = max(np.ravel(xgboost_fitted.predict_proba([data_to_predict_unscaled.iloc[i].values])))\n",
    "            #print(\"XGB Prob: \", xgb_prob)\n",
    "            lsvc_pred = linear_svc_fitted.predict([data_to_predict_scaled.iloc[i]])[0]\n",
    "            lsvc_prob = max(np.ravel(linear_svc_fitted.predict_proba([data_to_predict_scaled.iloc[i].values])))\n",
    "            #print(\"LinSVC Prob: \", lsvc_prob)\n",
    "            \n",
    "            res = np.ravel(model.predict(np.array([lsvc_pred,lsvc_prob,xgb_pred,xgb_prob]).reshape((1, -1))))[0]\n",
    "            prediction.append(1 if res > 0.5 else 0)\n",
    "    return pd.DataFrame({\"fraud\": prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_cv(xgboost_fitted, linear_svc_fitted, data_to_predict, transformer, model):\n",
    "    prediction = []\n",
    "    data_to_predict_scaled = transformer.apply_scaler(data_to_predict.copy().reset_index(drop=True))\n",
    "    data_to_predict_unscaled = data_to_predict.copy().reset_index(drop=True)\n",
    "    \n",
    "    threshhold = 25/35    \n",
    "    for i, row in data_to_predict_unscaled.iterrows():\n",
    "        \n",
    "        if row.trustLevel >= 3:\n",
    "            prediction.append(0)\n",
    "\n",
    "        else:\n",
    "            xgb_pred = xgboost_fitted.predict([data_to_predict_unscaled.iloc[i].values])[0]\n",
    "            xgb_prob = max(np.ravel(xgboost_fitted.predict_proba([data_to_predict_unscaled.iloc[i].values])))\n",
    "            #print(\"XGB Prob: \", xgb_prob)\n",
    "            lsvc_pred = linear_svc_fitted.predict([data_to_predict_scaled.iloc[i]])[0]\n",
    "            lsvc_prob = max(np.ravel(linear_svc_fitted.predict_proba([data_to_predict_scaled.iloc[i].values])))\n",
    "            #print(\"LinSVC Prob: \", lsvc_prob)\n",
    "            \n",
    "            res = np.ravel(model.predict(np.array([lsvc_pred,lsvc_prob,xgb_pred,xgb_prob]).reshape((1, -1))))[0]\n",
    "            prediction.append(1 if res > 0.5 else 0)\n",
    "    return pd.DataFrame(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_supervised_learning_procedure(test_X_unscaled,train_X_unscaled, train_y, test_data_dict, transformer, model):\n",
    "    test_data_dict = test_data_dict\n",
    "    pred_train_dataframes = []\n",
    "    pred_val_dataframes = []\n",
    "    results = []\n",
    "    \n",
    "    # initialize pseudo labeled train (pltrain) dataset with the transformed training data\n",
    "    pltrain_X_unscaled_df = train_X_unscaled.copy()\n",
    "    train_X_scaled_df = transformer.apply_scaler(train_X_unscaled)\n",
    "\n",
    "    pltrain_X_unscaled_df = test_data_dict['train_X_unscaled_df'].copy()\n",
    "    pltrain_X_scaled_df = transformer.apply_scaler(pltrain_X_unscaled_df)\n",
    "    pltrain_y_df = test_data_dict['train_y_df']\n",
    "    \n",
    "    \n",
    "    pltrain_y_df = train_y.copy()\n",
    "    train_X_scaled_len = len(train_X_unscaled)\n",
    "    train_X_scaled_len = len(pltrain_X_unscaled_df)\n",
    "    print(\"{} available train data before pseudo labeling\".format(train_X_scaled_len))\n",
    "\n",
    "    train_X_scaled_df = transformer.apply_scaler(train_X_unscaled)   \n",
    "   \n",
    "    print(\"with a batchsize of \",TEST_BATCH_SIZE, \"we will need\", len(test_X_unscaled)/TEST_BATCH_SIZE, \"iterations:\")\n",
    "    for i in range(TEST_BATCH_SIZE, len(test_X_unscaled), TEST_BATCH_SIZE):\n",
    "        if int(i / TEST_BATCH_SIZE) % ITER_PRINT_EVERY == 0:\n",
    "            print(\"iteration\", int(i / TEST_BATCH_SIZE), \"\\t/\", int(np.ceil(len(test_X_unscaled) / TEST_BATCH_SIZE)),\n",
    "                  \"with batch from\", i - TEST_BATCH_SIZE, \"\\t to\", i, \", training with\", len(pltrain_y_df), \"samples\")\n",
    "        # get batch from test set\n",
    "        testbatch_X_df = test_X_unscaled.iloc[i - TEST_BATCH_SIZE:i].copy().reset_index(drop=True)\n",
    "                \n",
    "\n",
    "        \"\"\"\n",
    "        ----------------------------------------------------NEW PART ------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        \n",
    "        pltrain_X_unscaled_df, pltrain_y_df = get_extended_pltrain_for_batch_shallow_nn_cv(testbatch_X_df, pltrain_X_unscaled_df,\n",
    "                                                                             pltrain_y_df, transformer, model)\n",
    "        pltrain_X_scaled_df = transformer.apply_scaler(pltrain_X_unscaled_df)\n",
    "        test_data_dict['pltrain_X_unscaled_df'] = pltrain_X_unscaled_df.copy()\n",
    "        test_data_dict['pltrain_X_scaled_df'] = pltrain_X_scaled_df.copy()\n",
    "        test_data_dict['pltrain_y_df'] = pltrain_y_df.copy()\n",
    "        \n",
    "        \n",
    "        linear_svc = get_classifier('svc')\n",
    "        linear_svc.fit(pltrain_X_scaled_df.values, pltrain_y_df.values)\n",
    "        \n",
    "        \n",
    "        xgboost = get_classifier('xgb')\n",
    "        xgboost.fit(pltrain_X_unscaled_df.values, pltrain_y_df.values)\n",
    "        \n",
    "        \n",
    "        # Only Test from PLTrain without the data from train\n",
    "        test_data_dict['test_X_unscaled_df'] = pltrain_X_unscaled_df.iloc[train_X_scaled_len:]\n",
    "        test_data_dict['test_X_scaled_df'] = pltrain_X_scaled_df.iloc[train_X_scaled_len:]\n",
    "        test_data_dict['test_y_df'] = pltrain_y_df.iloc[train_X_scaled_len:]\n",
    "        \n",
    "        \n",
    "        res, pred_train_df, pred_val_df = test_routine_shallow_nn(test_data_dict, transformer, model)\n",
    "       \n",
    "        results.append(res)\n",
    "        pred_train_dataframes.append(pred_train_df)\n",
    "        pred_val_dataframes.append(pred_val_df)\n",
    "        \n",
    "        # TODO: also output prediction for validation set when trained on full final dataset\n",
    "        if int(i / TEST_BATCH_SIZE) % ITER_PRINT_EVERY == 0:\n",
    "            print(\"No KNN:   XGBoost: PLTrain auf Val: {} --- PLTest auf Train: {} || LinearSVC:  PLTrain auf Val: {} --- PLTest auf Train: {} || Own Classifier:  PLTrain auf Val: {} --- PLTest auf Train: {}\".format(\n",
    "                res['xgboost']['val']['dmc_score'],res['xgboost']['train']['dmc_score'],res['lin_svc']['val']['dmc_score'],res['lin_svc']['train']['dmc_score'],res['own_classifier']['val']['dmc_score'],res['own_classifier']['train']['dmc_score']))\n",
    "        \n",
    "        break\n",
    "    xgb_final = get_classifier(\"xgb\")\n",
    "    lsvc_final = get_classifier(\"svc\")\n",
    "    \n",
    "    xgb_final.fit(pltrain_X_unscaled_df.values, pltrain_y_df.values)\n",
    "    lsvc_final.fit(pltrain_X_scaled_df.values, pltrain_y_df.values)\n",
    "    final_prediction = classify_with_shallow_nn(xgb_final, lsvc_final, test_final_X_df, transformer,model)\n",
    "    \n",
    "    \n",
    "    return results, final_prediction, pred_val_dataframes, pred_val_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.) iterative model training using pseudo-labeling\n",
    "predict batches of the test set, add them to the previous training set and use this new training set to predict the next batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extended_pltrain_for_batch_shallow_nn(testbatch_X_unscaled_df, pltrain_X_unscaled_df, pltrain_y_df,transformer, model):\n",
    "    # train a classificator on the pseudo labeled train (pltrain) dataset\n",
    "    pltrain_X_scaled_df = transformer.apply_scaler(pltrain_X_unscaled_df)\n",
    "    linear_svc = get_classifier('svc')\n",
    "    linear_svc.fit(pltrain_X_scaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "    xgboost = get_classifier('xgb')\n",
    "    xgboost.fit(pltrain_X_unscaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "    testbatch_y = classify_with_shallow_nn(xgboost, linear_svc, testbatch_X_unscaled_df, transformer, model)\n",
    "    \n",
    "    # add batch to pseudo labeled train (pltrain) dataset. needs to ignore index as ids in test also start with 0\n",
    "    pltrainnew_X_unscaled_df = pltrain_X_unscaled_df.append(testbatch_X_unscaled_df, ignore_index=True)\n",
    "    pltrainnew_y_df = pltrain_y_df.append(testbatch_y,\n",
    "                                          ignore_index=True)  # pltrainnew_Xy_unscaled_df[['fraud']], ignore_index=True)\n",
    "    return pltrainnew_X_unscaled_df, pltrainnew_y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extended_pltrain_for_batch_shallow_nn_cv(testbatch_X_unscaled_df, pltrain_X_unscaled_df, pltrain_y_df,transformer, model):\n",
    "    # train a classificator on the pseudo labeled train (pltrain) dataset\n",
    "    pltrain_X_scaled_df = transformer.apply_scaler(pltrain_X_unscaled_df)\n",
    "    linear_svc = get_classifier('svc')\n",
    "    linear_svc.fit(pltrain_X_scaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "    xgboost = get_classifier('xgb')\n",
    "    xgboost.fit(pltrain_X_unscaled_df.values, pltrain_y_df.values)\n",
    "\n",
    "    testbatch_y = classify_cv(xgboost, linear_svc, testbatch_X_unscaled_df, transformer, model)\n",
    "    \n",
    "    # add batch to pseudo labeled train (pltrain) dataset. needs to ignore index as ids in test also start with 0\n",
    "    pltrainnew_X_unscaled_df = pltrain_X_unscaled_df.append(testbatch_X_unscaled_df, ignore_index=True)\n",
    "    pltrainnew_y_df = pltrain_y_df.append(testbatch_y,\n",
    "                                          ignore_index=True)  # pltrainnew_Xy_unscaled_df[['fraud']], ignore_index=True)\n",
    "    return pltrainnew_X_unscaled_df, pltrainnew_y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Xy_for_crossval = train_complete_X_unscaled_df.copy()\n",
    "train_Xy_for_crossval['fraud'] = train_complete_y_df.copy()\n",
    "train_Xy_for_crossval = train_Xy_for_crossval[train_Xy_for_crossval.trustLevel <= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "keras_model = None\n",
    "with open('../data/model_architecture_low_trust.json', 'r') as f:\n",
    "    keras_model = model_from_json(f.read())\n",
    "    \n",
    "keras_model.load_weights(\"../data/nn_weights_low_trust.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679 available train data before pseudo labeling\n",
      "with a batchsize of  500 we will need 996.242 iterations:\n",
      "iteration 1 \t/ 997 with batch from 0 \t to 500 , training with 679 samples\n",
      "No KNN:   XGBoost: PLTrain auf Val: 115 --- PLTest auf Train: -15 || LinearSVC:  PLTrain auf Val: 70 --- PLTest auf Train: 155 || Own Classifier:  PLTrain auf Val: 95 --- PLTest auf Train: 90\n"
     ]
    }
   ],
   "source": [
    "data_dict = dict()\n",
    "data_dict['train_complete_X_unscaled_df'] = train_complete_X_unscaled_df.copy()\n",
    "data_dict['train_complete_X_scaled_df'] = transformer.apply_scaler(train_complete_X_unscaled_df)\n",
    "data_dict['train_complete_y_df'] = train_complete_y_df.copy()\n",
    "\n",
    "data_dict['val_X_unscaled_df'] = val_X_unscaled_df.copy()\n",
    "data_dict['val_X_scaled_df'] = transformer.apply_scaler(val_X_unscaled_df)\n",
    "data_dict['val_y_df'] = val_y_df.copy()\n",
    "data_dict['train_X_unscaled_df'] = train_X_unscaled_df\n",
    "data_dict['train_X_scaled_df'] = train_X_scaled_df\n",
    "data_dict['train_y_df'] = train_y_df\n",
    "\n",
    "\n",
    "# results, final_prediction, knn_dataframes, pred_dfs, pred_val_dfs, results_wo_knn, final_prediction_wo_knn,  pred_train_wo_knn_dfs, pred_val_wo_knn_dfs\n",
    "results, final_prediction,  pred_train_dfs, pred_val_dfs= semi_supervised_learning_procedure(test_X_unscaled_df, train_X_unscaled_df, train_y_df, data_dict, transformer, keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>498121.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.045638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.208698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               fraud\n",
       "count  498121.000000\n",
       "mean        0.045638\n",
       "std         0.208698\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.000000\n",
       "max         1.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prediction.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of derivatives at boundaries does not match: expected 3, got 0+0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-47115ec8cd31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_results_ssl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-22a3ee398e82>\u001b[0m in \u001b[0;36mplot_results_ssl\u001b[0;34m(res)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mspl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_interp_spline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdmc_scores_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lin_svc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#BSpline object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0msmooth_lin_svc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_lin_svc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lin_svc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Entwicklung der Lin SVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/dmc2019/venv/lib/python3.5/site-packages/scipy/interpolate/_bsplines.py\u001b[0m in \u001b[0;36mmake_interp_spline\u001b[0;34m(x, y, k, t, bc_type, axis, check_finite)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnleft\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnright\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         raise ValueError(\"The number of derivatives at boundaries does not \"\n\u001b[0;32m--> 827\u001b[0;31m                          \"match: expected %s, got %s+%s\" % (nt-n, nleft, nright))\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[0;31m# set up the LHS: the collocation matrix + derivatives at boundaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The number of derivatives at boundaries does not match: expected 3, got 0+0"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results_ssl(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>totalScannedLineItems</th>\n",
       "      <th>fraud</th>\n",
       "      <th>xgb_predict</th>\n",
       "      <th>lsvc_predict</th>\n",
       "      <th>xgb_proba</th>\n",
       "      <th>lsvc_proba</th>\n",
       "      <th>own_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>2</td>\n",
       "      <td>1461</td>\n",
       "      <td>39.82</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017112</td>\n",
       "      <td>0.027255</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>1</td>\n",
       "      <td>923</td>\n",
       "      <td>72.62</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.022752</td>\n",
       "      <td>0.078678</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  \\\n",
       "312           2                    1461       39.82              6   \n",
       "353           1                     923       72.62              5   \n",
       "\n",
       "     scansWithoutRegistration  quantityModifications  \\\n",
       "312                         9                      1   \n",
       "353                         9                      3   \n",
       "\n",
       "     scannedLineItemsPerSecond  valuePerSecond  lineItemVoidsPerPosition  \\\n",
       "312                   0.017112        0.027255                  0.240000   \n",
       "353                   0.022752        0.078678                  0.238095   \n",
       "\n",
       "     totalScannedLineItems  fraud  xgb_predict  lsvc_predict  xgb_proba  \\\n",
       "312                   25.0      1            1             0      0.724   \n",
       "353                   21.0      1            1             0      0.758   \n",
       "\n",
       "     lsvc_proba  own_predict  \n",
       "312       0.682            0  \n",
       "353       0.571            0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = pred_val_dfs[0]\n",
    "val[val.fraud != val.own_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>totalScannedLineItems</th>\n",
       "      <th>fraud</th>\n",
       "      <th>xgb_predict</th>\n",
       "      <th>lsvc_predict</th>\n",
       "      <th>xgb_proba</th>\n",
       "      <th>lsvc_proba</th>\n",
       "      <th>own_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1</td>\n",
       "      <td>872</td>\n",
       "      <td>41.86</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.025229</td>\n",
       "      <td>0.048005</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>2</td>\n",
       "      <td>1461</td>\n",
       "      <td>39.82</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017112</td>\n",
       "      <td>0.027255</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>1</td>\n",
       "      <td>923</td>\n",
       "      <td>72.62</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.022752</td>\n",
       "      <td>0.078678</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  \\\n",
       "134           1                     872       41.86              3   \n",
       "312           2                    1461       39.82              6   \n",
       "353           1                     923       72.62              5   \n",
       "\n",
       "     scansWithoutRegistration  quantityModifications  \\\n",
       "134                        10                      4   \n",
       "312                         9                      1   \n",
       "353                         9                      3   \n",
       "\n",
       "     scannedLineItemsPerSecond  valuePerSecond  lineItemVoidsPerPosition  \\\n",
       "134                   0.025229        0.048005                  0.136364   \n",
       "312                   0.017112        0.027255                  0.240000   \n",
       "353                   0.022752        0.078678                  0.238095   \n",
       "\n",
       "     totalScannedLineItems  fraud  xgb_predict  lsvc_predict  xgb_proba  \\\n",
       "134                   22.0      0            0             1      0.843   \n",
       "312                   25.0      1            1             0      0.724   \n",
       "353                   21.0      1            1             0      0.758   \n",
       "\n",
       "     lsvc_proba  own_predict  \n",
       "134       0.518            0  \n",
       "312       0.682            0  \n",
       "353       0.571            0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val[val.fraud != val.lsvc_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val[val.fraud != val.xgb_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val2 = pred_val_dfs[10]\n",
    "val2[val2.fraud != val2.own_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_complete_X_unscaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>totalScannedLineItems</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>770</td>\n",
       "      <td>11.09</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033766</td>\n",
       "      <td>0.014403</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1545</td>\n",
       "      <td>22.80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006472</td>\n",
       "      <td>0.014757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>725</td>\n",
       "      <td>41.08</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037241</td>\n",
       "      <td>0.056662</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>870</td>\n",
       "      <td>32.45</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.037299</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>125</td>\n",
       "      <td>25.50</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  \\\n",
       "5            1                     770       11.09             11   \n",
       "7            2                    1545       22.80              0   \n",
       "9            2                     725       41.08             10   \n",
       "15           1                     870       32.45              3   \n",
       "23           2                     125       25.50              5   \n",
       "\n",
       "    scansWithoutRegistration  quantityModifications  \\\n",
       "5                          5                      2   \n",
       "7                          8                      4   \n",
       "9                          2                      4   \n",
       "15                         1                      5   \n",
       "23                         6                      2   \n",
       "\n",
       "    scannedLineItemsPerSecond  valuePerSecond  lineItemVoidsPerPosition  \\\n",
       "5                    0.033766        0.014403                  0.423077   \n",
       "7                    0.006472        0.014757                  0.000000   \n",
       "9                    0.037241        0.056662                  0.370370   \n",
       "15                   0.006897        0.037299                  0.500000   \n",
       "23                   0.192000        0.204000                  0.208333   \n",
       "\n",
       "    totalScannedLineItems  fraud  \n",
       "5                    26.0      1  \n",
       "7                    10.0      0  \n",
       "9                    27.0      0  \n",
       "15                    6.0      0  \n",
       "23                   24.0      0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_complete_Xy = train_complete_X_unscaled_df\n",
    "train_complete_Xy['fraud'] = train_complete_Xy_original_df.fraud\n",
    "train_low_trust_Xy = train_complete_Xy[train_complete_Xy.trustLevel <= 2]\n",
    "train_low_trust_Xy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Own Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DMC Mean: 56.0 --- DMC Sum: 280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(56.0, 280)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "own_classifier_cross_validation(train_complete_Xy[train_complete_Xy.trustLevel <= 2], test_X_unscaled_df, 500, 5, transformer,keras_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best n testsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking  100 testsamples\n",
      "DMC Mean: 63.0 --- DMC Sum: 315\n",
      "DMC Mean: 68.0 --- DMC Sum: 340\n",
      "DMC Mean: 56.0 --- DMC Sum: 280\n",
      "DMC Mean: 60.0 --- DMC Sum: 300\n",
      "DMC Mean: 64.0 --- DMC Sum: 320\n",
      "DMC Mean: 59.0 --- DMC Sum: 295\n",
      "DMC Mean: 63.0 --- DMC Sum: 315\n",
      "DMC Mean: 64.0 --- DMC Sum: 320\n",
      "DMC Mean: 53.0 --- DMC Sum: 265\n",
      "DMC Mean: 60.0 --- DMC Sum: 300\n",
      "Mean of DMC Mean is 61.0 and DMC Sum is 305.0 for 100 testsamples\n",
      "Taking  200 testsamples\n",
      "DMC Mean: 51.0 --- DMC Sum: 255\n",
      "DMC Mean: 48.0 --- DMC Sum: 240\n",
      "DMC Mean: 66.0 --- DMC Sum: 330\n",
      "DMC Mean: 54.0 --- DMC Sum: 270\n",
      "DMC Mean: 64.0 --- DMC Sum: 320\n",
      "DMC Mean: 62.0 --- DMC Sum: 310\n",
      "DMC Mean: 40.0 --- DMC Sum: 200\n",
      "DMC Mean: 66.0 --- DMC Sum: 330\n",
      "DMC Mean: 63.0 --- DMC Sum: 315\n",
      "DMC Mean: 61.0 --- DMC Sum: 305\n",
      "Mean of DMC Mean is 57.5 and DMC Sum is 287.5 for 200 testsamples\n",
      "Taking  300 testsamples\n",
      "DMC Mean: 58.0 --- DMC Sum: 290\n",
      "DMC Mean: 49.0 --- DMC Sum: 245\n",
      "DMC Mean: 59.0 --- DMC Sum: 295\n",
      "DMC Mean: 56.0 --- DMC Sum: 280\n",
      "DMC Mean: 53.0 --- DMC Sum: 265\n",
      "DMC Mean: 63.0 --- DMC Sum: 315\n",
      "DMC Mean: 47.0 --- DMC Sum: 235\n",
      "DMC Mean: 49.0 --- DMC Sum: 245\n",
      "DMC Mean: 60.0 --- DMC Sum: 300\n",
      "DMC Mean: 57.0 --- DMC Sum: 285\n",
      "Mean of DMC Mean is 55.1 and DMC Sum is 275.5 for 300 testsamples\n",
      "Taking  400 testsamples\n",
      "DMC Mean: 59.0 --- DMC Sum: 295\n",
      "DMC Mean: 47.0 --- DMC Sum: 235\n",
      "DMC Mean: 61.0 --- DMC Sum: 305\n",
      "DMC Mean: 59.0 --- DMC Sum: 295\n",
      "DMC Mean: 68.0 --- DMC Sum: 340\n",
      "DMC Mean: 59.0 --- DMC Sum: 295\n",
      "DMC Mean: 64.0 --- DMC Sum: 320\n",
      "DMC Mean: 55.0 --- DMC Sum: 275\n",
      "DMC Mean: 64.0 --- DMC Sum: 320\n",
      "DMC Mean: 55.0 --- DMC Sum: 275\n",
      "Mean of DMC Mean is 59.1 and DMC Sum is 295.5 for 400 testsamples\n",
      "Taking  500 testsamples\n",
      "DMC Mean: 65.0 --- DMC Sum: 325\n",
      "DMC Mean: 61.0 --- DMC Sum: 305\n",
      "DMC Mean: 75.0 --- DMC Sum: 375\n",
      "DMC Mean: 60.0 --- DMC Sum: 300\n",
      "DMC Mean: 61.0 --- DMC Sum: 305\n",
      "DMC Mean: 49.0 --- DMC Sum: 245\n",
      "DMC Mean: 61.0 --- DMC Sum: 305\n",
      "DMC Mean: 44.0 --- DMC Sum: 220\n",
      "DMC Mean: 61.0 --- DMC Sum: 305\n",
      "DMC Mean: 59.0 --- DMC Sum: 295\n",
      "Mean of DMC Mean is 59.6 and DMC Sum is 298.0 for 500 testsamples\n",
      "Taking  600 testsamples\n",
      "DMC Mean: 61.0 --- DMC Sum: 305\n",
      "DMC Mean: 57.0 --- DMC Sum: 285\n",
      "DMC Mean: 68.0 --- DMC Sum: 340\n",
      "DMC Mean: 62.0 --- DMC Sum: 310\n",
      "DMC Mean: 59.0 --- DMC Sum: 295\n",
      "DMC Mean: 48.0 --- DMC Sum: 240\n",
      "DMC Mean: 54.0 --- DMC Sum: 270\n",
      "DMC Mean: 60.0 --- DMC Sum: 300\n",
      "DMC Mean: 63.0 --- DMC Sum: 315\n",
      "DMC Mean: 64.0 --- DMC Sum: 320\n",
      "Mean of DMC Mean is 59.6 and DMC Sum is 298.0 for 600 testsamples\n",
      "Taking  700 testsamples\n",
      "DMC Mean: 39.0 --- DMC Sum: 195\n",
      "DMC Mean: 59.0 --- DMC Sum: 295\n",
      "DMC Mean: 61.0 --- DMC Sum: 305\n",
      "DMC Mean: 60.0 --- DMC Sum: 300\n",
      "DMC Mean: 36.0 --- DMC Sum: 180\n",
      "DMC Mean: 66.0 --- DMC Sum: 330\n",
      "DMC Mean: 64.0 --- DMC Sum: 320\n",
      "DMC Mean: 62.0 --- DMC Sum: 310\n",
      "DMC Mean: 72.0 --- DMC Sum: 360\n",
      "DMC Mean: 56.0 --- DMC Sum: 280\n",
      "Mean of DMC Mean is 57.5 and DMC Sum is 287.5 for 700 testsamples\n"
     ]
    }
   ],
   "source": [
    "for i in [100,200,300,400,500,600,700]:\n",
    "    print(\"Taking \",i, \"testsamples\")\n",
    "    dmc_scores = 0\n",
    "    dmc_sum = 0\n",
    "    for j in range(10):\n",
    "        train_Xy_for_crossval = train_complete_X_unscaled_df.copy()\n",
    "        train_Xy_for_crossval['fraud'] = train_complete_y_df.copy()\n",
    "        train_Xy_for_crossval.head()\n",
    "        test_X_for_crossval = test_X_unscaled_df.copy()\n",
    "        res = own_classifier_cross_validation(train_Xy_for_crossval,test_X_for_crossval,i,5,transformer, keras_model)\n",
    "        dmc_scores += res[0]\n",
    "        dmc_sum += res[1]\n",
    "    print(\"Mean of DMC Mean is {} and DMC Sum is {} for {} testsamples\".format(dmc_scores/10,dmc_sum/10, i, ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Traindataset for neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = transformer.add_features(train_complete_X_unscaled_df)\n",
    "train['fraud'] = train_complete_y_df\n",
    "train = train[train.trustLevel <= 2]\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_set = create_nn_train_data(train.copy(), 5, transformer)\n",
    "pred_val_set.to_csv(\"../data/nn_train_data_low_trust.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Define train1\n",
    "\n",
    "\"\"\"\n",
    "prf_own = metrics.precision_recall_fscore_support(train1.fraud, train1.own_predict, beta=0.5172, average='binary')\n",
    "prf_xgb = metrics.precision_recall_fscore_support(train1.fraud, train1.xgb_predict, beta=0.5172, average='binary')\n",
    "prf_lsvc = metrics.precision_recall_fscore_support(train1.fraud, train1.lsvc_predict, beta=0.5172, average='binary')\n",
    "dmc_score_own = np.sum(metrics.confusion_matrix(train1.fraud, train1.own_predict)* np.array([[0, -25], [-5, 5]]))\n",
    "dmc_score_xgb = np.sum(metrics.confusion_matrix(train1.fraud, train1.xgb_predict)* np.array([[0, -25], [-5, 5]]))\n",
    "dmc_score_lsvc = np.sum(metrics.confusion_matrix(train1.fraud, train1.lsvc_predict)* np.array([[0, -25], [-5, 5]]))\n",
    "\n",
    "\n",
    "print(\"OWN CLASSIFIER: Precision: {} Recall: {} Fbeta: {} DMC: {}\".format(prf[0], prf[1], prf[2],dmc_score_own))\n",
    "print(\"XGBOOST: Precision: {} Recall: {} Fbeta: {} DMC: {}\".format(prf_xgb[0], prf_xgb[1], prf_xgb[2],dmc_score_xgb))\n",
    "print(\"LINEAR SVCPrecision: {} Recall: {} Fbeta: {} DMC: {}\".format(prf_lsvc[0], prf_lsvc[1], prf_lsvc[2],dmc_score_lsvc))\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.) pseudo-label the test set and create new classifier based on this\n",
    "first we predict the original test data labels using the new extended pltrain from above cell and second we use this test data labels to train a new classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.) evaluate our new classifier with the original training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.) combine the pseudo labeled test set with the original train data to train our final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_clf = ...\n",
    "#TODO; it would be cool if our classify loop can get a sklearn-like api for fit and predict, but we run out of time.\n",
    "#      we can instead extract final classification from the existing dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.) predict labels for the test set using our final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_testpred_y_original = final_clf.predict(test_X_scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.) generate output file neeeded for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(final_testpred_y_original, columns=[\"fraud\"]).to_csv(\"HS_Karlsruhe_1.csv\", index=False)\n",
    "#pd.read_csv(\"HS_Karlsruhe_1.csv\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.) evaluate our new classifier with the validation set\n",
    "Now at the very end we can also test our final model on a validation split never used before. just for comparison. \n",
    "\n",
    "**For the final submission, the following code should will not be run and the full train (incl. this val split) set will be used above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not FINAL_SUBMISSION:\n",
    "#    final_valpred_y_original = final_clf.predict(val_X_scaled_df)\n",
    "#    print(calc_scores(val_y_original_df.fraud.values, final_valpred_y_original))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
