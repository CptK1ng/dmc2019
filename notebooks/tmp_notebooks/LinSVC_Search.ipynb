{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors, KDTree\n",
    "from scores import scores\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.) Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebooks/utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainandknn_Xy_original_df = pd.read_csv(\"../data/train_new.csv\", sep=\"|\")\n",
    "train_Xy_original_df, knn_Xy_original_df = train_test_split(trainandknn_Xy_original_df,train_size=0.75, random_state=42) # if FINAL_SUBMISSION else 0.8) #small\n",
    "test_X_original_df  = pd.read_csv(\"../data/test.csv\", sep=\"|\").iloc[0:500] #TODO: For faster testing we use less data from the test set\n",
    "test_final_X_df = pd.read_csv(\"../data/test.csv\", sep=\"|\")\n",
    "\n",
    "train_Xy_wo_knn_df = pd.read_csv(\"../data/train_new.csv\", sep=\"|\")\n",
    "\n",
    "#Only for test routines\n",
    "val_Xy_original_df = pd.read_csv(\"../data/val_new.csv\", sep=\"|\")\n",
    "train_complete_Xy_original_df = pd.read_csv(\"../data/train.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convention for variables names: datasetname_columntype_transformstatus_dataframeornot\n",
    "train_y_original_df = train_Xy_original_df[[\"fraud\"]].copy()\n",
    "train_X_original_df = train_Xy_original_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "knn_y_original_df = knn_Xy_original_df[[\"fraud\"]].copy()\n",
    "knn_X_original_df = knn_Xy_original_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "# Only for test routie#nes\n",
    "val_y_originial_df = val_Xy_original_df[[\"fraud\"]].copy()\n",
    "val_X_originial_df = val_Xy_original_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "train_y_wo_knn_df = train_Xy_wo_knn_df[[\"fraud\"]].copy()\n",
    "train_X_wo_knn_df = train_Xy_wo_knn_df.copy().drop(\"fraud\", axis=1)\n",
    "\n",
    "train_complete_y_originial_df = train_complete_Xy_original_df[[\"fraud\"]].copy()\n",
    "train_complete_X_originial_df = train_complete_Xy_original_df.copy().drop(\"fraud\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>totalScannedLineItems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.925130</td>\n",
       "      <td>0.240558</td>\n",
       "      <td>0.153259</td>\n",
       "      <td>0.449502</td>\n",
       "      <td>-1.558114</td>\n",
       "      <td>0.300299</td>\n",
       "      <td>-0.117812</td>\n",
       "      <td>-0.128310</td>\n",
       "      <td>-0.371627</td>\n",
       "      <td>1.559426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.249301</td>\n",
       "      <td>-1.548098</td>\n",
       "      <td>-0.789781</td>\n",
       "      <td>-0.126199</td>\n",
       "      <td>-0.923450</td>\n",
       "      <td>0.886361</td>\n",
       "      <td>0.244945</td>\n",
       "      <td>0.043534</td>\n",
       "      <td>-0.282261</td>\n",
       "      <td>-0.173782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.249301</td>\n",
       "      <td>1.114087</td>\n",
       "      <td>0.410578</td>\n",
       "      <td>-0.701901</td>\n",
       "      <td>1.615204</td>\n",
       "      <td>1.472424</td>\n",
       "      <td>-0.185091</td>\n",
       "      <td>-0.137604</td>\n",
       "      <td>-0.379817</td>\n",
       "      <td>-0.289330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.512346</td>\n",
       "      <td>1.634045</td>\n",
       "      <td>1.450544</td>\n",
       "      <td>0.737353</td>\n",
       "      <td>-0.288787</td>\n",
       "      <td>0.886361</td>\n",
       "      <td>-0.158033</td>\n",
       "      <td>-0.128614</td>\n",
       "      <td>-0.345007</td>\n",
       "      <td>1.559426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.925130</td>\n",
       "      <td>-0.939274</td>\n",
       "      <td>1.078709</td>\n",
       "      <td>-0.701901</td>\n",
       "      <td>0.663209</td>\n",
       "      <td>-0.285764</td>\n",
       "      <td>0.007505</td>\n",
       "      <td>-0.010833</td>\n",
       "      <td>-0.472189</td>\n",
       "      <td>1.328332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  \\\n",
       "0    0.925130                0.240558    0.153259       0.449502   \n",
       "1   -0.249301               -1.548098   -0.789781      -0.126199   \n",
       "2   -0.249301                1.114087    0.410578      -0.701901   \n",
       "3    1.512346                1.634045    1.450544       0.737353   \n",
       "4    0.925130               -0.939274    1.078709      -0.701901   \n",
       "\n",
       "   scansWithoutRegistration  quantityModifications  scannedLineItemsPerSecond  \\\n",
       "0                 -1.558114               0.300299                  -0.117812   \n",
       "1                 -0.923450               0.886361                   0.244945   \n",
       "2                  1.615204               1.472424                  -0.185091   \n",
       "3                 -0.288787               0.886361                  -0.158033   \n",
       "4                  0.663209              -0.285764                   0.007505   \n",
       "\n",
       "   valuePerSecond  lineItemVoidsPerPosition  totalScannedLineItems  \n",
       "0       -0.128310                 -0.371627               1.559426  \n",
       "1        0.043534                 -0.282261              -0.173782  \n",
       "2       -0.137604                 -0.379817              -0.289330  \n",
       "3       -0.128614                 -0.345007               1.559426  \n",
       "4       -0.010833                 -0.472189               1.328332  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: fraud, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "transformer = DataTransformer(scaler)\n",
    "\n",
    "# Adding new Features to train and test set\n",
    "train_X_unscaled_df = transformer.add_features(train_X_original_df)\n",
    "test_X_unscaled_df = transformer.add_features(test_X_original_df)\n",
    "knn_X_unscaled_df = transformer.add_features(knn_X_original_df)\n",
    "\n",
    "val_X_unscaled_df = transformer.add_features(val_X_originial_df)\n",
    "train_complete_X_unscaled_df = transformer.add_features(train_complete_X_originial_df) \n",
    "\n",
    "transformer.fit_scaler(transformer.add_features(train_complete_X_unscaled_df.append(test_X_unscaled_df, sort=False)))\n",
    "train_X_scaled_df = transformer.apply_scaler(train_X_unscaled_df)\n",
    "knn_X_scaled_df   = transformer.apply_scaler(knn_X_unscaled_df)\n",
    "\n",
    "\n",
    "test_X_scaled_df  = transformer.apply_scaler(test_X_unscaled_df)\n",
    "val_X_scaled_df = transformer.apply_scaler(val_X_unscaled_df)\n",
    "train_complete_X_scaled_df = transformer.apply_scaler(train_complete_X_unscaled_df)\n",
    "\n",
    "train_X_wo_knn_unscaled_df = transformer.add_features(train_X_wo_knn_df.copy())\n",
    "train_X_wo_knn_scaled_df = transformer.apply_scaler(train_X_wo_knn_unscaled_df)\n",
    "\n",
    "# labels\n",
    "train_y_df = train_y_original_df.copy()\n",
    "val_y_df = val_y_originial_df.copy()\n",
    "knn_y_df = knn_y_original_df.copy()\n",
    "\n",
    "train_complete_X_scaled_df = transformer.apply_scaler(transformer.add_features(train_complete_Xy_original_df.copy().drop(columns=['fraud'])))\n",
    "train_complete_y_df = train_complete_Xy_original_df.copy().fraud\n",
    "\n",
    "test_final_X_df = transformer.add_features(test_final_X_df)\n",
    "\n",
    "\n",
    "display(train_complete_X_scaled_df.head(5))\n",
    "display(train_complete_y_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring functions\n",
    "Defining multiple scores which should be tracked in the HyperParamSearch Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = scores.Scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'AUC': 'roc_auc', 'FBeta': metrics.make_scorer(metrics.fbeta_score, beta=0.5172), \"Precision\":'precision', \"Recall\": 'recall', \"AP\": score.average_precision, \"DMC\" : score.dmc_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the paramteres which should be tuned\n",
    "To tune the hyperparameters, i looked into the [documentation here](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster). `randint` can be used for integer values, for float values, use `uniform`. \n",
    "\n",
    "You can also use a Grid search on single parameters to get a feeling for a good interval. If you want to try only two possibilities, you can create a list like for the `scale_pos_weight` parameter.\n",
    "\n",
    "**Note: For the classifiers which work without gpu support, you can probably set a parameter n_jobs=-1 to use all processors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"tol\": uniform(1e-5, 1e-1),  # default 100\n",
    "    \"C\" :  uniform(0.0, 80.0),\n",
    "    \"shrinking\" : [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a classifier with some default values\n",
    "Not all paramters of a classifier should be fine tuned. For SVM for example, the `kernel`-paramter should be set manually. In the case of xgboost, some things like the objective, the booster and the tree method should not be tuned. The choice of paramters depend on the specific classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['class_weight', 'max_iter', 'shrinking', 'gamma', 'degree', 'decision_function_shape', 'C', 'probability', 'cache_size', 'random_state', 'tol', 'verbose', 'coef0', 'kernel'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_svm = SVC(kernel=\"linear\", probability=True, cache_size=8000,  verbose=0, random_state=42)\n",
    "default_svm.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 798 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1698 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2798 tasks      | elapsed:   30.9s\n",
      "[Parallel(n_jobs=-1)]: Done 3000 out of 3000 | elapsed:   33.0s finished\n"
     ]
    }
   ],
   "source": [
    "search = RandomizedSearchCV(default_svm, scoring=scoring, param_distributions=params, random_state=42, n_iter=1000,\n",
    "                            cv=3, verbose=1, n_jobs=-1, return_train_score=True,refit='DMC')\n",
    "search.fit(train_complete_X_scaled_df, train_complete_y_df)\n",
    "results = search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_complete_Xy_scaled_df = train_complete_X_scaled_df.copy()\n",
    "train_complete_Xy_scaled_df['fraud'] =train_complete_y_df.copy()\n",
    "\n",
    "train_Xy_wo_knn_scaled_df = train_X_wo_knn_scaled_df.copy()\n",
    "train_Xy_wo_knn_scaled_df['fraud'] = train_y_wo_knn_df\n",
    "\n",
    "val_Xy_scaled = val_X_scaled_df.copy()\n",
    "val_Xy_scaled['fraud'] = val_y_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Fix Split: \n",
      "DMC Score: 55  ---  Normalized DMC Score: 0.14627659574468085, \n",
      "\n",
      "Results Cross Validation: \n",
      "DMC Score: 66.0  ---  Normalized DMC Score: 0.1755886524822695 \n"
     ]
    }
   ],
   "source": [
    "scorings = {'AUC': 'roc_auc', 'FBeta': metrics.make_scorer(metrics.fbeta_score, beta=0.5172), \"Precision\":'precision', \"Recall\": 'recall', \"AP\": score.average_precision, \"DMC\" : score.dmc_score}\n",
    "xgbo = search.best_estimator_\n",
    "result_dict = test_classification(xgbo,df_train=train_Xy_wo_knn_scaled_df, df_val=val_Xy_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmc = np.mean(search.cv_results_['mean_test_DMC'])\n",
    "ap = np.mean(search.cv_results_['mean_test_AP'])\n",
    "precision = np.mean(search.cv_results_['mean_test_Precision'])\n",
    "recall = np.mean(search.cv_results_['mean_test_Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DMC: 89.43990420436403 --- Average Precision: 0.9786406695625676 --- Precision: 0.9262799178732629 --- Recall: 0.9497573946449254\n"
     ]
    }
   ],
   "source": [
    "print(\"DMC: {} --- Average Precision: {} --- Precision: {} --- Recall: {}\".format(dmc, ap, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=11.439334564226868, cache_size=8000, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=True, random_state=42,\n",
       "  shrinking=False, tol=0.08370638742373739, verbose=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 11.439334564226868, 'shrinking': False, 'tol': 0.08370638742373739}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
